{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a MAMBA-like or SSM model.\n",
    "---\n",
    "Last Modified: 31 March, 2024\n",
    "\n",
    "An attempt to learn about and implement a MAMBA-like architecture or [selective] simpler state-space model (SSM).\n",
    "Borrows extensively code from https://github.com/johnma2006/mamba-minimal/tree/master and develops on material from https://srush.github.io/annotated-s4/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models._pkg import *\n",
    "from models._utils import RMSNorm, InnerHiddenModel, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement MAMBA Model\n",
    "@dataclass\n",
    "class MambaArgs:\n",
    "    d_model: int  # dimension of input\n",
    "    n_layer: int\n",
    "    d_state: int = 16  # dimension of hidden state\n",
    "    expand: int = 1  # expansion factor (1 = no expansion)\n",
    "    dt_rank: Union[int, str] = \"auto\"\n",
    "    d_conv: int = 4\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "\n",
    "        if self.dt_rank == \"auto\":\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "\n",
    "\n",
    "class MambaBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Code taken from https://github.com/johnma2006/mamba-minimal/blob/master/model.py#L143.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.norm = RMSNorm(args.d_model)\n",
    "\n",
    "        self.in_proj = torch.nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
    "\n",
    "        self.conv1d = torch.nn.Conv1d(\n",
    "            in_channels=args.d_inner,\n",
    "            out_channels=args.d_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.d_conv,\n",
    "            groups=args.d_inner,\n",
    "            padding=args.d_conv - 1,\n",
    "        )\n",
    "\n",
    "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
    "        self.x_proj = torch.nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
    "\n",
    "        # dt_proj projects Δ from dt_rank to d_in\n",
    "        self.dt_proj = torch.nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
    "\n",
    "        A = repeat(torch.arange(1, args.d_state + 1), \"n -> d n\", d=args.d_inner)\n",
    "        self.A_log = torch.nn.Parameter(torch.log(A))\n",
    "        self.D = torch.nn.Parameter(torch.ones(args.d_inner))\n",
    "        self.out_proj = torch.nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        x_copy, x = torch.clone(x), self.norm(x)  # hack that does what a residual block would\n",
    "\n",
    "        (b, l, d) = x.shape\n",
    "\n",
    "        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
    "        (x, res) = x_and_res.split(split_size=[self.args.d_inner, self.args.d_inner], dim=-1)\n",
    "\n",
    "        x = rearrange(x, \"b l d_in -> b d_in l\")\n",
    "        x = self.conv1d(x)[:, :, :l]\n",
    "        x = rearrange(x, \"b d_in l -> b l d_in\")\n",
    "\n",
    "        x = torch.nn.functional.silu(x)\n",
    "\n",
    "        y = self.ssm(x)\n",
    "\n",
    "        y = y * torch.nn.functional.silu(res)\n",
    "\n",
    "        output = self.out_proj(y) + x_copy  # residual/skip connection\n",
    "\n",
    "        return output\n",
    "\n",
    "    def ssm(self, x):\n",
    "        \"\"\"Runs the SSM. See:\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (d_in, n) = self.A_log.shape\n",
    "\n",
    "        # Compute ∆ A B C D, the state space parameters.\n",
    "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
    "        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
    "        #                                  and is why Mamba is called **selective** state spaces)\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
    "        D = self.D.float()\n",
    "\n",
    "        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n",
    "\n",
    "        (delta, B, C) = x_dbl.split(\n",
    "            split_size=[self.args.dt_rank, n, n], dim=-1\n",
    "        )  # delta: (b, l, dt_rank). B, C: (b, l, n)\n",
    "        delta = torch.nn.functional.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
    "\n",
    "        y = self.selective_scan(\n",
    "            x, delta, A, B, C, D\n",
    "        )  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        return y\n",
    "\n",
    "    def selective_scan(self, u, delta, A, B, C, D):\n",
    "        \"\"\"Does selective scan algorithm. See:\n",
    "            - Section 2 State Space Models in the Mamba paper [1]\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        This is the classic discrete state space formula:\n",
    "            x(t + 1) = Ax(t) + Bu(t)\n",
    "            y(t)     = Cx(t) + Du(t)\n",
    "        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n",
    "\n",
    "        Args:\n",
    "            u: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "            delta: shape (b, l, d_in)\n",
    "            A: shape (d_in, n)\n",
    "            B: shape (b, l, n)\n",
    "            C: shape (b, l, n)\n",
    "            D: shape (d_in,)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n",
    "            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d_in) = u.shape\n",
    "        n = A.shape[1]\n",
    "\n",
    "        # Discretize continuous parameters (A, B)\n",
    "        # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
    "        # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors:\n",
    "        #   \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
    "        deltaA = torch.exp(einsum(delta, A, \"b l d_in, d_in n -> b l d_in n\"))\n",
    "        deltaB_u = einsum(delta, B, u, \"b l d_in, b l n, b l d_in -> b l d_in n\")\n",
    "\n",
    "        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n",
    "        # Note that the below is sequential, while the official implementation does a much faster parallel scan that\n",
    "        # is additionally hardware-aware (like FlashAttention).\n",
    "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
    "        ys = []\n",
    "        for i in range(l):\n",
    "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
    "            y = einsum(x, C[:, i, :], \"b d_in n, b n -> b d_in\")\n",
    "            ys.append(y)\n",
    "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
    "\n",
    "        y = y + u * D\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaCore(Model):\n",
    "    \"\"\"\n",
    "    TODO: Better understand the implementation of Mamba/SSMs to get this model working.\n",
    "        Also write a docstring for this model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: Union[int, None] = None,\n",
    "        loss: Union[Callable, None] = None,\n",
    "        l1_reg_param: float = 0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Initialize super class\n",
    "        super(MambaCore, self).__init__(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            loss,\n",
    "            l1_reg_param,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Special parameters for this model\n",
    "        self.mamba_args = MambaArgs(\n",
    "            d_model=self.hidden_size,\n",
    "            n_layer=1,  # all our cores are single-layer modules\n",
    "            d_state=self.hidden_size,\n",
    "            # remaining args use defaults of MambaArgs dataclass\n",
    "        )\n",
    "        # Input to hidden transformation\n",
    "        self.input_hidden = torch.nn.Sequential(\n",
    "            self.latent_embedding,\n",
    "            # NOTE: Do NOT use LayerNorm here!\n",
    "        )\n",
    "        # Hidden to hidden transformation: FeedForward layer\n",
    "        self.hidden_hidden = MambaBlock(self.mamba_args)\n",
    "        # Instantiate internal hidden model (i.e. the \"core\")\n",
    "        self.inner_hidden_model = InnerHiddenModel(\n",
    "            hidden_hidden_model=self.hidden_hidden,\n",
    "            hidden_state=self.hidden,\n",
    "        )\n",
    "\n",
    "    def init_hidden(self, input_shape=None):\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
