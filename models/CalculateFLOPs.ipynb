{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from utils import DEVICE, init_random_seeds\n",
    "from models._utils import (\n",
    "    print_parameters,\n",
    "    NaivePredictor,\n",
    "    LinearRegression,\n",
    "    FeatureFFNN,\n",
    "    PureAttention,\n",
    "    NeuralTransformer,\n",
    "    NetworkLSTM,\n",
    "    NetworkCTRNN,\n",
    "    LiquidCfC,\n",
    ")\n",
    "\n",
    "# from fvcore.nn import (\n",
    "#     FlopCountAnalysis,\n",
    "#     ActivationCountAnalysis,\n",
    "#     flop_count_table,\n",
    "#     flop_count_str,\n",
    "# )\n",
    "\n",
    "\n",
    "# Initialize the random seeds\n",
    "init_random_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple PyTorch model: Linear(in_features=302, out_features=300, bias=True)\n",
      "\n",
      "Input: torch.Size([1, 100, 302]) \t Output: torch.Size([1, 100, 300])\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Custom model: NetworkCTRNN(\n",
      "  (identity): Identity()\n",
      "  (input_hidden): Sequential(\n",
      "    (0): Linear(in_features=302, out_features=300, bias=True)\n",
      "    (1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (hidden_hidden): CTRNN(\n",
      "    (input2h): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (h2h): Linear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (inner_hidden_model): InnerHiddenModel(\n",
      "    (hidden_hidden): CTRNN(\n",
      "      (input2h): Linear(in_features=300, out_features=300, bias=True)\n",
      "      (h2h): Linear(in_features=300, out_features=300, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (latent_embedding): Linear(in_features=302, out_features=300, bias=True)\n",
      "  (linear): Linear(in_features=300, out_features=302, bias=True)\n",
      "  (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "\n",
      "Input: torch.Size([1, 100, 302]) \t Mask: torch.Size([1, 302]) \t Output: torch.Size([1, 100, 302])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Prepare model and input\n",
    "# @markdown Make sure the model and input are on the same device.\n",
    "\n",
    "# Set shapes for model and input\n",
    "seq_len = 100\n",
    "input_size = 302\n",
    "hidden_size = 300\n",
    "\n",
    "# Use a standard PyTorch model\n",
    "model = torch.nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # switch to eval mode\n",
    "print(f\"Simple PyTorch model: {model}\\n\")\n",
    "\n",
    "# Create input of the correct shape for the model\n",
    "input = torch.randn(1, seq_len, input_size).to(DEVICE)  # batch_size=1\n",
    "mask = None\n",
    "print(f\"Input: {input.shape} \\t Output: {model(input).shape}\", end=\"\\n\\n\")\n",
    "print(f\"\\n{'~'*100}\\n\")\n",
    "\n",
    "# Load one of our custom models instead\n",
    "model_args = dict(input_size=input_size, hidden_size=hidden_size, loss=\"MSE\")\n",
    "# model = NaivePredictor(**model_args)\n",
    "# model = LinearRegression(**model_args)\n",
    "# model = FeatureFFNN(**model_args)  # hidden_size = 516 -> num_params = 580286\n",
    "# model = PureAttention(**model_args)  # hidden_size = 312 -> num_params = 580310\n",
    "# model = NeuralTransformer(**model_args)  # hidden_size = 196 -> num_params = 584186\n",
    "# model = NetworkLSTM(**model_args)  # hidden_size = 234 -> num_params = 582260\n",
    "model = NetworkCTRNN(**model_args)  # hidden_size = 408 -> num_params = 582110\n",
    "# model = LiquidCfC(**model_args)  # hidden_size = 422 -> num_params = 582368\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # switch to eval mode\n",
    "print(f\"Custom model: {model}\\n\")\n",
    "\n",
    "# Create input of the correct shape for the model\n",
    "input = torch.randn(1, seq_len, input_size).to(DEVICE)  # batch_size=1\n",
    "mask = torch.ones(1, input_size).to(bool).to(DEVICE)\n",
    "print(\n",
    "    f\"Input: {input.shape} \\t Mask: {mask.shape} \\t Output: {model(input, mask).shape}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All params: 363302\n",
      "Trainable params: 363302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Using fvcore\n",
    "\n",
    "# Adjust input based on if we use standard PyTorch model or custom model\n",
    "input = (input, mask) if mask is not None else input\n",
    "\n",
    "# Count the total and number of trainable parameters\n",
    "all_params_ct, train_params_ct = print_parameters(model)\n",
    "\n",
    "print(f\"\\nAll params: {all_params_ct}\\nTrainable params: {train_params_ct}\", end=\"\\n\\n\")\n",
    "\n",
    "### DEBUG: Find way to compute FLOP using Pytorch Profiler ###\n",
    "\n",
    "# # Perform FLOP Counting: Use the FlopCountAnalysis class to analyze your model:\n",
    "# flops = FlopCountAnalysis(model, input)\n",
    "\n",
    "# # Print Results: You can now print out the FLOP and parameter information:\n",
    "# print(\n",
    "#     f\"FLOP: {flops.total(), flops.by_operator(), flops.by_module(), flops.by_module_and_operator()}\",\n",
    "#     end=\"\\n\\n\",\n",
    "# )\n",
    "# print(flop_count_str(flops), end=\"\\n\\n\")\n",
    "# print(flop_count_table(flops), end=\"\\n\\n\")\n",
    "# print(\n",
    "#     f\"\\tParams: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\",\n",
    "#     end=\"\\n\\n\",\n",
    "# )\n",
    "\n",
    "# # Perform Activations Counting: Use the ActivationCountAnalysis class to analyze your model:\n",
    "# acts = ActivationCountAnalysis(model, input)\n",
    "\n",
    "# # Print Results: You can now print out the FLOP and parameter information:\n",
    "# print(\n",
    "#     f\"Activations: {acts.total(), acts.by_operator(), acts.by_module(), acts.by_module_and_operator()}\",\n",
    "#     end=\"\\n\\n\",\n",
    "# )\n",
    "# print(\n",
    "#     f\"\\tParams: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\",\n",
    "#     end=\"\\n\\n\",\n",
    "# )\n",
    "\n",
    "### DEBUG: Find way to compute FLOP using Pytorch Profiler ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Identity()\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=302, out_features=300, bias=True)\n",
       "    (1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (2): CTRNN(\n",
       "    (input2h): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (h2h): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (3): InnerHiddenModel(\n",
       "    (hidden_hidden): CTRNN(\n",
       "      (input2h): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (h2h): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (4): Linear(in_features=302, out_features=300, bias=True)\n",
       "  (5): Linear(in_features=300, out_features=302, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Sequential(*(list(model.children())[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetworkCTRNN(\n",
       "  (identity): Identity()\n",
       "  (input_hidden): Sequential(\n",
       "    (0): Linear(in_features=302, out_features=300, bias=True)\n",
       "    (1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (hidden_hidden): CTRNN(\n",
       "    (input2h): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (h2h): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (inner_hidden_model): InnerHiddenModel(\n",
       "    (hidden_hidden): CTRNN(\n",
       "      (input2h): Linear(in_features=300, out_features=300, bias=True)\n",
       "      (h2h): Linear(in_features=300, out_features=300, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (latent_embedding): Linear(in_features=302, out_features=300, bias=True)\n",
       "  (linear): Linear(in_features=300, out_features=302, bias=True)\n",
       "  (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
