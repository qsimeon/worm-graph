{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from utils import DEVICE, init_random_seeds\n",
    "from models._utils import print_parameters, FeatureFFNN, NeuralTransformer, NetworkLSTM\n",
    "from fvcore.nn import (\n",
    "    FlopCountAnalysis,\n",
    "    ActivationCountAnalysis,\n",
    "    flop_count_table,\n",
    "    flop_count_str,\n",
    ")\n",
    "\n",
    "# Display the DEVICE\n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "\n",
    "# Initialize the random seeds\n",
    "init_random_seeds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple PyTorch model: Linear(in_features=302, out_features=512, bias=True)\n",
      "\n",
      "Input: torch.Size([1, 100, 302]) \t Output: torch.Size([1, 100, 512])\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Custom model: NetworkLSTM(\n",
      "  (identity): Identity()\n",
      "  (input_hidden): Sequential(\n",
      "    (0): Linear(in_features=302, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (hidden_hidden): LSTM(512, 512, batch_first=True)\n",
      "  (inner_hidden_model): InnerHiddenModel(\n",
      "    (hidden_hidden): LSTM(512, 512, batch_first=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=302, bias=True)\n",
      ")\n",
      "\n",
      "Input: torch.Size([1, 100, 302]) \t Mask: torch.Size([1, 302]) \t Output: torch.Size([1, 100, 302])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Prepare model and input\n",
    "# @markdown Make sure the model and input are on the same device.\n",
    "\n",
    "# Set shapes for model and input\n",
    "seq_len = 100\n",
    "input_size = 302\n",
    "hidden_size = 512\n",
    "\n",
    "# Use a standard PyTorch model\n",
    "model = torch.nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # switch to eval mode\n",
    "print(f\"Simple PyTorch model: {model}\\n\")\n",
    "\n",
    "# Create input of the correct shape for the model\n",
    "input = torch.randn(1, seq_len, input_size).to(DEVICE)  # batch_size=1\n",
    "mask = None\n",
    "print(f\"Input: {input.shape} \\t Output: {model(input).shape}\", end=\"\\n\\n\")\n",
    "print(f\"\\n{'~'*100}\\n\")\n",
    "\n",
    "# Load one of our custom models instead\n",
    "model_args = dict(input_size=input_size, hidden_size=hidden_size, loss=\"MSE\")\n",
    "model = FeatureFFNN(**model_args)\n",
    "# model = NeuralTransformer(**model_args)\n",
    "# model = NetworkLSTM(**model_args)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # switch to eval mode\n",
    "print(f\"Custom model: {model}\\n\")\n",
    "\n",
    "# Create input of the correct shape for the model\n",
    "input = torch.randn(1, seq_len, input_size).to(DEVICE)  # batch_size=1\n",
    "mask = torch.ones(1, input_size).to(bool).to(DEVICE)\n",
    "print(\n",
    "    f\"Input: {input.shape} \\t Mask: {mask.shape} \\t Output: {model(input, mask).shape}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::expand_as encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 2 time(s)\n",
      "Unsupported operator aten::lstm encountered 1 time(s)\n",
      "Unsupported operator aten::expand_as encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 2 time(s)\n",
      "Unsupported operator aten::layer_norm encountered 1 time(s)\n",
      "Unsupported operator aten::lstm encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All params: 2412334\n",
      "Trainable params: 2412334\n",
      "\n",
      "FLOPs: (31180800, Counter({'linear': 30924800, 'layer_norm': 256000}), Counter({'': 31180800, 'input_hidden': 15718400, 'input_hidden.0': 15462400, 'linear': 15462400, 'input_hidden.2': 256000, 'identity': 0, 'input_hidden.1': 0, 'hidden_hidden': 0, 'inner_hidden_model': 0}), {'': Counter({'linear': 30924800, 'layer_norm': 256000}), 'identity': Counter(), 'input_hidden': Counter({'linear': 15462400, 'layer_norm': 256000}), 'input_hidden.0': Counter({'linear': 15462400}), 'input_hidden.1': Counter(), 'input_hidden.2': Counter({'layer_norm': 256000}), 'hidden_hidden': Counter(), 'inner_hidden_model': Counter(), 'linear': Counter({'linear': 15462400})})\n",
      "\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "NetworkLSTM(\n",
      "  #params: 2.41M, #flops: 31.18M\n",
      "  (identity): Identity(#params: 0, #flops: N/A)\n",
      "  (input_hidden): Sequential(\n",
      "    #params: 0.16M, #flops: 15.72M\n",
      "    (0): Linear(\n",
      "      in_features=302, out_features=512, bias=True\n",
      "      #params: 0.16M, #flops: 15.46M\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm(\n",
      "      (512,), eps=1e-05, elementwise_affine=True\n",
      "      #params: 1.02K, #flops: 0.26M\n",
      "    )\n",
      "  )\n",
      "  (hidden_hidden): LSTM(\n",
      "    512, 512, batch_first=True\n",
      "    #params: 2.1M, #flops: 0\n",
      "  )\n",
      "  (inner_hidden_model): InnerHiddenModel(\n",
      "    (hidden_hidden): LSTM(512, 512, batch_first=True)\n",
      "  )\n",
      "  (linear): Linear(\n",
      "    in_features=512, out_features=302, bias=True\n",
      "    #params: 0.15M, #flops: 15.46M\n",
      "  )\n",
      ")\n",
      "\n",
      "| module                       | #parameters or shape   | #flops    |\n",
      "|:-----------------------------|:-----------------------|:----------|\n",
      "| model                        | 2.412M                 | 31.181M   |\n",
      "|  input_hidden                |  0.156M                |  15.718M  |\n",
      "|   input_hidden.0             |   0.155M               |   15.462M |\n",
      "|    input_hidden.0.weight     |    (512, 302)          |           |\n",
      "|    input_hidden.0.bias       |    (512,)              |           |\n",
      "|   input_hidden.2             |   1.024K               |   0.256M  |\n",
      "|    input_hidden.2.weight     |    (512,)              |           |\n",
      "|    input_hidden.2.bias       |    (512,)              |           |\n",
      "|  hidden_hidden               |  2.101M                |  0        |\n",
      "|   hidden_hidden.weight_ih_l0 |   (2048, 512)          |           |\n",
      "|   hidden_hidden.weight_hh_l0 |   (2048, 512)          |           |\n",
      "|   hidden_hidden.bias_ih_l0   |   (2048,)              |           |\n",
      "|   hidden_hidden.bias_hh_l0   |   (2048,)              |           |\n",
      "|  linear                      |  0.155M                |  15.462M  |\n",
      "|   linear.weight              |   (302, 512)           |           |\n",
      "|   linear.bias                |   (302,)               |           |\n",
      "\n",
      "\tParams: 2412334\n",
      "\n",
      "Activations: (81400, Counter({'linear': 81400}), Counter({'': 81400, 'input_hidden': 51200, 'input_hidden.0': 51200, 'linear': 30200, 'identity': 0, 'input_hidden.1': 0, 'input_hidden.2': 0, 'hidden_hidden': 0, 'inner_hidden_model': 0}), {'': Counter({'linear': 81400}), 'identity': Counter(), 'input_hidden': Counter({'linear': 51200}), 'input_hidden.0': Counter({'linear': 51200}), 'input_hidden.1': Counter(), 'input_hidden.2': Counter(), 'hidden_hidden': Counter(), 'inner_hidden_model': Counter(), 'linear': Counter({'linear': 30200})})\n",
      "\n",
      "\tParams: 2412334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Using fvcore\n",
    "\n",
    "# Adjust input based on if we use standard PyTorch model or custom model\n",
    "input = (input, mask) if mask is not None else input\n",
    "\n",
    "# Count the total and number of trainable parameters\n",
    "all_params_ct, train_params_ct = print_parameters(model)\n",
    "\n",
    "print(f\"\\nAll params: {all_params_ct}\\nTrainable params: {train_params_ct}\", end=\"\\n\\n\")\n",
    "\n",
    "# Perform FLOP Counting: Use the FlopCountAnalysis class to analyze your model:\n",
    "flops = FlopCountAnalysis(model, input)\n",
    "\n",
    "# Print Results: You can now print out the FLOPs and parameter information:\n",
    "print(\n",
    "    f\"FLOPs: {flops.total(), flops.by_operator(), flops.by_module(), flops.by_module_and_operator()}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(flop_count_str(flops), end=\"\\n\\n\")\n",
    "print(flop_count_table(flops), end=\"\\n\\n\")\n",
    "print(\n",
    "    f\"\\tParams: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# Perform Activations Counting: Use the ActivationCountAnalysis class to analyze your model:\n",
    "acts = ActivationCountAnalysis(model, input)\n",
    "\n",
    "# Print Results: You can now print out the FLOPs and parameter information:\n",
    "print(\n",
    "    f\"Activations: {acts.total(), acts.by_operator(), acts.by_module(), acts.by_module_and_operator()}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"\\tParams: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
