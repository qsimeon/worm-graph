{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils import DEVICE, init_random_seeds\n",
    "from models._utils import (\n",
    "    print_parameters,\n",
    "    NaivePredictor,\n",
    "    LinearRegression,\n",
    "    FeatureFFNN,\n",
    "    PureAttention,\n",
    "    NeuralTransformer,\n",
    "    NetworkLSTM,\n",
    "    NetworkCTRNN,\n",
    "    LiquidCfC,\n",
    ")\n",
    "\n",
    "# from fvcore.nn import (\n",
    "#     FlopCountAnalysis,\n",
    "#     ActivationCountAnalysis,\n",
    "#     flop_count_table,\n",
    "#     flop_count_str,\n",
    "# )\n",
    "\n",
    "\n",
    "# Initialize the random seeds\n",
    "init_random_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple PyTorch model: Linear(in_features=302, out_features=300, bias=True)\n",
      "\n",
      "Input: torch.Size([1, 100, 302]) \t Output: torch.Size([1, 100, 300])\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Custom model: PureAttention(\n",
      "  (identity): Identity()\n",
      "  (input_hidden): Sequential(\n",
      "    (0): Linear(in_features=302, out_features=300, bias=True)\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (hidden_hidden): SelfAttention(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (inner_hidden_model): InnerHiddenModel(\n",
      "    (hidden_hidden): SelfAttention(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (latent_embedding): Linear(in_features=302, out_features=300, bias=True)\n",
      "  (linear): Linear(in_features=300, out_features=302, bias=True)\n",
      "  (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Input: torch.Size([1, 100, 302]) \t Mask: torch.Size([1, 302]) \t Output: torch.Size([1, 100, 302])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Prepare model and input\n",
    "# @markdown Make sure the model and input are on the same device.\n",
    "\n",
    "# Set shapes for model and input\n",
    "seq_len = 100\n",
    "input_size = 302\n",
    "hidden_size = 300\n",
    "\n",
    "# Use a standard PyTorch model\n",
    "model = torch.nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # switch to eval mode\n",
    "print(f\"Simple PyTorch model: {model}\\n\")\n",
    "\n",
    "# Create input of the correct shape for the model\n",
    "input = torch.randn(1, seq_len, input_size).to(DEVICE)  # batch_size=1\n",
    "mask = None\n",
    "print(f\"Input: {input.shape} \\t Output: {model(input).shape}\", end=\"\\n\\n\")\n",
    "print(f\"\\n{'~'*100}\\n\")\n",
    "\n",
    "# Load one of our custom models instead\n",
    "model_args = dict(input_size=input_size, hidden_size=hidden_size, loss=\"MSE\")\n",
    "# model = NaivePredictor(**model_args)\n",
    "# model = LinearRegression(**model_args)\n",
    "# model = FeatureFFNN(**model_args)  # hidden_size = 516 -> num_params = 580286\n",
    "model = PureAttention(**model_args)  # hidden_size = 312 -> num_params = 580310\n",
    "# model = NeuralTransformer(**model_args)  # hidden_size = 196 -> num_params = 584186\n",
    "# model = NetworkLSTM(**model_args)  # hidden_size = 234 -> num_params = 582260\n",
    "# model = NetworkCTRNN(**model_args)  # hidden_size = 408 -> num_params = 582110\n",
    "# model = LiquidCfC(**model_args)  # hidden_size = 422 -> num_params = 582368\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # switch to eval mode\n",
    "print(f\"Custom model: {model}\\n\")\n",
    "\n",
    "# Create input of the correct shape for the model\n",
    "input = torch.randn(1, seq_len, input_size).to(DEVICE)  # batch_size=1\n",
    "mask = torch.ones(1, input_size).to(bool).to(DEVICE)\n",
    "print(\n",
    "    f\"Input: {input.shape} \\t Mask: {mask.shape} \\t Output: {model(input, mask).shape}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::expand_as encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 6 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "Unsupported operator aten::div encountered 1 time(s)\n",
      "Unsupported operator aten::unflatten encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "hidden_hidden.attn.out_proj, layer_norm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All params: 543602\n",
      "Trainable params: 543602\n",
      "\n",
      "FLOP: (54120000, Counter({'linear': 54120000}), Counter({'': 54120000, 'hidden_hidden': 36000000, 'hidden_hidden.attn': 36000000, 'input_hidden': 9060000, 'input_hidden.0': 9060000, 'linear': 9060000, 'identity': 0, 'input_hidden.1': 0, 'input_hidden.1.dropout': 0, 'hidden_hidden.attn.out_proj': 0, 'inner_hidden_model': 0, 'layer_norm': 0}), {'': Counter({'linear': 54120000}), 'identity': Counter(), 'input_hidden': Counter({'linear': 9060000}), 'input_hidden.0': Counter({'linear': 9060000}), 'input_hidden.1': Counter(), 'input_hidden.1.dropout': Counter(), 'hidden_hidden': Counter({'linear': 36000000}), 'hidden_hidden.attn': Counter({'linear': 36000000}), 'hidden_hidden.attn.out_proj': Counter(), 'inner_hidden_model': Counter(), 'linear': Counter({'linear': 9060000}), 'layer_norm': Counter()})\n",
      "\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "PureAttention(\n",
      "  #params: 0.54M, #flops: 54.12M\n",
      "  (identity): Identity(#params: 0, #flops: N/A)\n",
      "  (input_hidden): Sequential(\n",
      "    #params: 90.9K, #flops: 9.06M\n",
      "    (0): Linear(\n",
      "      in_features=302, out_features=300, bias=True\n",
      "      #params: 90.9K, #flops: 9.06M\n",
      "    )\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (hidden_hidden): SelfAttention(\n",
      "    #params: 0.36M, #flops: 36M\n",
      "    (attn): MultiheadAttention(\n",
      "      #params: 0.36M, #flops: 36M\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(\n",
      "        in_features=300, out_features=300, bias=True\n",
      "        #params: 90.3K, #flops: N/A\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (inner_hidden_model): InnerHiddenModel(\n",
      "    (hidden_hidden): SelfAttention(\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (latent_embedding): Linear(in_features=302, out_features=300, bias=True)\n",
      "  (linear): Linear(\n",
      "    in_features=300, out_features=302, bias=True\n",
      "    #params: 90.9K, #flops: 9.06M\n",
      "  )\n",
      "  (layer_norm): LayerNorm(\n",
      "    (300,), eps=1e-05, elementwise_affine=True\n",
      "    #params: 0.6K, #flops: N/A\n",
      "  )\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "| module                                | #parameters or shape   | #flops   |\n",
      "|:--------------------------------------|:-----------------------|:---------|\n",
      "| model                                 | 0.544M                 | 54.12M   |\n",
      "|  input_hidden.0                       |  90.9K                 |  9.06M   |\n",
      "|   input_hidden.0.weight               |   (300, 302)           |          |\n",
      "|   input_hidden.0.bias                 |   (300,)               |          |\n",
      "|  hidden_hidden.attn                   |  0.361M                |  36M     |\n",
      "|   hidden_hidden.attn.in_proj_weight   |   (900, 300)           |          |\n",
      "|   hidden_hidden.attn.in_proj_bias     |   (900,)               |          |\n",
      "|   hidden_hidden.attn.out_proj         |   90.3K                |          |\n",
      "|    hidden_hidden.attn.out_proj.weight |    (300, 300)          |          |\n",
      "|    hidden_hidden.attn.out_proj.bias   |    (300,)              |          |\n",
      "|  linear                               |  90.902K               |  9.06M   |\n",
      "|   linear.weight                       |   (302, 300)           |          |\n",
      "|   linear.bias                         |   (302,)               |          |\n",
      "|  layer_norm                           |  0.6K                  |          |\n",
      "|   layer_norm.weight                   |   (300,)               |          |\n",
      "|   layer_norm.bias                     |   (300,)               |          |\n",
      "\n",
      "\tParams: 543602\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::expand_as encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 6 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "Unsupported operator aten::div encountered 1 time(s)\n",
      "Unsupported operator aten::unflatten encountered 1 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "hidden_hidden.attn.out_proj, layer_norm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations: (180200, Counter({'linear': 180200}), Counter({'': 180200, 'hidden_hidden': 120000, 'hidden_hidden.attn': 120000, 'linear': 30200, 'input_hidden': 30000, 'input_hidden.0': 30000, 'identity': 0, 'input_hidden.1': 0, 'input_hidden.1.dropout': 0, 'hidden_hidden.attn.out_proj': 0, 'inner_hidden_model': 0, 'layer_norm': 0}), {'': Counter({'linear': 180200}), 'identity': Counter(), 'input_hidden': Counter({'linear': 30000}), 'input_hidden.0': Counter({'linear': 30000}), 'input_hidden.1': Counter(), 'input_hidden.1.dropout': Counter(), 'hidden_hidden': Counter({'linear': 120000}), 'hidden_hidden.attn': Counter({'linear': 120000}), 'hidden_hidden.attn.out_proj': Counter(), 'inner_hidden_model': Counter(), 'linear': Counter({'linear': 30200}), 'layer_norm': Counter()})\n",
      "\n",
      "\tParams: 543602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Using fvcore\n",
    "\n",
    "# Adjust input based on if we use standard PyTorch model or custom model\n",
    "input = (input, mask) if mask is not None else input\n",
    "\n",
    "# Count the total and number of trainable parameters\n",
    "all_params_ct, train_params_ct = print_parameters(model)\n",
    "\n",
    "print(f\"\\nAll params: {all_params_ct}\\nTrainable params: {train_params_ct}\", end=\"\\n\\n\")\n",
    "\n",
    "### DEBUG: Find way to compute FLOP using Pytorch Profiler ###\n",
    "\n",
    "# # Perform FLOP Counting: Use the FlopCountAnalysis class to analyze your model:\n",
    "# flops = FlopCountAnalysis(model, input)\n",
    "\n",
    "# # Print Results: You can now print out the FLOP and parameter information:\n",
    "# print(\n",
    "#     f\"FLOP: {flops.total(), flops.by_operator(), flops.by_module(), flops.by_module_and_operator()}\",\n",
    "#     end=\"\\n\\n\",\n",
    "# )\n",
    "# print(flop_count_str(flops), end=\"\\n\\n\")\n",
    "# print(flop_count_table(flops), end=\"\\n\\n\")\n",
    "# print(\n",
    "#     f\"\\tParams: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\",\n",
    "#     end=\"\\n\\n\",\n",
    "# )\n",
    "\n",
    "# # Perform Activations Counting: Use the ActivationCountAnalysis class to analyze your model:\n",
    "# acts = ActivationCountAnalysis(model, input)\n",
    "\n",
    "# # Print Results: You can now print out the FLOP and parameter information:\n",
    "# print(\n",
    "#     f\"Activations: {acts.total(), acts.by_operator(), acts.by_module(), acts.by_module_and_operator()}\",\n",
    "#     end=\"\\n\\n\",\n",
    "# )\n",
    "# print(\n",
    "#     f\"\\tParams: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\",\n",
    "#     end=\"\\n\\n\",\n",
    "# )\n",
    "\n",
    "### DEBUG: Find way to compute FLOP using Pytorch Profiler ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
