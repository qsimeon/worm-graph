train:
  
  optimizer: SGD # which Pytorch optimizer to use; options: Adam, SGD, None (defaults to SGD)
  learn_rate: 0.01 # learning rate
  seq_len: 100 # length of time-series sequences to train
  k_splits: 2 # number of interleaved train/test splits per single worm data
  epochs: 1 # number of epochs (worm cohorts) the model will be trained for
  save_freq: 100 # frequency (in epochs) with which to save model checkpoints
  # constraint: epochs: int > 0  and  save_freq: int <= epochs: int
  reverse: false # if True reverse the order of generation of the samples (sequences)
  num_samples: 32 # the number of samples (sequences) per worm
  num_batches: 2 # the number of batches per worm
  # interpretation: larger num. batches <-> smaller batch size; smaller num. batches <-> more parallelization; 
  tau_in: 1 # offset of target sequence from input sequence that we wish to train with 
  tau_out: 1 # offset of target sequence from input sequence that we wish to make predictions with
  # constraint: tau_in: int > 0 or list[int > 0] and tau_out: int > 0
  smooth_data: true # if True use the smoothed calcium data as input
  shuffle: true # whether to shuffle samples from each worm
  
