train:
  learn_rate: 0.01 # learning rate
  seq_len: 100 # length of time-series sequences to train
  k_splits: 2 # number of interleaved train/test splits per single worm data
  epochs: 1 # number of epochs (worm cohorts) the model will be trained for
  save_freq: 100 # frequency (in epochs) with which to save model checkpoints
  # constraint: epochs: int > 0  &&  save_freq: int <= epochs: int
  smooth_data: true # if True use the smoothed calcium data as input
  reverse: false # if True reverse the order of generation of the samples (sequences)
  # interpretation: larger num. batches corresponds to smaller batch size;
  # thus, smaller num. batches corresponds to more parallelization; 
  # num_batches corresponds to the number of gradient updates per epoch
  num_batches: 4 # total number of batches per epoch (worm cohorts)
  # constraint: batch_size: int <= min(train_size: int, test_size: int)
  shuffle: true # whether to shuffle samples from each worm
  tau_in: [1, 10] # offset of target sequence from input sequence that we wish to train with 
  tau_out: 1 # offset of target sequence from input sequence that we wish to make predictions with
  # constraint: tau_in: int == 1, tau_out: int > 0
  optimizer: SGD # which Pytorch optimizer to use; options: Adam, SGD, None
