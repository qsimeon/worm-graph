train:
  optimizer: Adam # which Pytorch optimizer to use; options: null (defaults to SGD), SGD, Adam, AdamW, Adagrad, Adadelta, RMSprop
  learn_rate: 0.0001 # learning rate
  epochs: 20 # number of epochs (worm cohorts) the model will be trained for
  # constraint: epochs: int > 0 
  save_freq: 100000 # frequency (in epochs) with which to save model checkpoints
  # save_freq: 0 < int <= epochs
  seq_len: 120 # length of time-series sequences to train
  # constraint: seq_len: 0 < int < len(data) 
  k_splits: 2 # number of interleaved train/test splits per single worm data
  # constraint: 2 <= int <= 5; keep fixed k_splits: int = 2
  num_samples: 16 # the number of sampled sequences per worm
  # constraint: due to memory limitations, keep fixed num_samples: 0 < int <= 32
  num_batches: 1 # the number of batches per worm
  # constraint: due to memory limitations, keep fixed num_batches: int = 1
  # interpretation: larger num. batches <-> smaller batch size; smaller num. batches <-> larger batch size; 
  tau_in: 1 # offset of target sequence from input sequence that we wish to train with 
  # constraint: keep fixed at tau_in = 1
  # interpretation: using a list of tau_in values may help train a multi-timescale prediciton model
  shuffle_samples: false # whether to shuffle samples (sequences) from each worm
  reverse: false # if True reverse the order of generation of the samples (sequences)
