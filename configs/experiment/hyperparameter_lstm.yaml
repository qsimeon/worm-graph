# @package _global_
defaults:
  - override /submodule: [dataset, model, train, visualize]
  - override /hydra/sweeper: optuna # Use optuna for hyperparameter optimization
  - _self_

hydra:
  mode: MULTIRUN
  sweeper:
    sampler:
      seed: 42 # Seed for reproducibility
    study_name: hyperparameter_search # Name of the study (for optuna)
    direction: minimize # Minimize the metric
    n_trials: 20 # Number of trials to run
    n_jobs: 1 # Number of jobs to run in parallel
    params:
      # Hyperparameters to optimize
      submodule.train.optimizer: Adam, AdamW, Adagrad, RMSprop
      submodule.train.lr: 1e-3, 1e-4, 1e-5
      submodule.train.batch_size: 16, 32, 64
      submodule.dataset.for_training.seq_len: 60, 120, 240
      submodule.train.num_train_samples: 8, 16, 32
      submodule.model.hidden_size: 128, 256, 512

submodule:

  dataset:
    for_training:
      experimental_datasets: Kato2015
      num_named_neurons: all
      k_splits: 2
      num_val_samples: 16
      tau: 1
      reverse: false
      use_residual: false
      smooth_data: true
      use_this_train_dataset: null
      use_this_val_dataset: null

  model:
    type: NetworkLSTM
    input_size: 302
    num_layers: 1
    loss: MSE
    fft_reg_param: 0.0
    l1_reg_param: 0.0
    use_this_pretrained_model: null

  train:
    epochs: 10
    save_freq: 1000
    shuffle: true
    use_this_train_dataset: null
    use_this_val_dataset: null
    use_this_pretrained_model: null

    early_stopping:
      delta: 1e-4
      patience: 10

experiment:
  name: hyperparameter_search
  mode: MULTIRUN
  seed: 42
