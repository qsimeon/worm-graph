train:
  optimizer: AdamW # options: Adam, AdamW, Adadelta, Adagrad, RMSprop, SGD
  lr: 0.0001
  epochs: 300
  save_freq: 100
  batch_size: 64
  shuffle: true

  early_stopping:
    delta: 0 # Minimum change in validation loss to be considered an improvement.
    patience: 50 # Number of epochs to wait for improvement before stopping.