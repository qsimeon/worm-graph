{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Synthetic Datasets\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to generate simple sinusoidal synthetic datasets with a specified numbers of worms, named neurons, and other characteristics of real datasets.\n",
    "\n",
    "**Last update:** _24 April 2024_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries and helper functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from openai import OpenAI\n",
    "from omegaconf import OmegaConf\n",
    "from typing import Protocol, Union\n",
    "from scipy.integrate import odeint\n",
    "from data._utils import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from utils import NUM_TOKENS, NEURONS_302, init_random_seeds\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from preprocess._utils import smooth_data_preprocess, reshape_calcium_data\n",
    "\n",
    "# Initialize the random seeds\n",
    "init_random_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neural_signals(data, time_tensor, neuron_idx=None, yax_limit=True, suptitle=None):\n",
    "    assert isinstance(data, torch.Tensor), \"data must be a PyTorch tensor\"\n",
    "    assert isinstance(time_tensor, torch.Tensor), \"time_tensor must be a PyTorch tensor\"\n",
    "    assert data.dim() == 2, \"data must be a 2D tensor\"\n",
    "    assert isinstance(neuron_idx, (int, list)), \"neuron_idx must be an integer or list\"\n",
    "\n",
    "    time_tensor = time_tensor.squeeze()\n",
    "    assert data.size(0) == time_tensor.size(0), \"Number of rows in data and time_tensor must match\"\n",
    "\n",
    "    num_neurons = data.size(1)\n",
    "\n",
    "    # Randomly select the column indices if not provided\n",
    "    if isinstance(neuron_idx, int):\n",
    "        assert neuron_idx <= num_neurons, \"neuron_idx cannot exceed the number of neurons\"\n",
    "        column_indices = np.random.choice(num_neurons, neuron_idx, replace=False)\n",
    "    elif isinstance(neuron_idx, list):\n",
    "        assert len(neuron_idx) <= num_neurons, \"neuron_idx cannot exceed the number of neurons\"\n",
    "        column_indices = np.array(neuron_idx)\n",
    "\n",
    "    num_columns = len(column_indices)\n",
    "\n",
    "    # Extract the selected columns from the data tensor\n",
    "    selected_columns = data[:, column_indices]\n",
    "\n",
    "    # Define the color palette using scientific colors\n",
    "    colors = sns.color_palette(\"bright\", num_columns)\n",
    "\n",
    "    # Plotting subplots vertically\n",
    "    fig, axs = plt.subplots(num_columns, 1, figsize=(15, num_columns))\n",
    "    fig.tight_layout(pad=0.0)\n",
    "\n",
    "    # If num_columns is 1, make ax iterable by wrapping it in a list\n",
    "    if num_columns == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Now your existing loop should work without modification\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.plot(time_tensor, selected_columns[:, i], color=colors[i])\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position(\"none\")\n",
    "        if yax_limit:\n",
    "            ax.set_ylim(-1.0, 1.0)\n",
    "        ax.set_ylabel(\"{}\".format(NEURONS_302[column_indices[i]]))\n",
    "\n",
    "        if i < num_columns - 1:\n",
    "            ax.set_xticks([])\n",
    "        else:\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "\n",
    "    # Add a super title to the figure if provided\n",
    "    if suptitle is not None:\n",
    "        fig.suptitle(suptitle, fontsize=16)\n",
    "\n",
    "    plt.tight_layout(pad=1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_trajectory(X, axis_labels=(\"Time\", \"Value\", \"Z Axis\"), title=\"Trajectory\", show=True):\n",
    "    \"\"\"\n",
    "    Plot a trajectory from a dataset, which can be 1D, 2D, or 3D.\n",
    "\n",
    "    Parameters:\n",
    "    - X: A 2D numpy array containing the trajectory data.\n",
    "    - axis_labels: A tuple containing the labels for the axes. Default is ('Time', 'Value', 'Z Axis').\n",
    "    - title: Title of the plot.\n",
    "    - show: If True, the plot will be displayed. If False, the plot object will be returned.\n",
    "\n",
    "    Returns:\n",
    "    - fig, ax: The figure and axis objects of the plot if show is False.\n",
    "    \"\"\"\n",
    "    max_timesteps = X.shape[0]\n",
    "    dims = X.shape[1] if len(X.shape) > 1 else 1\n",
    "\n",
    "    # Create a new figure for the plot\n",
    "    fig = plt.figure()\n",
    "    if dims >= 3:\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        plot_dims = 3\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "        plot_dims = dims\n",
    "\n",
    "    # Create a color map based on the time progression\n",
    "    norm = plt.Normalize(0, max_timesteps)\n",
    "    colors = plt.cm.viridis(norm(np.arange(max_timesteps)))\n",
    "\n",
    "    # Extract coordinates from the data and plot accordingly\n",
    "    if plot_dims == 1:\n",
    "        x = np.arange(max_timesteps)\n",
    "        y = X.flatten()\n",
    "        z = np.zeros(max_timesteps)\n",
    "        ax_labels = (\"Time\", axis_labels[0], \"Fixed Z\")\n",
    "        # Plot the 1D trajectory with a color gradient\n",
    "        for i in range(1, max_timesteps):\n",
    "            ax.plot(x[i - 1 : i + 1], y[i - 1 : i + 1], color=colors[i], lw=0.5)\n",
    "        # Mark the start and end of the trajectory\n",
    "        ax.plot(x[0], y[0], \"g*\", markersize=8)  # Start with a green star\n",
    "        ax.plot(x[-1], y[-1], \"ro\", markersize=7)  # End with a red circle\n",
    "    elif plot_dims == 2:\n",
    "        x, y = X[:, 0], X[:, 1]\n",
    "        z = np.zeros(max_timesteps)\n",
    "        ax_labels = (axis_labels[0], axis_labels[1], \"Fixed Z\")\n",
    "        # Plot the 2D trajectory\n",
    "        for i in range(1, max_timesteps):\n",
    "            ax.plot(x[i - 1 : i + 1], y[i - 1 : i + 1], color=colors[i], lw=0.5)\n",
    "        # Mark the start and end of the trajectory\n",
    "        ax.plot(x[0], y[0], \"g*\", markersize=8)  # Start with a green star\n",
    "        ax.plot(x[-1], y[-1], \"ro\", markersize=7)  # End with a red circle\n",
    "    else:  # plot_dims == 3\n",
    "        x, y, z = X[:, 0], X[:, 1], X[:, 2]\n",
    "        ax_labels = axis_labels\n",
    "        # Plot the 3D trajectory\n",
    "        for i in range(1, max_timesteps):\n",
    "            ax.plot(\n",
    "                x[i - 1 : i + 1],\n",
    "                y[i - 1 : i + 1],\n",
    "                z[i - 1 : i + 1],\n",
    "                color=colors[i],\n",
    "                lw=0.5,\n",
    "            )\n",
    "        # Mark the start and end of the trajectory\n",
    "        ax.plot(x[0], y[0], z[0], \"g*\", markersize=8)  # Start with a green star\n",
    "        ax.plot(x[-1], y[-1], z[-1], \"ro\", markersize=7)  # End with a red circle\n",
    "\n",
    "    # Set labels for the axes\n",
    "    ax.set_xlabel(ax_labels[0])\n",
    "    ax.set_ylabel(ax_labels[1])\n",
    "    if plot_dims == 3:\n",
    "        ax.set_zlabel(ax_labels[2])\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autocorrelation_and_pacf(X, neurons):\n",
    "    \"\"\"\n",
    "    Plot the autocorrelation and partial autocorrelation for each neuron's trajectory.\n",
    "\n",
    "    Parameters:\n",
    "    - X: A 2D numpy array of shape (max_timesteps, num_neurons) containing the neural trajectory data.\n",
    "    - neurons: A list or array containing the neuron identifiers.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function creates and displays a plot.\n",
    "    \"\"\"\n",
    "    # Number of neurons\n",
    "    num_neurons = X.shape[1]\n",
    "\n",
    "    # Create a figure and an array of subplots with 2 columns\n",
    "    fig, axes = plt.subplots(nrows=num_neurons, ncols=2, figsize=(10, 2 * num_neurons))\n",
    "\n",
    "    # Set a suptitle for the entire figure\n",
    "    fig.suptitle(\"Autocorrelation and Partial Autocorrelation for Neurons\")\n",
    "\n",
    "    # Iterate over the number of neurons to create individual plots\n",
    "    for i in range(num_neurons):\n",
    "        # Plot ACF on the left column (0th index)\n",
    "        ax_left = axes[i, 0] if num_neurons > 1 else axes[0]\n",
    "        plot_acf(X[:, i], ax=ax_left, use_vlines=False, marker=\"v\", linestyle=\"--\")\n",
    "        ax_left.set_title(f\"Neuron {neurons[i]} ACF\")\n",
    "\n",
    "        # Plot PACF on the right column (1st index)\n",
    "        ax_right = axes[i, 1] if num_neurons > 1 else axes[1]\n",
    "        plot_pacf(X[:, i], ax=ax_right, use_vlines=False, marker=\"*\", linestyle=\"--\")\n",
    "        ax_right.set_title(f\"Neuron {neurons[i]} PACF\")\n",
    "        ax_right.set_ylabel(\"\")  # Clear the y-axis label to prevent clutter\n",
    "\n",
    "        # Adjust y-axis limits so there is some white space around -1 and 1\n",
    "        ax_left.set_ylim(-1.1, 1.1)\n",
    "        ax_right.set_ylim(-1.1, 1.1)\n",
    "\n",
    "        # Only the bottom plots need x-axis labels\n",
    "        if i < num_neurons - 1:\n",
    "            ax_left.set_xlabel(\"\")\n",
    "            ax_right.set_xlabel(\"\")\n",
    "        else:\n",
    "            ax_left.set_xlabel(\"Lag\")\n",
    "            ax_right.set_xlabel(\"Lag\")\n",
    "\n",
    "    # Adjust the layout to not overlap plots\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    # Show the plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_delay_embedding(x, delay, dimension):\n",
    "    \"\"\"\n",
    "    Constructs a time-delay embedding matrix from time series data.\n",
    "\n",
    "    :param x: Time series data as a 1D numpy array.\n",
    "    :param delay: The delay tau between time series elements in the embedding.\n",
    "    :param dimension: The embedding dimension m.\n",
    "    :return: The time-delay embedded data as a 2D numpy array.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if n < (dimension - 1) * delay + 1:\n",
    "        raise ValueError(\"Time series data is too short for the given delay and dimension.\")\n",
    "    m = n - (dimension - 1) * delay\n",
    "    embedded_data = np.empty((m, dimension))\n",
    "    for i in range(m):\n",
    "        for j in range(dimension):\n",
    "            embedded_data[i, j] = x[i + j * delay]\n",
    "    return embedded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz(X, t, sigma=10.0, beta=8.0 / 3, rho=28.0):\n",
    "    \"\"\"The Lorenz equations.\"\"\"\n",
    "    x, y, z = X\n",
    "    dx_dt = sigma * (y - x)\n",
    "    dy_dt = x * (rho - z) - y\n",
    "    dz_dt = x * y - beta * z\n",
    "    return [dx_dt, dy_dt, dz_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def van_der_pol_oscillator(X, t, mu=1.0):\n",
    "    \"\"\"The Van der Pol oscillator equations.\"\"\"\n",
    "    x, y = X\n",
    "    dx_dt = y\n",
    "    dy_dt = mu * (1 - x**2) * y - x\n",
    "    return [dx_dt, dy_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidsOscillator:\n",
    "    def __init__(self, sum_freqs=0, random=True):\n",
    "        self.eps = np.finfo(float).eps\n",
    "        # the number of frequencies sum composing a frequency fingerprint\n",
    "        self.sum_freqs = sum_freqs\n",
    "        self.random = random\n",
    "        # create frequencies and phases\n",
    "        if self.random:  # use random frequencies\n",
    "            self.freqs = np.round(self.eps + np.random.random(self.sum_freqs) / 10, 4)\n",
    "        else:  # use deterministic frequencies\n",
    "            self.freqs = np.round(\n",
    "                np.arange(1 / (2 * self.sum_freqs + self.eps), self.sum_freqs)\n",
    "                / (10 * self.sum_freqs),\n",
    "                4,\n",
    "            )\n",
    "        # always use random phases\n",
    "        self.phases = np.random.random(self.sum_freqs) * 2 * np.pi\n",
    "\n",
    "    def __call__(self, Y, t):\n",
    "        y = Y\n",
    "        # the leak term -y ensures convergence to a fixed point if no external input is provided\n",
    "        dy_dt = -y + sum(np.cos(2 * np.pi * f * t + p) for f, p in zip(self.freqs, self.phases))\n",
    "        return dy_dt\n",
    "\n",
    "\n",
    "# create an instance of a SinusoidsOscillator dynamical system\n",
    "sinusoids_oscillator = SinusoidsOscillator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_synthetic_dataset(file_name, dataset):\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spectral_radius(matrix):\n",
    "    eigenvalues = np.linalg.eigvals(matrix)\n",
    "    spectral_radius = max(abs(eigenvalues))\n",
    "    return spectral_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_matrix_to_edge_of_chaos(matrix, target_radius=1.0):\n",
    "    # Calculate the current spectral radius\n",
    "    current_radius = calculate_spectral_radius(matrix)\n",
    "\n",
    "    # Calculate the gain needed to adjust the spectral radius to the target\n",
    "    gain = target_radius / current_radius\n",
    "\n",
    "    # Scale the matrix by the gain\n",
    "    adjusted_matrix = matrix * gain\n",
    "\n",
    "    return adjusted_matrix, gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_tokenize_and_chunk(texts, max_length=510):\n",
    "    \"\"\"\n",
    "    Pre-tokenize and chunk the text into segments of a specified maximum length.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of strings.\n",
    "        max_length (int): Maximum length of each chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    joined_text = \" \".join(texts)\n",
    "    pre_tokenizer = WhitespaceSplit()\n",
    "    pre_tokens = pre_tokenizer.pre_tokenize_str(joined_text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for token, (start, end) in pre_tokens:\n",
    "        if len(current_chunk) + len(token) + 1 > max_length:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = token\n",
    "        else:\n",
    "            current_chunk += \" \" + token if current_chunk else token\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(texts, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize and chunk texts.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): Input texts.\n",
    "        tokenizer: Tokenizer instance.\n",
    "        max_length (int): Max length (in chars) string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing text and token ids.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    input_ids = []\n",
    "    chunks = pre_tokenize_and_chunk(texts, max_length - 2)  # adjust for special tokens\n",
    "    for chunk in chunks:\n",
    "        text.append(chunk) # each chunk is a string shorter than max_length characters\n",
    "        tokens = tokenizer.encode(chunk) # returns a list of integers\n",
    "        input_ids.append(tokens) \n",
    "    text_and_tokens = {\"text\": text, \"input_ids\": input_ids}\n",
    "    return text_and_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\", dimensions=NUM_TOKENS):\n",
    "    \"\"\"\n",
    "    We can ue OpenAI's embeddings API to get embeddings directly from text to embeddings.\n",
    "    This is shorter than tokenizing the text first followed by using an embedding layer.\n",
    "    The OpenAI Embeddings model directly converts an entire string (i.e. chunk of text) \n",
    "    into a single embedding vector.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    if isinstance(text, list):\n",
    "        assert isinstance(text[0], str), \"List of strings expected.\"\n",
    "        text = [string.replace(\"\\n\", \" \") for string in text]\n",
    "        input = text\n",
    "    else:\n",
    "        assert isinstance(text, str), \"String expected.\"\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        input = [text]\n",
    "    data = client.embeddings.create(input=input, model=model, dimensions=dimensions).data\n",
    "    result = [np.array(d.embedding) for d in data] # list of array; each array is an embedding vector\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some tokenizers to experiment with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKENIZER = AutoTokenizer.from_pretrained(\"google/byt5-large\")\n",
    "BASE_TOKENIZER = tiktoken.get_encoding(\"cl100k_base\")\n",
    "GPT4_TOKENIZER = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "class TokenizerProtocol(Protocol):\n",
    "    def encode(self, text: str) -> dict:\n",
    "        ...\n",
    "\n",
    "# # ##### DEBUG: Alternatively we can go straight from text chunks to embeddings using OpenAI's Embeddings model #####\n",
    "tokenizer = BASE_TOKENIZER\n",
    "text_dataset = load_hf_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "text_dataset = tokenize_and_chunk(text_dataset[\"train\"][\"text\"], tokenizer)\n",
    "chunks = pre_tokenize_and_chunk(text_dataset[\"text\"], max_length=64)\n",
    "embdeddings = get_embedding(chunks[:10]) # restrict to 10 API calls\n",
    "print()\n",
    "print(f\"embeddings array: {np.array(embdeddings).shape}\\n\")\n",
    "print(f\"embeddings tensor: {torch.tensor(embdeddings).shape}\\n\")\n",
    "# # ##################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get default parameter values from configs and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"../configs/submodule/preprocess.yaml\")\n",
    "DELTA_T = config.preprocess.resample_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikitext dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_wikitext(\n",
    "    max_timesteps: int = 1000,\n",
    "    num_worms: int = 1,\n",
    "    num_signals: int = 302,\n",
    "    num_named_neurons: Union[None, int] = None,\n",
    "    add_noise: bool = False,\n",
    "    noise_std: float = 0.01,\n",
    "    random_walk: bool = False,\n",
    "    use_openai: bool = False,\n",
    "    tokenizer: Union[None, TokenizerProtocol] = None, \n",
    "    delta_seconds: float = DELTA_T,\n",
    "    smooth_method: Union[None, str] = \"ES\",\n",
    "    transform: Union[None, callable] = StandardScaler(),\n",
    "    dataset_name: str = \"Wikitext0000\",\n",
    "):\n",
    "    \"\"\"\n",
    "    TODO: Write a comprehensive docstring.\n",
    "    The main idea here is to treat the embeddings of a 1D sequence of tokens  \n",
    "    as if it was a multi-dimensional time series of neural activity.\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    dataset = {}\n",
    "    # Determine the timepoints for sampling the data (i.e. \"measurement\" times)\n",
    "    time_in_seconds = delta_seconds * np.arange(max_timesteps).reshape(-1, 1)  # column vector\n",
    "    # Calculate number of named and unknown neurons\n",
    "    if num_named_neurons is None or num_named_neurons > num_signals:  # default to all neurons\n",
    "        num_named_neurons = num_signals\n",
    "    elif num_named_neurons < 0:  # default to no neurons\n",
    "        num_named_neurons = 0\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "    # Initializae the tokenizer and set vocab_size attribute \n",
    "    if tokenizer is None:\n",
    "        tokenizer = BASE_TOKENIZER\n",
    "    if isinstance(tokenizer, tiktoken.core.Encoding):\n",
    "        tokenizer.vocab_size = tokenizer.n_vocab\n",
    "    # Load the WikiText dataset\n",
    "    text_dataset = load_hf_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "    # Process and tokenize just the train split\n",
    "    text_dataset = tokenize_and_chunk(text_dataset[\"train\"][\"text\"], tokenizer, max_length=256)\n",
    "    # Create a fixed embedding map from tokens to embedding vectors\n",
    "    embed_dim = num_signals\n",
    "    # Random initialization of embeddings using normal distribution\n",
    "    embedding_weights = torch.randn(tokenizer.vocab_size, embed_dim)\n",
    "    torch.nn.init.normal_(embedding_weights, mean=0.0, std=1.0)\n",
    "    num_tokens = embedding_weights.shape[0]\n",
    "    assert isinstance(embedding_weights, torch.Tensor) and embedding_weights.shape == (\n",
    "        num_tokens,\n",
    "        embed_dim,\n",
    "    ), f\"embedding_weight must be a PyTorch tensor of shape ({num_tokens}, {embed_dim})\"\n",
    "    # Create fixed embedding layer / table \n",
    "    embedding = torch.nn.Embedding(\n",
    "        num_embeddings=num_tokens,\n",
    "        embedding_dim=embed_dim,\n",
    "        dtype=torch.half,\n",
    "        _weight=embedding_weights,\n",
    "        _freeze=True,\n",
    "    )\n",
    "    # Create combined dataset of the embedded token sequences from the train split\n",
    "    if use_openai:\n",
    "        text_chunks = pre_tokenize_and_chunk(text_dataset[\"text\"], max_length=64)\n",
    "        max_size = 1 + max_timesteps * num_worms # restricts the number of API calls\n",
    "        combo_embd_chunks = get_embedding(text_chunks[:max_size], dimensions=embed_dim)\n",
    "    else:\n",
    "        combo_embd_chunks = [ embedding(torch.LongTensor(sequence)).detach().numpy() for sequence in text_dataset[\"input_ids\"] ]\n",
    "    # Create data for as many worms as possible if not enough data\n",
    "    worm_idx = 0\n",
    "    calcium_data = [] # will be shaped (time, neurons)\n",
    "    total_time = 0\n",
    "    # Keep growing a sequence of embeddings until we reach the maximum timesteps\n",
    "    for chunk in combo_embd_chunks:\n",
    "        # Reshape to a row vector if necessary\n",
    "        if chunk.ndim == 1:\n",
    "            chunk = chunk.reshape(1,-1)\n",
    "        # If we have enough worms break out of the loop\n",
    "        if worm_idx >= num_worms:\n",
    "            break\n",
    "        # Otherwise continue extending the calcium data\n",
    "        calcium_data.append(chunk)\n",
    "        total_time += chunk.shape[0]\n",
    "        # Preprocess calcium data once enough timesteps have been created\n",
    "        if total_time >= max_timesteps:\n",
    "            calcium_data = np.vstack(calcium_data)[:max_timesteps]\n",
    "            # Choose a random subset of neurons to record / observe / measure\n",
    "            named_neuron_indices = random.sample(\n",
    "                range(num_signals), num_named_neurons\n",
    "            )  # without replacement\n",
    "            named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "            # Create neuron to idx mapping and vice versa\n",
    "            neuron_to_idx = {\n",
    "                (neuron) if neuron in named_neurons else str(idx): idx\n",
    "                for idx, neuron in enumerate(NEURONS_302)\n",
    "            }\n",
    "            idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "            # Add noise\n",
    "            if add_noise:\n",
    "                if random_walk:\n",
    "                    noise_walk = np.cumsum(\n",
    "                        [0]\n",
    "                        + np.random.normal(loc=0, scale=noise_std, size=max_timesteps - 1).tolist()\n",
    "                    )\n",
    "                    calcium_data += noise_walk\n",
    "                else:\n",
    "                    noise_iid = np.random.normal(0, noise_std, max_timesteps)\n",
    "                    calcium_data += noise_iid\n",
    "            # Zero out non-recorded / \"unobserved\" neurons\n",
    "            unknown_neuron_indices = np.array(\n",
    "                [idx for idx in range(num_signals) if idx not in named_neuron_indices]\n",
    "            )\n",
    "            calcium_data[:, unknown_neuron_indices] *= 0.0\n",
    "            # Normalize data\n",
    "            if transform:\n",
    "                calcium_data = transform.fit_transform(calcium_data)\n",
    "            # Calculate residuals\n",
    "            dt = np.diff(time_in_seconds, axis=0, prepend=0.0)\n",
    "            resample_dt = np.median(dt[1:]).item()\n",
    "            residual_calcium = np.gradient(calcium_data, time_in_seconds.squeeze(), axis=0)\n",
    "            # Smooth the data\n",
    "            smooth_calcium_data = smooth_data_preprocess(\n",
    "                calcium_data,\n",
    "                time_in_seconds,\n",
    "                smooth_method,\n",
    "                **dict(alpha=0.5, window_size=15, sigma=5),\n",
    "            )\n",
    "            smooth_residual_calcium = smooth_data_preprocess(\n",
    "                residual_calcium,\n",
    "                time_in_seconds,\n",
    "                smooth_method,\n",
    "                **dict(alpha=0.5, window_size=15, sigma=5),\n",
    "            )\n",
    "            # Initialize worm data\n",
    "            worm = f\"worm{worm_idx}\"\n",
    "            worm_data = dict()\n",
    "            # Save the data\n",
    "            worm_data[\"worm\"] = worm\n",
    "            worm_data[\"source_dataset\"] = dataset_name\n",
    "            worm_data[\"smooth_method\"] = smooth_method\n",
    "            worm_data[\"calcium_data\"] = calcium_data\n",
    "            worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "            worm_data[\"residual_calcium\"] = residual_calcium\n",
    "            worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "            worm_data[\"max_timesteps\"] = max_timesteps\n",
    "            worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "            worm_data[\"dt\"] = dt\n",
    "            worm_data[\"median_dt\"] = resample_dt\n",
    "            worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "            worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "            worm_data[\"num_neurons\"] = num_signals\n",
    "            worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "            worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "            worm_data[\"extra_info\"] = {\n",
    "                \"embedding_weights\": embedding_weights,\n",
    "                \"num_tokens\": num_tokens,\n",
    "                \"use_openai\": use_openai,\n",
    "                \"meta_text\": \"This dataset simulates neural activity data as the embedding vectors of language tokens using some fixed, \"\n",
    "                \"pre-specified embedding table given by `embedding_weights`. If `use_openai` is True, then the OpenAI Embeddings model \"\n",
    "                \"was used to generate the emebeddings rather than the `embedding_weights`.\\n\",\n",
    "            }\n",
    "            # Reshape the data to the standardized format\n",
    "            worm_data = reshape_calcium_data(worm_data)\n",
    "            # Save the data\n",
    "            dataset[worm] = worm_data\n",
    "            # Increment the worm index and reset the calcium data and timestep counter\n",
    "            worm_idx += 1\n",
    "            calcium_data = []\n",
    "            total_time = 0\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "max_timesteps = 1000\n",
    "num_worms = 300\n",
    "num_signals = 302\n",
    "num_named_neurons = 100\n",
    "add_noise = False\n",
    "noise_std = 0.0\n",
    "random_walk = False\n",
    "use_openai = False\n",
    "tokenizer = BASE_TOKENIZER \n",
    "delta_seconds = DELTA_T\n",
    "smooth_method = None\n",
    "transform = StandardScaler()\n",
    "dataset_name = \"Wikitext0000\"\n",
    "\n",
    "# Creating and saving datasets\n",
    "dataset = create_synthetic_dataset_wikitext(\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_named_neurons=num_named_neurons,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    use_openai=use_openai,\n",
    "    tokenizer=tokenizer,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Get the number of worms in the dataset\n",
    "num_worms = len(dataset)\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "num_worms = len(dataset)\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][:num_named_neurons]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key insights\n",
    "\n",
    "What if real calcium fluorescence signals (our proxy for neural activity/state) that we measure from neurons are just smoothed (i.e low-pass filtered) embeddings of some underlying but unknown 1-D token sequence? \n",
    "\n",
    "What would be the interpetation of this 1-D sequence? - could this be \"behavior\"; some more general notion of \"state\" that is finite (i.e fixed vocabiulary size)? \n",
    "\n",
    "Can we invert the embeddings to figure out what this 1-D sequence was? Notice there is no smooth transition in the embeddings when viewed as a time series. This is because there is no semantic relationship between consecutive embeddings vectors since we just initialized them randomly. \n",
    "\n",
    "Does training of embeddings make their dynamics smooth?\n",
    "\n",
    "TODO: Look at `EmbeddingsOpenAI.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Network dataset\n",
    "\n",
    "Dynamics evolve according to\n",
    "\n",
    "$$ \\tau \\frac{d\\mathbf{x}}{dt} = -\\mathbf{x} + \\mathbf{M} f(\\mathbf{x}) + \\mathbf{b} $$\n",
    "\n",
    "where $\\mathbf{b}$ is a vector of external inputs and $\\mathbf{M}$ is a connectivity matrix. $\\mathbf{M}$ is a sparse random graph with non-zero weights chosen i.i.d $\\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "We define $\\alpha = \\frac{\\Delta t}{\\tau}$. The discrete-time update equation of the RNN is given by\n",
    "\n",
    "$$ \\mathbf{x}(t + \\Delta t) = \\left(1 - \\alpha\\right) \\mathbf{x}(t) + \\alpha \\mathbf{M} f(\\mathbf{x}) + \\alpha \\mathbf{b}(t) $$\n",
    "\n",
    "**Considerations:**\n",
    "- The time constant $\\tau$ may be diferent for different neurons in the network, in which case $\\mathbf{\\tau}$ should be viewed as a vector $\\mathbf{\\tau}$. Furthermore, $\\tau$ could be time-dependent $\\tau(t)$, which may be due to neuromodulation.\n",
    "- For our simulations, and also with real experimental data, we generally know (or can resample) the sampling interval (i.e. measurement timestep) $\\Delta t$. But since we don't know (or don't have good estimates of) the time constant $\\tau$, this makes $\\alpha$ unknown. \n",
    "- However, a common assumption from rate-based modeling is that $\\Delta t$ is much shorter $\\tau$.\n",
    "- Therefore $\\alpha$  (or $\\mathbf{\\alpha}$ in the vectorized case) is non-negative and bounded $0 < \\alpha <1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_recurrent(\n",
    "    max_timesteps: int = 1000,\n",
    "    num_worms: int = 1,\n",
    "    num_signals: int = 302,\n",
    "    num_named_neurons: Union[None, int] = None,\n",
    "    add_noise: bool = False,\n",
    "    noise_std: float = 0.01,\n",
    "    random_walk: bool = False,\n",
    "    # <<< any special arguments for this function should go here >>>\n",
    "    delta_seconds: float = DELTA_T,\n",
    "    smooth_method: Union[None, str] = \"ES\",\n",
    "    transform: Union[None, callable] = StandardScaler(),\n",
    "    dataset_name: str = \"Recurrent0000\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a synthetic worm datasets using the Lorenz attractor.\n",
    "    Three neurons are chosen randomly to represent x, y, z trajectories from the Lorenz system.\n",
    "\n",
    "    :param max_timesteps: The number of timepoints of synthetic data to generate.\n",
    "    :param num_worms: The number of synthetic worms to create datasets for.\n",
    "    :param num_signals: The number of signals corresponding to number of neurons.\n",
    "    :param num_named_neurons: The number of named neurons to create non-zero signals for.\n",
    "    :param add_noise: Whether to add Gaussian noise to the synthetic data.\n",
    "    :param noise_std: The standard deviation of the noise.\n",
    "    :param random_walk: If True, use a random walk to generate the noise. Otherwise, use iid noise.\n",
    "    :param delta_seconds: The constant time difference (in seconds) between each measurement of the system.\n",
    "    :param smooth_method: The method to use for smoothing the data.\n",
    "    :param transform: The sklearn method to scale or transform the data before use.\n",
    "    :param dataset_name: The name to give the synthetic dataset.\n",
    "    :return: A dictionary containing the synthetic worm datasets.\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    dataset = {}\n",
    "    # Determine the timepoints for sampling the data (i.e. \"measurement\" times)\n",
    "    time_in_seconds = delta_seconds * np.arange(max_timesteps).reshape(-1, 1)  # column vector\n",
    "    # Calculate number of named and unknown neurons\n",
    "    if num_named_neurons is None or num_named_neurons > num_signals:  # default to all neurons\n",
    "        num_named_neurons = num_signals\n",
    "    elif num_named_neurons < 0:  # default to no neurons\n",
    "        num_named_neurons = 0\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "    # Define a fixed minimal connectivity matrix which ensures that the network is connected\n",
    "    sparsity = 0.0\n",
    "    connected = False\n",
    "    while not connected:\n",
    "        sparsity += 1 / num_signals\n",
    "        sparse_mask = np.random.choice(\n",
    "            [0, 1], size=(num_signals, num_signals), p=[1 - sparsity, sparsity]\n",
    "        )\n",
    "        # Ensure there are autapse connection to ensure the connectivity matrix is full rank\n",
    "        sparse_mask[np.diag_indices(num_signals)] = 1\n",
    "        G = nx.from_numpy_array(sparse_mask)\n",
    "        connected = nx.is_connected(G)\n",
    "    connectivity_matrix = sparse_mask * np.random.randn(num_signals, num_signals)\n",
    "    # Adjust the connectivity matrix to the edge of chaos\n",
    "    target_radius = 1.0\n",
    "    connectivity_matrix, _ = adjust_matrix_to_edge_of_chaos(connectivity_matrix, target_radius)\n",
    "    # Evolve the dynamics specified by the fixed connectivity and prespecified time constants\n",
    "    alphas = np.random.rand(num_signals) + eps #np.ones(num_signals)  # inverse time constants\n",
    "    # Specify a small input gain\n",
    "    inp_gain = 1 / np.sqrt(sparsity * num_signals)\n",
    "    # Some warmup timesteps to allow the system to reach a steady state\n",
    "    warmup_timesteps = max_timesteps // 2\n",
    "    simulation_steps = max_timesteps + warmup_timesteps\n",
    "    # Create data for each worm\n",
    "    for worm_idx in range(num_worms):\n",
    "        # Initialize worm data\n",
    "        worm = f\"worm{worm_idx}\"\n",
    "        worm_data = dict()\n",
    "        calcium_data = np.zeros((max_timesteps + warmup_timesteps, num_signals))\n",
    "        # Choose a random subset of neurons to record / observe / measure\n",
    "        named_neuron_indices = random.sample(\n",
    "            range(num_signals), num_named_neurons\n",
    "        )  # without replacement\n",
    "        named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "        # Create neuron to idx mapping and vice versa\n",
    "        neuron_to_idx = {\n",
    "            (neuron) if neuron in named_neurons else str(idx): idx\n",
    "            for idx, neuron in enumerate(NEURONS_302)\n",
    "        }\n",
    "        idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "        # Create calcium data\n",
    "        for t in range(simulation_steps):\n",
    "            # Initial conditions\n",
    "            if t == 0:\n",
    "                state = np.random.randn(num_signals)  # initial values\n",
    "                calcium_data[t][named_neuron_indices] = state[named_neuron_indices]\n",
    "            # Evolve recurrent dynamics\n",
    "            else:\n",
    "                # Specify the input to apply at each neuron for this timestep\n",
    "                inputs =  inp_gain * np.random.randn(num_signals)  # i.i.d inp_gain * N(0, 1) inputs; vector w/ shape (num_signals, )\n",
    "                state = (1 - alphas) * state + alphas * (\n",
    "                    connectivity_matrix @ np.tanh(state) + inputs\n",
    "                )\n",
    "                calcium_data[t][named_neuron_indices] = state[named_neuron_indices]\n",
    "        # Discard warmup timesteps\n",
    "        calcium_data = calcium_data[warmup_timesteps:]\n",
    "        # Add noise\n",
    "        for neuron_index in named_neuron_indices:\n",
    "            if add_noise:\n",
    "                if random_walk:\n",
    "                    noise_walk = np.cumsum(\n",
    "                        [0]\n",
    "                        + np.random.normal(loc=0, scale=noise_std, size=max_timesteps - 1).tolist()\n",
    "                    )\n",
    "                    calcium_data[:, neuron_index] += noise_walk\n",
    "                else:\n",
    "                    noise_iid = np.random.normal(0, noise_std, max_timesteps)\n",
    "                    calcium_data[:, neuron_index] += noise_iid\n",
    "        # Normalize the data\n",
    "        if transform:\n",
    "            calcium_data = transform.fit_transform(calcium_data)\n",
    "        # Calculate residuals\n",
    "        dt = np.diff(time_in_seconds, axis=0, prepend=0.0)\n",
    "        resample_dt = np.median(dt[1:]).item()\n",
    "        residual_calcium = np.gradient(calcium_data, time_in_seconds.squeeze(), axis=0)\n",
    "        # Smooth the data\n",
    "        smooth_calcium_data = smooth_data_preprocess(\n",
    "            calcium_data,\n",
    "            time_in_seconds,\n",
    "            smooth_method,\n",
    "            **dict(alpha=0.5, window_size=15, sigma=5),\n",
    "        )\n",
    "        smooth_residual_calcium = smooth_data_preprocess(\n",
    "            residual_calcium,\n",
    "            time_in_seconds,\n",
    "            smooth_method,\n",
    "            **dict(alpha=0.5, window_size=15, sigma=5),\n",
    "        )\n",
    "        # Save the data\n",
    "        worm_data[\"worm\"] = worm\n",
    "        worm_data[\"source_dataset\"] = dataset_name\n",
    "        worm_data[\"smooth_method\"] = smooth_method\n",
    "        worm_data[\"calcium_data\"] = calcium_data\n",
    "        worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "        worm_data[\"residual_calcium\"] = residual_calcium\n",
    "        worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "        worm_data[\"max_timesteps\"] = max_timesteps\n",
    "        worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "        worm_data[\"dt\"] = dt\n",
    "        worm_data[\"median_dt\"] = resample_dt\n",
    "        worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "        worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "        worm_data[\"num_neurons\"] = num_signals\n",
    "        worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "        worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "        worm_data[\"extra_info\"] = {\n",
    "            \"adjacency_matrix\": sparse_mask,\n",
    "            \"inputs\": inputs,\n",
    "            \"input_gain\": inp_gain,\n",
    "            \"spectral_radius\": target_radius,\n",
    "            \"sparsity\": sparsity,\n",
    "            \"alphas\": alphas,\n",
    "            \"connection_weights\": connectivity_matrix,\n",
    "            \"meta_text\": \"`inputs` generated as iid N(0,1) noise multiplied by `input_gain`\\n.\"\n",
    "            \"adjacency_matrix` is binary.\\n`connection_weights` has th edge strengths.\\n\"\n",
    "            \"`spectral_radius` is the maximum absolute value of the eigenvalues of `connection_weights`.\\n\"\n",
    "            \"`alphas` is a vector defined by alpha[i] dt/tau_i where tau is the time constant of the i-th neuron.\\n\",\n",
    "        }\n",
    "        # Reshape the data to the standardized format\n",
    "        worm_data = reshape_calcium_data(worm_data)\n",
    "        # Save the data\n",
    "        dataset[worm] = worm_data\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "max_timesteps = 1000 \n",
    "num_worms = 300\n",
    "num_signals = 302\n",
    "num_named_neurons = 100\n",
    "add_noise = False\n",
    "noise_std = 0.0\n",
    "random_walk = False\n",
    "delta_seconds = DELTA_T\n",
    "smooth_method = None\n",
    "transform = StandardScaler()\n",
    "dataset_name = \"Recurrent0000\"\n",
    "\n",
    "# Creating and saving datasets\n",
    "dataset = create_synthetic_dataset_recurrent(\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_named_neurons=num_named_neurons,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][:num_named_neurons]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of recurrent network system recovery from sparse data\n",
    "\n",
    "Let us rearrange the discrete-time update equation from earlier:\n",
    "\n",
    "$$ \\mathbf{x}(t + \\Delta t) = \\left(1 - \\alpha\\right) \\mathbf{x}(t) + \\alpha \\mathbf{M} f(\\mathbf{x}) + \\alpha \\mathbf{b}(t) $$\n",
    "\n",
    "in a way that makes the \"push-forward\" function more obvious and makes the matrix we wish to recover from the data more explicit. \n",
    "\n",
    "**N.B.**: What we refer to as the \"push-forward\" is the function the maps from the current state to the state ate the next timestep.\n",
    "\n",
    "We will make use of the linear approximation $f(x) \\approx x$ for small $x$:\n",
    "\n",
    "$$ \\left[ \\left(1 - \\alpha\\right) \\mathbf{I}  + \\alpha \\mathbf{M} \\right] \\mathbf{x}(t) + \\alpha \\mathbf{b}(t) = \\mathbf{x}(t + \\Delta t)$$\n",
    "\n",
    "Let us define the matrix $\\mathbf{B} \\triangleq \\left[ \\left(1 - \\alpha\\right) \\mathbf{I}  + \\alpha \\mathbf{M} \\right]$.\n",
    "\n",
    "$$\\mathbf{B} \\mathbf{x}(t) + \\alpha \\mathbf{b}(t) = \\mathbf{x}(t + \\Delta t)$$\n",
    "\n",
    "Multiplying on both sides by $\\mathbf{x}(t)^\\intercal$ gives:\n",
    "\n",
    "$$\\mathbf{B} \\mathbf{x}(t)\\mathbf{x}(t)^\\intercal + \\alpha \\mathbf{b}(t)\\mathbf{x}(t)^\\intercal = \\mathbf{x}(t + \\Delta t)\\mathbf{x}(t)^\\intercal$$\n",
    "\n",
    "Since $\\mathbf{x}(t)$ is a vector we know that the matrices $\\mathbf{x}(t)\\mathbf{x}(t)^\\intercal, \\mathbf{b}(t)\\mathbf{x}(t)^\\intercal, \\mathbf{x}(t + \\Delta t)\\mathbf{x}(t)^\\intercal$ are all rank-$1$ and thus not invertible. But since we have multiple samples (a.k.a observations or measurements) of the state $\\mathbf{x}(t), t \\in \\{0,1,2, ...,T\\}$ the corresponding sample covariance matrices:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\mathbf{\\Sigma}}_x &= \\frac{1}{T} \\sum_{t=0}^{T}\\mathbf{x}(t)\\mathbf{x}(t)^\\intercal \\\\\n",
    "\\hat{\\mathbf{\\Sigma}}_b &= \\frac{1}{T} \\sum_{t=0}^{T} {\\mathbf{b}(t)\\mathbf{x}(t)^\\intercal} \\\\\n",
    "\\hat{\\mathbf{\\Sigma}}_{\\Delta t} &= \\frac{1}{T} \\sum_{t=0}^{T}\\mathbf{x}(t + \\Delta t)\\mathbf{x}(t)^\\intercal\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "will be full rank, and thus invertible, iff a minimal set $\\geq \\operatorname{dim}(\\mathbf{x})$ of linearly independent states are sampled. If suffficiently rich dynamics are present then the probability of this being true $\\to 1$ as $T \\to \\infty$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{B}\\hat{\\mathbf{\\Sigma}}_x + \\alpha \\hat{\\mathbf{\\Sigma}}_b &= \\hat{\\mathbf{\\Sigma}}_{\\Delta t} \\\\\n",
    "\\rightarrow \\mathbf{B} &= \\left( \\hat{\\mathbf{\\Sigma}}_{\\Delta t} - \\alpha \\hat{\\mathbf{\\Sigma}}_b \\right) \\left( \\hat{\\mathbf{\\Sigma}}_x \\right)^{-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can think of the external input $\\mathbf{b}$ as the environment, which is impossible measure perfectly and often not measured at all. However, we can simplify the problem of recovering the matrix $\\mathbf{B}$ by assuming either: \n",
    " 1. no external input $\\mathbf{b}(t) = \\mathbf{0}$, or\n",
    " 2. the input and state are uncorrelated $\\hat{\\mathbf{\\Sigma}}_b = \\mathbf{0}$.\n",
    "\n",
    "**N.B.**: Neither of these assumptions are valid in the real world but the problem becomes next to impossible without them.\n",
    "\n",
    "With the above assumptions, the equation for recovering the matrix $\\mathbf{B}$ from observations/measurement data simplifies to:\n",
    "\n",
    "$$\\mathbf{B} = \\hat{\\mathbf{\\Sigma}}_{\\Delta t} \\left( \\hat{\\mathbf{\\Sigma}}_x \\right)^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_X = np.zeros((num_signals, num_signals))\n",
    "cov_X_fwd = np.zeros((num_signals, num_signals))\n",
    "total_mask = np.zeros((num_signals, num_signals))\n",
    "M = None\n",
    "cap = 250  # < num_worms\n",
    "for idx in range(min(cap, num_worms)):\n",
    "    # Get current worm ID\n",
    "    worm_idx = f\"worm{idx}\"\n",
    "    # Check that \"connectome\" is the same for all worms\n",
    "    if M is None:\n",
    "        M = dataset[worm_idx][\"extra_info\"][\"connection_weights\"]\n",
    "        A = dataset[worm_idx][\"extra_info\"][\"adjacency_matrix\"]\n",
    "        a = dataset[worm_idx][\"extra_info\"][\"alphas\"]\n",
    "    else:\n",
    "        M_ = dataset[worm_idx][\"extra_info\"][\"connection_weights\"]\n",
    "        assert np.allclose(M, M_), \"Inconsisent connection weights!\"\n",
    "        M = M_\n",
    "        A_ = dataset[worm_idx][\"extra_info\"][\"adjacency_matrix\"]\n",
    "        assert np.allclose(A, A_), \"Inconsistent adjacency matrix!\"\n",
    "        A = A_\n",
    "        a_ = dataset[worm_idx][\"extra_info\"][\"alphas\"]\n",
    "        assert np.allclose(a, a_), \"Inconsistent alphas!\"\n",
    "        a = a_\n",
    "    # Compute matrices needed to estimate the connectivity matrix\n",
    "    mask = dataset[worm_idx][\"named_neurons_mask\"].numpy().reshape(1, -1)\n",
    "    data = dataset[worm_idx][\"calcium_data\"].numpy()\n",
    "    X = data\n",
    "    T = data.shape[0]\n",
    "    S = mask.T @ mask\n",
    "    total_mask += S\n",
    "    cov_X += (X[1:].T @ X[1:]) / T * S\n",
    "    cov_X_fwd += (X[1:].T @ X[:-1]) / T * S\n",
    "total_mask = np.clip(total_mask, a_min=1, a_max=None)\n",
    "\n",
    "# True B = (1-a)I + aM matrix\n",
    "B = (1 - a) * np.eye(num_signals) + a * M\n",
    "\n",
    "# Average covariance matrix over repetitions of the same neuron across all worms\n",
    "cov_X = np.divide(cov_X, total_mask)\n",
    "cov_X_fwd = np.divide(cov_X_fwd, total_mask)\n",
    "\n",
    "# Estimate of B matrix\n",
    "approx_B = cov_X_fwd @ np.linalg.pinv(cov_X)\n",
    "\n",
    "# Plot figures\n",
    "plt.figure()\n",
    "ax = sns.heatmap(approx_B, cmap=\"coolwarm\")  # , xticklabels=NEURONS_302, yticklabels=NEURONS_302)\n",
    "ax.set_title(f\"Approximate B = (1-a)I + aM matrix : {dataset_name}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(B, cmap=\"coolwarm\")  # , xticklabels=NEURONS_302, yticklabels=NEURONS_302)\n",
    "ax.set_title(f\"Ground truth B matrix : {dataset_name}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"num_worms:\", num_worms)\n",
    "print(\"total_mask:\", total_mask)\n",
    "print(\"number of neurons never observed:\", (np.diag(total_mask) == 0).sum(), end=\"\\n\\n\")\n",
    "\n",
    "print(\"cov_X_fwd symmetric:\", scp.linalg.issymmetric(cov_X_fwd))\n",
    "print(\"cov_X symmetric:\", scp.linalg.issymmetric(cov_X))\n",
    "print(\"cov_X rank:\", np.linalg.matrix_rank(cov_X))\n",
    "print(\"cov_X determinant\", np.linalg.det(cov_X), end=\"\\n\\n\")\n",
    "\n",
    "print(f\"True B diagonal (min, max): {np.diag(B).min(), np.diag(B).max()}\\nApproximate B diagonal (min, max): {np.diag(approx_B).min(), np.diag(approx_B).max()}\\n\") \n",
    "print(f\"True B eigenvalues (min, max): {np.linalg.eigvals(B).real.min(), np.linalg.eigvals(B).real.max()}\\nApproximate B eigenvalues (min, max): {np.linalg.eigvals(approx_B).real.min(), np.linalg.eigvals(approx_B).real.max()}\", end=\"\\n\\n\") \n",
    "\n",
    "print(\"estimate distance (B - approx_B):\", np.linalg.norm(B - approx_B, ord=\"fro\"), end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"(distribution) chance distance:\", np.linalg.norm(B - np.random.randn(*B.shape), ord=\"fro\")\n",
    ")  # if all you knew was that the weights were ~N(0,1)\n",
    "print(\n",
    "    \"(adjacency + distribution) chance distance:\",\n",
    "    np.linalg.norm(B - A * np.random.randn(*B.shape), ord=\"fro\"),\n",
    ")  # if you additionally knew the adjacency matrix\n",
    "print(\n",
    "    \"(sign + adjacency + distribution) chance distance:\",\n",
    "    np.linalg.norm(B - A * np.sign(B) * np.abs(np.random.randn(*B.shape)), ord=\"fro\"),\n",
    "    end=\"\\n\\n\",\n",
    ")  # if you additionally knew the signs of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between the correlation coefficient matrix, $R$, and the covariance matrix, $C$, is:\n",
    "\n",
    "$$R_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} C_{jj} } }$$\n",
    "\n",
    "The values of $R$ are between $-1$ and $1$, inclusive.\n",
    "\n",
    "We have normalized the trace of each neuron to be zero-mean and unit-variance for our datasets (simulated and real). Therefore, the covariance matrix $C$ is equivalent to the correlation matrix $R$.\n",
    "\n",
    "---\n",
    "\n",
    "After estimating $\\mathbf{B}$ above, we can use the definition $\\mathbf{B} = \\left[ \\left(1 - \\alpha\\right) \\mathbf{I}  + \\alpha \\mathbf{M} \\right]$ to to solve for $\\alpha$ and $\\mathbf{M}$ using convex optimization or gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "We can approach this problem using PyTorch to implement a neural network model that learns the parameters $\\alpha$ and $\\mathbf{M}$ through backpropagation. This approach refelcts a teacher-student setup, where the teacher (the fixed matrix $\\mathbf{B}$) generates target outputs $y$ using random inputs $x$, and the student (the neural network) tries to learn to replicate this mapping.\n",
    "\n",
    "**Set-up**\n",
    "\n",
    "1. **Generate Data**: Create pairs of inputs $x$ and targets $y$ using $\\mathbf{B}$.\n",
    "2. **Define the Model**: The model will represent $(1-\\alpha) \\mathbf{I} + \\alpha \\mathbf{M}$, where $\\alpha$ and $\\mathbf{M}$ are trainable parameters.\n",
    "3. **Training**: Use a loss function (like mean squared error) to measure the difference between the network output and the target $y$, and use an optimizer to update $\\alpha$ and $\\mathbf{M}$.\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "- **Model Setup**: The model consists of parameters $\\alpha$ and $\\mathbf{M}$. $\\alpha$ is implemented as a diagonal matrix through element-wise multiplication.\n",
    "- **Data Generation**: Data pairs $(x, y)$ are generated using $\\mathbf{B}$, where $x$ is drawn from a standard normal distribution and $y$ is computed as $\\mathbf{B}x$.\n",
    "- **Training**: We use a simple training loop with an Adam optimizer. Mean squared error (MSE) measures how well the model's outputs match the targets generated using $\\mathbf{B}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, N=approx_B.shape[0]):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.alpha = torch.nn.Parameter(torch.full((N,), 0.5))  # Vector of alphas\n",
    "        M = torch.eye(N) # Initialize M to the identity matrix\n",
    "        self.M = torch.nn.Parameter(M)  # Matrix M\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x should have shape (N, num_samples).\n",
    "        \"\"\"\n",
    "        I = torch.eye(self.N, device=x.device)\n",
    "        D_alpha = torch.diag(1 - self.alpha)\n",
    "        B_model = D_alpha @ I + torch.mul(self.alpha.unsqueeze(1), self.M)\n",
    "        return B_model @ x # (N, N) @ (N, num_samples) -> (N, num_samples)\n",
    "    \n",
    "def generate_data(B=approx_B, num_samples=1000):\n",
    "    N = B.shape[0]\n",
    "    B_tensor = torch.tensor(B, dtype=torch.float32)  # Convert B to a PyTorch tensor\n",
    "    x = torch.randn(num_samples, N)\n",
    "    y = (B_tensor @ x.T).T  # Corrected matrix multiplication\n",
    "    return x, y # (num_samples, N), (num_samples, N)\n",
    "\n",
    "def train_model(model, batch_size=1000, epochs=100, lr=0.01, lambda_l1=0.01):\n",
    "    \"\"\"\n",
    "    Method 1: Projection\n",
    "    This method involves directly clipping the alpha values after each gradient step within the training loop.\n",
    "    \"\"\"\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = generate_data(B=approx_B, num_samples=batch_size)\n",
    "        outputs = model(x.T)  # Transpose x here\n",
    "        loss = criterion(outputs.T, y)  # Transpose outputs back to match y\n",
    "        diagonal = torch.diag(model.M)\n",
    "        # Impose L1 penalty only on the off-diagonal elements\n",
    "        l1_penalty = lambda_l1 * torch.sum(torch.abs(model.M - torch.diagflat(diagonal))) \n",
    "        total_loss = loss + l1_penalty\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % int(epochs**0.333) == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {total_loss.item()}, \\n\\tBase Loss: {loss.item()}, \\n\\tL1 Penalty: {l1_penalty.item()}\\n\")\n",
    "\n",
    "# Example usage\n",
    "model = LinearModel(N=approx_B.shape[0])\n",
    "train_model(model, batch_size=1000, epochs=1000, lr=0.01, lambda_l1=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures\n",
    "plt.figure()\n",
    "learned_a = model.alpha.detach().numpy()\n",
    "plt.scatter(a, learned_a)\n",
    "plt.xlabel(\"true alpha\")\n",
    "plt.ylabel(\"learned alpha\")\n",
    "plt.title(\"Correlation between true and learned alphas\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "learned_M = model.M.detach().numpy()\n",
    "ax = sns.heatmap(learned_M, cmap=\"coolwarm\")  # , xticklabels=NEURONS_302, yticklabels=NEURONS_302)\n",
    "ax.set_title(f\"Learned connectivity matrix : {dataset_name}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(M, cmap=\"coolwarm\")  # , xticklabels=NEURONS_302, yticklabels=NEURONS_302)\n",
    "ax.set_title(f\"Ground truth connectivity matrix : {dataset_name}\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"True connectivity diagonal (min, max): {np.diag(M).min(), np.diag(M).max()}\\nLearned connectivity diagonal (min, max): {np.diag(learned_M).min(), np.diag(learned_M).max()}\\n\") \n",
    "print(f\"True connectivity eigenvalues (min, max): {np.linalg.eigvals(M).real.min(), np.linalg.eigvals(M).real.max()}\\nLeanred connectivity eigenvalues (min, max): {np.linalg.eigvals(learned_M).real.min(), np.linalg.eigvals(learned_M).real.max()}\", end=\"\\n\\n\") \n",
    "\n",
    "print(\"estimate distance (M - learned_M):\", np.linalg.norm(M - learned_M, ord=\"fro\"), end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"(distribution) chance distance:\", np.linalg.norm(M - np.random.randn(*M.shape), ord=\"fro\")\n",
    ")  # if all you knew was that the weights were ~N(0,1)\n",
    "print(\n",
    "    \"(adjacency + distribution) chance distance:\",\n",
    "    np.linalg.norm(M - A * np.random.randn(*M.shape), ord=\"fro\"),\n",
    ")  # if you additionally knew the adjacency matrix\n",
    "print(\n",
    "    \"(sign + adjacency + distribution) chance distance:\",\n",
    "    np.linalg.norm(M - A * np.sign(M) * np.abs(np.random.randn(*M.shape)), ord=\"fro\"), \n",
    "    end=\"\\n\\n\",\n",
    ")  # if you additionally knew the signs of the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WOW! I can't believe I got that to work!\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "* Solving for the connectivity matrix is impossible without input noise, even though when we wrote the equation we ignore the noise because we don't know it!\n",
    "\n",
    "* Also, we don't know the time constants but that ends up not making a huge difference to our empirical results!\n",
    "\n",
    "* It seems more important to have more recorded neurons per worm and have fewer worms than to have have many worms with only a few recorded neurons. So the optimizing the number of neurons recorded per animal is more important than improving the throughput of animals.\n",
    "\n",
    "* Initialization of the $M$ matrix as a parameter is very important. The closer we can initialize it to the true connectivity, the better our learned approximation is to the truth. But obviously in the real world we don't have any hints about the initialization. Empirically we find initializing to the identity matrix works fairly well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sines dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_sines(\n",
    "    max_timesteps: int = 1000,\n",
    "    num_worms: int = 1,\n",
    "    num_signals: int = 302,\n",
    "    num_named_neurons: Union[None, int] = None,\n",
    "    add_noise: bool = False,\n",
    "    noise_std: float = 0.01,\n",
    "    random_walk: bool = False,\n",
    "    sum_frequencies: int = 1,\n",
    "    random_freqs: bool = False,\n",
    "    delta_seconds: float = 0.5,\n",
    "    smooth_method: Union[None, str] = \"ES\",\n",
    "    transform: Union[\n",
    "        None, callable\n",
    "    ] = StandardScaler(),  # StandardScaler(), #CausalNormalizer() #None\n",
    "    dataset_name: str = \"Sines0000\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a synthetic worm datasets using a sum of sinusoidal functions.\n",
    "    Out of the `num_signals` neurons, `num_named_neurons` neurons are chosen randomly\n",
    "    to have non-zero (i.e. \"measured\") signals. Each neuron's calcium signal is a sinusoid\n",
    "    with `sum_frequencies` frequencies added together and a random phase. Each neuron class\n",
    "    has a distinct frequency fingerprint (i.e. the particular frequencies summed together).\n",
    "\n",
    "    :param max_timesteps: The number of timepoints of synthetic data to generate.\n",
    "    :param num_worms: The number of synthetic worms to create datasets for.\n",
    "    :param num_signals: The number of signals corresponding to number of neurons.\n",
    "    :param num_named_neurons: The number of named neurons to create non-zero signals for.\n",
    "    :param add_noise: Whether to add Gaussian noise to the synthetic data.\n",
    "    :param noise_std: The standard deviation of the noise.\n",
    "    :param random_walk: If True, use a random walk to generate the noise. Otherwise, use iid noise.\n",
    "    :param sum_frequencies: The number of sine functions to add together to create a signal.\n",
    "    :param random_freqs: If True, use random frequencies for each neuron. Otherwise, use the same frequencies for all neurons.\n",
    "    :param delta_seconds: The constant time difference (in seconds) between each timepoint.\n",
    "    :param smooth_method: The method to use for smoothing the data.\n",
    "    :param transform: The sklearn method to scale or transform the data before use.\n",
    "    :param dataset_name: The name to give the synthetic dataset.\n",
    "    :return: A dictionary containing the synthetic worm datasets.\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    dataset = {}\n",
    "    # Determine the timepoints for sampling the data (i.e. \"measurement\" times)\n",
    "    time_in_seconds = delta_seconds * np.arange(max_timesteps).reshape(-1, 1)  # column vector\n",
    "    # Create a memo of neurons to frequencies so that distinct neurons have distinct frequency signatures\n",
    "    neuron_to_frequencies = dict()  # this will be consistent across worms\n",
    "    # Calculate number of named and unknown neurons\n",
    "    if num_named_neurons is None or num_named_neurons > num_signals:  # default to all neurons\n",
    "        num_named_neurons = num_signals\n",
    "    elif num_named_neurons < 0:  # default to no neurons\n",
    "        num_named_neurons = 0\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "    # Create data for each worm\n",
    "    for worm_idx in range(num_worms):\n",
    "        # Create a memo of neurons to phases so that distinct neurons have distinct phase signatures\n",
    "        neuron_to_phases = dict()  # this will vary across worms\n",
    "        # Initialize worm data\n",
    "        worm = f\"worm{worm_idx}\"\n",
    "        worm_data = {}\n",
    "        calcium_data = np.zeros((max_timesteps, num_signals))\n",
    "        # Choose a random subset of neurons to record / observe / measure\n",
    "        named_neuron_indices = random.sample(\n",
    "            range(num_signals), num_named_neurons\n",
    "        )  # without replacement\n",
    "        named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "        # Create neuron to idx mapping and vice versa\n",
    "        neuron_to_idx = {\n",
    "            (neuron) if neuron in named_neurons else str(idx): idx\n",
    "            for idx, neuron in enumerate(NEURONS_302)\n",
    "        }\n",
    "        idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "        # Create calcium data\n",
    "        for i in named_neuron_indices:\n",
    "            neuron_index = i\n",
    "            neuron_name = NEURONS_302[neuron_index]\n",
    "            # Each neuron class has a distinct deterministic frequency fingerprint\n",
    "            frequencies = neuron_to_frequencies.setdefault(\n",
    "                neuron_name,\n",
    "                (\n",
    "                    np.round(\n",
    "                        eps + np.random.random(sum_frequencies) / 10, 4\n",
    "                    ).tolist()  # random frequencies\n",
    "                    if random_freqs\n",
    "                    else np.round(\n",
    "                        np.arange(1 / (2 * sum_frequencies + eps), sum_frequencies)\n",
    "                        / (10 * sum_frequencies),\n",
    "                        4,\n",
    "                    ).tolist()\n",
    "                ),  # deterministic frequencies\n",
    "            )\n",
    "            # All neuron classes have a distinct but random phase offset\n",
    "            phases = neuron_to_phases.setdefault(\n",
    "                neuron_name,\n",
    "                [np.random.random() * 2 * np.pi for _ in range(sum_frequencies)],  # random phase\n",
    "            )\n",
    "            time_points = np.arange(max_timesteps)\n",
    "            # Add sine functions together to create the simulated calcium signal\n",
    "            for freq, phase in zip(frequencies, phases):\n",
    "                calcium_data[:, neuron_index] += np.sin(2 * np.pi * freq * time_points + phase)\n",
    "            # Add noise\n",
    "            if add_noise:\n",
    "                if random_walk:\n",
    "                    noise_walk = np.cumsum(\n",
    "                        [0]\n",
    "                        + np.random.normal(loc=0, scale=noise_std, size=max_timesteps - 1).tolist()\n",
    "                    )\n",
    "                    calcium_data[:, neuron_index] += noise_walk\n",
    "                else:\n",
    "                    noise_iid = np.random.normal(0, noise_std, max_timesteps)\n",
    "                    calcium_data[:, neuron_index] += noise_iid\n",
    "        # Normalize the data\n",
    "        if transform:\n",
    "            calcium_data = transform.fit_transform(calcium_data)\n",
    "        # Calculate residuals\n",
    "        dt = np.diff(time_in_seconds, axis=0, prepend=0.0)\n",
    "        resample_dt = np.median(dt[1:]).item()\n",
    "        residual_calcium = np.gradient(calcium_data, time_in_seconds.squeeze(), axis=0)\n",
    "        # Smooth the data\n",
    "        smooth_calcium_data = smooth_data_preprocess(\n",
    "            calcium_data, time_in_seconds, smooth_method, **dict(alpha=0.5, window_size=15, sigma=5)\n",
    "        )\n",
    "        smooth_residual_calcium = smooth_data_preprocess(\n",
    "            residual_calcium,\n",
    "            time_in_seconds,\n",
    "            smooth_method,\n",
    "            **dict(alpha=0.5, window_size=15, sigma=5),\n",
    "        )\n",
    "        # Build the dataset\n",
    "        worm_data[\"worm\"] = worm\n",
    "        worm_data[\"source_dataset\"] = dataset_name\n",
    "        worm_data[\"smooth_method\"] = smooth_method\n",
    "        worm_data[\"num_neurons\"] = num_signals\n",
    "        worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "        worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "        worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "        worm_data[\"dt\"] = dt\n",
    "        worm_data[\"max_timesteps\"] = max_timesteps\n",
    "        worm_data[\"median_dt\"] = resample_dt\n",
    "        worm_data[\"calcium_data\"] = calcium_data\n",
    "        worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "        worm_data[\"residual_calcium\"] = residual_calcium\n",
    "        worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "        worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "        worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "        worm_data[\"extra_info\"] = {\n",
    "            \"neuron_to_frequencies\": neuron_to_frequencies,\n",
    "            \"neuron_to_phases\": neuron_to_phases,\n",
    "            \"meta_text\": \"This dataset simulates neural activity data as the sum of sinusoidal functions with distinct frequency and phase fingerprints for each neuron class.\"\n",
    "            \"The 'fingerprint' for each neuron class is the particular set deterministic frequencies that are summed to give that neuron's signal and each neuron class has a distinct but random phase offset.\\n\",\n",
    "        }\n",
    "        # Reshape the data to the standardized format\n",
    "        worm_data = reshape_calcium_data(worm_data)\n",
    "        # Save the data\n",
    "        dataset[worm] = worm_data\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "max_timesteps = 1500\n",
    "num_worms = 500\n",
    "num_signals = 302\n",
    "num_named_neurons = 100\n",
    "add_noise = False\n",
    "noise_std = 0.1\n",
    "random_walk = False\n",
    "sum_frequencies = 1\n",
    "random_freqs = True\n",
    "delta_seconds = 1.0\n",
    "smooth_method = None\n",
    "transform = StandardScaler()\n",
    "dataset_name = \"Sines0000\"\n",
    "\n",
    "# Creating and saving datasets\n",
    "dataset = create_synthetic_dataset_sines(\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_named_neurons=num_named_neurons,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    sum_frequencies=sum_frequencies,\n",
    "    random_freqs=random_freqs,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][:num_named_neurons]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocorrelation\n",
    "---\n",
    "Autocorrelation and partial autocorrelation are concepts used in time series analysis to examine the relationship of a variable with itself at different lags.\n",
    "\n",
    "**Autocorrelation Function (ACF):**\n",
    "- This is a correlation of a signal with a delayed copy of itself as a function of delay.\n",
    "- The autocorrelation plot (or ACF plot) displays the correlation between the time series and its lagged values.\n",
    "- The y-axis shows the autocorrelation coefficient, which ranges from -1 to 1. A value of 1 indicates perfect positive correlation, while -1 indicates perfect negative correlation.\n",
    "- The x-axis represents the lag at which the autocorrelation is computed. For example, at lag 1, you're comparing the series with itself one time step back.\n",
    "- The ACF considers the combined effect of all previous time points up to the lagged time point being calculated. It doesn't isolate the correlation at each lag.\n",
    "\n",
    "**Partial Autocorrelation Function (PACF):**\n",
    "- This measures the correlation between the time series and its lagged version after accounting for the variations already explained by the intervening comparisons. In other words, it controls for the previous lags.\n",
    "- The PACF plot shows the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags.\n",
    "- It isolates the additional correlation at a particular lag that is not explained by previous lags. This is useful to identify the actual lagged relationship, without the confounding effects of the correlations at shorter lags.\n",
    "\n",
    "In summary, while the ACF shows the cumulative effect of past data points on the current value, the PACF removes this cumulative effect and tries to show the unique contribution of each past data point.\n",
    "\n",
    "In practical applications like ARIMA (AutoRegressive Integrated Moving Average) modeling, the ACF and PACF are used to determine the AR (autoregressive) and MA (moving average) terms:\n",
    "\n",
    "* The PACF helps to identify the order of the AR part (p) by finding the lag after which the PACF cuts off (drops to zero or below the significance level).\n",
    "* The ACF is used to identify the order of the MA part (q) by finding the lag after which the ACF cuts off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting autocorrelation and partial autocorrelation\n",
    "\n",
    "# X is our data matrix (time, untis) and neurons is a list of names for each unit\n",
    "plot_autocorrelation_and_pacf(X, neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lorenz dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_lorenz(\n",
    "    max_timesteps: int = 1000,\n",
    "    num_worms: int = 1,\n",
    "    num_signals: int = 302,\n",
    "    num_lorenz_systems: int = 1,\n",
    "    add_noise: bool = False,\n",
    "    noise_std: float = 0.01,\n",
    "    random_walk: bool = False,\n",
    "    fine_integration_step: float = 0.1,\n",
    "    delta_seconds: float = 0.5,\n",
    "    smooth_method: Union[None, str] = \"ES\",\n",
    "    transform: Union[\n",
    "        None, callable\n",
    "    ] = StandardScaler(),  # StandardScaler() #CausalNormalizer() #None\n",
    "    dataset_name: str = \"Lorenz0000\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a synthetic worm datasets using the Lorenz attractor.\n",
    "    Three neurons are chosen randomly to represent x, y, z trajectories from the Lorenz system.\n",
    "\n",
    "    :param max_timesteps: The number of timepoints of synthetic data to generate.\n",
    "    :param num_worms: The number of synthetic worms to create datasets for.\n",
    "    :param num_signals: The number of signals corresponding to number of neurons.\n",
    "    :param num_lorenz_systems: The number of Lorenz systems to use for creating the synthetic data.\n",
    "    :param add_noise: Whether to add Gaussian noise to the synthetic data.\n",
    "    :param noise_std: The standard deviation of the noise.\n",
    "    :param random_walk: If True, use a random walk to generate the noise. Otherwise, use iid noise.\n",
    "    :param fine_integration_step: The integration step size for the Lorenz system.\n",
    "    :param delta_seconds: The constant time difference (in seconds) between each measurement of the system.\n",
    "    :param smooth_method: The method to use for smoothing the data.\n",
    "    :param transform: The sklearn method to scale or transform the data before use.\n",
    "    :param dataset_name: The name to give the synthetic dataset.\n",
    "    :return: A dictionary containing the synthetic worm datasets.\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    dataset = {}\n",
    "\n",
    "    # Determine the timepoints for sampling the data (i.e. \"measurement\" times)\n",
    "    time_in_seconds = delta_seconds * np.arange(max_timesteps).reshape(-1, 1)  # Column vector\n",
    "\n",
    "    # Create a memo of neurons to state variable so that distinct neurons represent a distinct variable\n",
    "    neuron_to_state_var = dict()  # this will be consistent across worms\n",
    "\n",
    "    # Calculate number of named and unknown neurons\n",
    "    num_named_neurons = num_lorenz_systems * 3\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "\n",
    "    # Fine integration time points for the true underlying dynamics\n",
    "    fine_time_points = np.arange(0, max_timesteps * delta_seconds, fine_integration_step)\n",
    "\n",
    "    # Create data for each worm\n",
    "    for worm_idx in range(num_worms):\n",
    "        # Initialize worm data\n",
    "        worm = f\"worm{worm_idx}\"\n",
    "        worm_data = {}\n",
    "        calcium_data = np.zeros((max_timesteps, num_signals))\n",
    "\n",
    "        # Choose a random subset of neurons to record / observe / measure\n",
    "        named_neuron_indices = random.sample(\n",
    "            range(num_signals), num_named_neurons\n",
    "        )  # without replacement\n",
    "        named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "\n",
    "        # Create neuron to idx mapping and vice versa\n",
    "        neuron_to_idx = {\n",
    "            (neuron) if neuron in named_neurons else str(idx): idx\n",
    "            for idx, neuron in enumerate(NEURONS_302)\n",
    "        }\n",
    "        idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "\n",
    "        # Create calcium data\n",
    "        calcium_data = np.zeros((max_timesteps, num_signals))\n",
    "\n",
    "        # Create multiple independent Lorenz systems\n",
    "        for system_idx in range(num_lorenz_systems):\n",
    "            # Define a new set of initial conditions for each Lorenz system\n",
    "            initial_conditions = np.random.uniform(low=-5.0, high=5.0, size=3)\n",
    "            # np.random.randn(3)  # or other method to generate initial conditions\n",
    "\n",
    "            # Integrate the Lorenz system with different initial conditions\n",
    "            fine_trajectories = odeint(lorenz, initial_conditions, fine_time_points)\n",
    "\n",
    "            # Downsample to get the \"measured\" data at delta_seconds intervals\n",
    "            downsample_indices = np.arange(\n",
    "                0, len(fine_time_points), int(delta_seconds / fine_integration_step)\n",
    "            )\n",
    "            sampled_trajectories = fine_trajectories[downsample_indices]\n",
    "\n",
    "            # Assign to the correct set of neurons for this Lorenz system\n",
    "            for i in range(3):\n",
    "                neuron_index = named_neuron_indices[system_idx * 3 + i]\n",
    "                neuron_name = NEURONS_302[neuron_index]\n",
    "\n",
    "                # Uniquely map each neuron to a committed state variable\n",
    "                var = neuron_to_state_var.setdefault(neuron_name, i)\n",
    "                calcium_data[:, neuron_index] = sampled_trajectories[:, var]\n",
    "\n",
    "                # Add noise\n",
    "                if add_noise:\n",
    "                    if random_walk:\n",
    "                        noise_walk = np.cumsum(\n",
    "                            [0]\n",
    "                            + np.random.normal(\n",
    "                                loc=0, scale=noise_std, size=max_timesteps - 1\n",
    "                            ).tolist()\n",
    "                        )\n",
    "                        calcium_data[:, neuron_index] += noise_walk\n",
    "                    else:\n",
    "                        noise_iid = np.random.normal(0, noise_std, max_timesteps)\n",
    "                        calcium_data[:, neuron_index] += noise_iid\n",
    "\n",
    "        # Normalize data\n",
    "        if transform:\n",
    "            calcium_data = transform.fit_transform(calcium_data)\n",
    "\n",
    "        # Calculate residuals\n",
    "        dt = np.diff(time_in_seconds, axis=0, prepend=0.0)\n",
    "        resample_dt = np.median(dt[1:]).item()\n",
    "        residual_calcium = np.gradient(calcium_data, time_in_seconds.squeeze(), axis=0)\n",
    "\n",
    "        # Smooth the data\n",
    "        smooth_calcium_data = smooth_data_preprocess(\n",
    "            calcium_data, time_in_seconds, smooth_method, **dict(alpha=0.5, window_size=15, sigma=5)\n",
    "        )\n",
    "        smooth_residual_calcium = smooth_data_preprocess(\n",
    "            residual_calcium,\n",
    "            time_in_seconds,\n",
    "            smooth_method,\n",
    "            **dict(alpha=0.5, window_size=15, sigma=5),\n",
    "        )\n",
    "\n",
    "        # Build the dataset\n",
    "        worm_data[\"worm\"] = worm\n",
    "        worm_data[\"source_dataset\"] = dataset_name\n",
    "        worm_data[\"smooth_method\"] = smooth_method\n",
    "        worm_data[\"num_neurons\"] = num_signals\n",
    "        worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "        worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "        worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "        worm_data[\"dt\"] = dt\n",
    "        worm_data[\"max_timesteps\"] = max_timesteps\n",
    "        worm_data[\"median_dt\"] = resample_dt\n",
    "        worm_data[\"calcium_data\"] = calcium_data\n",
    "        worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "        worm_data[\"residual_calcium\"] = residual_calcium\n",
    "        worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "        worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "        worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "        worm_data[\"extra_info\"] = dict()\n",
    "\n",
    "        # Reshape the data to the standardized format\n",
    "        worm_data = reshape_calcium_data(worm_data)\n",
    "\n",
    "        # Save the data\n",
    "        dataset[worm] = worm_data\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "max_timesteps = 1500\n",
    "num_worms = 500\n",
    "num_signals = 302\n",
    "num_lorenz_systems = 17\n",
    "add_noise = False\n",
    "noise_std = 0.1\n",
    "random_walk = False\n",
    "fine_integration_step = 0.04\n",
    "delta_seconds = 0.04\n",
    "smooth_method = None\n",
    "transform = StandardScaler()\n",
    "dataset_name = \"Lorenz0000\"\n",
    "\n",
    "# Creating and saving datasets\n",
    "dataset = create_synthetic_dataset_lorenz(\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_lorenz_systems=num_lorenz_systems,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    fine_integration_step=fine_integration_step,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][: num_lorenz_systems * 3]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} dataset - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Delay Embedding\n",
    "---\n",
    "Time delay embedding is a technique used in the analysis of dynamical systems, particularly in the context of reconstructing the phase space of a system from a series of observations over time. This technique is based on Takens' Embedding Theorem, which states that the dynamics of a system can be reconstructed from the time series of a single observable of the system, under certain conditions.\n",
    "\n",
    "Here are some key points about time delay embedding:\n",
    "\n",
    "- **Time Delay, τ (tau):** This is the time interval between successive observations in the reconstructed phase space. Choosing an appropriate τ is crucial; too short a delay may lead to redundant information, while too long a delay may lose the dynamics of interest.\n",
    "\n",
    "- **Embedding Dimension, m:** This represents the number of delayed observations used to reconstruct the phase space. It should be high enough to unfold the dynamics, but not too high to avoid overcomplicating the model.\n",
    "\n",
    "- **Phase Space Reconstruction:** By plotting the time-delayed copies of the time series against each other, one can reconstruct the phase space, which can reveal underlying dynamical properties like attractors or limit cycles.\n",
    "\n",
    "- **Mutual Information:** To empirically choose the right τ, one common method is to calculate the mutual information between the time series and its delayed version, and select τ at the first minimum of the mutual information function.\n",
    "\n",
    "- **False Nearest Neighbors (FNN):** The method of False Nearest Neighbors can help determine a suitable embedding dimension m by identifying when points that appear to be neighbors in lower-dimensional space are no longer neighbors in higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Let's assume you have a time series data stored in a variable called `time_series_data`.\n",
    "idx = 1\n",
    "time_series_data = X[:, idx]\n",
    "# You would first determine `tau` and `m` based on your data.\n",
    "\n",
    "tau = 1  # replace with the delay you've computed or chosen\n",
    "m = 3  # replace with the embedding dimension you've computed or chosen\n",
    "\n",
    "# You can then compute the time-delay embedding of your data as follows:\n",
    "embedded_data = time_delay_embedding(time_series_data, tau, m)\n",
    "plot_3d_trajectory(embedded_data, title=f\"Time delay embedding of {neurons[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamics datasets\n",
    "___\n",
    "\n",
    "We attempt to generalize the previous approaches to generating synthetic data from arbitrary dynamical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_dynamics(\n",
    "    dynamics_func: callable,\n",
    "    state_dim: int,\n",
    "    func_args: tuple = (),\n",
    "    max_timesteps: int = 1000,\n",
    "    num_worms: int = 1,\n",
    "    num_signals: int = 302,\n",
    "    num_dynamic_systems: int = 1,\n",
    "    add_noise: bool = False,\n",
    "    random_walk: bool = False,\n",
    "    noise_std: float = 0.01,\n",
    "    fine_integration_step: float = 0.1,\n",
    "    delta_seconds: float = 0.5,\n",
    "    smooth_method: Union[None, str] = \"ES\",\n",
    "    transform: Union[\n",
    "        None, callable\n",
    "    ] = StandardScaler(),  # StandardScaler(), #CausalNormalizer() #None\n",
    "    dataset_name: str = \"Dynamics0000\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a synthetic worm dataset using a user-defined dynamical system.\n",
    "\n",
    "    Parameters:\n",
    "    :param dynamics_func: The function defining the dynamics, should take arguments (y, t, *args).\n",
    "    :param state_dim: The dimension of the state vector y to the dynamics function.\n",
    "    :param func_args: Any extra arguments to pass to the dynamics function.\n",
    "    :param max_timesteps: The number of timepoints of synthetic data to generate.\n",
    "    :param num_worms: The number of synthetic worms to create datasets for.\n",
    "    :param num_signals: The number of signals corresponding to number of neurons.\n",
    "    :param num_dynamic_systems: The number of independent dynamical systems to use for creating the synthetic data.\n",
    "    :param add_noise: Whether to add Gaussian noise to the synthetic data.\n",
    "    :param noise_std: The standard deviation of the noise.\n",
    "    :param fine_integration_step: The integration step size for the Lorenz system.\n",
    "    :param delta_seconds: The constant time difference (in seconds) between each measurement of the system.\n",
    "    :param smooth_method: The method to use for smoothing the data.\n",
    "    :param transform: The sklearn method to scale or transform the data before use.\n",
    "    :param dataset_name: The name to give the synthetic dataset.\n",
    "\n",
    "    :return: A dictionary containing the synthetic worm datasets.\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    dataset = {}\n",
    "\n",
    "    # Determine the timepoints for sampling the data (i.e. \"measurement\" times)\n",
    "    time_in_seconds = delta_seconds * np.arange(max_timesteps).reshape(-1, 1)  # Column vector\n",
    "\n",
    "    # Create a memo of neurons to state variable so that distinct neurons represent a distinct variable\n",
    "    neuron_to_state_var = dict()  # this will be consistent across worms\n",
    "\n",
    "    # Calculate number of named and unknown neurons\n",
    "    num_named_neurons = num_dynamic_systems * state_dim\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "\n",
    "    # Add a warmup period to allow the system to reach a steady state\n",
    "    warmup_timesteps = 10\n",
    "\n",
    "    # Fine integration time points for the true underlying dynamics\n",
    "    fine_time_points = np.arange(\n",
    "        0, (max_timesteps + warmup_timesteps) * delta_seconds, fine_integration_step\n",
    "    )\n",
    "\n",
    "    # Create data for each worm\n",
    "    for worm_idx in range(num_worms):\n",
    "        # Initialize worm data\n",
    "        worm = f\"worm{worm_idx}\"\n",
    "        worm_data = {}\n",
    "        calcium_data = np.zeros((max_timesteps, num_signals))\n",
    "\n",
    "        # Choose a random subset of neurons to record / observe / measure\n",
    "        named_neuron_indices = random.sample(\n",
    "            range(num_signals), num_named_neurons\n",
    "        )  # without replacement\n",
    "        named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "\n",
    "        # Create neuron to idx mapping and vice versa\n",
    "        neuron_to_idx = {\n",
    "            (neuron) if neuron in named_neurons else str(idx): idx\n",
    "            for idx, neuron in enumerate(NEURONS_302)\n",
    "        }\n",
    "        idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "\n",
    "        # Create calcium data\n",
    "        calcium_data = np.zeros((max_timesteps, num_signals))\n",
    "\n",
    "        # Create multiple independent systems; same dynamics but different initial conditions\n",
    "        for system_idx in range(num_dynamic_systems):\n",
    "            # Define a new set of initial conditions for each system\n",
    "            initial_conditions = np.random.randn(\n",
    "                state_dim\n",
    "            )  # or other method to generate initial conditions\n",
    "\n",
    "            # Integrate the dynamical system\n",
    "            fine_trajectories = odeint(\n",
    "                dynamics_func,\n",
    "                initial_conditions,\n",
    "                fine_time_points,\n",
    "                args=func_args,\n",
    "            )\n",
    "\n",
    "            # Downsample to get the \"measured\" data at delta_seconds intervals\n",
    "            downsample_indices = np.arange(\n",
    "                0, len(fine_time_points), int(delta_seconds / fine_integration_step)\n",
    "            )\n",
    "            sampled_trajectories = fine_trajectories[downsample_indices]\n",
    "\n",
    "            # Ignore the warmup period\n",
    "            sampled_trajectories = sampled_trajectories[warmup_timesteps:]\n",
    "\n",
    "            # Assign to the correct set of neurons for this dynamical system\n",
    "            for i in range(state_dim):\n",
    "                neuron_index = named_neuron_indices[system_idx * state_dim + i]\n",
    "                neuron_name = NEURONS_302[neuron_index]\n",
    "\n",
    "                # Uniquely map each neuron to a committed state variable\n",
    "                var = neuron_to_state_var.setdefault(neuron_name, i)\n",
    "                calcium_data[:, neuron_index] = sampled_trajectories[:, var]\n",
    "\n",
    "                # Add noise\n",
    "                if add_noise:\n",
    "                    if random_walk:\n",
    "                        noise_walk = np.cumsum(\n",
    "                            [0]\n",
    "                            + np.random.normal(\n",
    "                                loc=0, scale=noise_std, size=max_timesteps - 1\n",
    "                            ).tolist()\n",
    "                        )\n",
    "                        calcium_data[:, neuron_index] += noise_walk\n",
    "                    else:\n",
    "                        noise_iid = np.random.normal(0, noise_std, max_timesteps)\n",
    "\n",
    "                        calcium_data[:, neuron_index] += noise_iid\n",
    "\n",
    "        # Normalize data\n",
    "        if transform:\n",
    "            calcium_data = transform.fit_transform(calcium_data)\n",
    "\n",
    "        # Calculate residuals\n",
    "        dt = np.diff(time_in_seconds, axis=0, prepend=0.0)\n",
    "        resample_dt = np.median(dt[1:]).item()\n",
    "        residual_calcium = np.gradient(calcium_data, time_in_seconds.squeeze(), axis=0)\n",
    "\n",
    "        # Smooth the data\n",
    "        smooth_calcium_data = smooth_data_preprocess(calcium_data, time_in_seconds, smooth_method)\n",
    "        smooth_residual_calcium = smooth_data_preprocess(\n",
    "            residual_calcium, time_in_seconds, smooth_method\n",
    "        )\n",
    "\n",
    "        # Save the data\n",
    "        worm_data[\"source_dataset\"] = dataset_name\n",
    "        worm_data[\"smooth_method\"] = smooth_method\n",
    "        worm_data[\"worm\"] = worm\n",
    "        worm_data[\"calcium_data\"] = calcium_data\n",
    "        worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "        worm_data[\"residual_calcium\"] = residual_calcium\n",
    "        worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "        worm_data[\"max_timesteps\"] = max_timesteps\n",
    "        worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "        worm_data[\"dt\"] = dt\n",
    "        worm_data[\"median_dt\"] = resample_dt\n",
    "        worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "        worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "        worm_data[\"num_neurons\"] = num_signals\n",
    "        worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "        worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "\n",
    "        # Reshape the data to the standardized format\n",
    "        worm_data = reshape_calcium_data(worm_data)\n",
    "\n",
    "        # Save the data\n",
    "        dataset[worm] = worm_data\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Van Der Pol dataset\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "dynamics_func = van_der_pol_oscillator\n",
    "state_dim = 2\n",
    "func_args = ()\n",
    "max_timesteps = 1500\n",
    "num_worms = 200\n",
    "num_signals = 302\n",
    "num_dynamic_systems = 25\n",
    "add_noise = False\n",
    "noise_std = 0.1\n",
    "random_walk = False\n",
    "fine_integration_step = 0.05\n",
    "delta_seconds = 0.05\n",
    "smooth_method = None\n",
    "transform = StandardScaler()\n",
    "dataset_name = \"VanDerPol0000\"\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "dataset = create_synthetic_dataset_dynamics(\n",
    "    dynamics_func=dynamics_func,\n",
    "    state_dim=state_dim,\n",
    "    func_args=func_args,\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_dynamic_systems=num_dynamic_systems,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    fine_integration_step=fine_integration_step,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][\n",
    "    : num_dynamic_systems * state_dim\n",
    "]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} dataset - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk dataset\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "dynamics_func = sinusoids_oscillator\n",
    "state_dim = 1\n",
    "func_args = ()\n",
    "max_timesteps = 1500\n",
    "num_worms = 200\n",
    "num_signals = 302\n",
    "num_dynamic_systems = 50\n",
    "add_noise = True\n",
    "noise_std = 0.1\n",
    "random_walk = True\n",
    "fine_integration_step = 1.0\n",
    "delta_seconds = 1.0\n",
    "smooth_method = None\n",
    "transform = StandardScaler()\n",
    "dataset_name = \"RandWalk0000\"\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "dataset = create_synthetic_dataset_dynamics(\n",
    "    dynamics_func=dynamics_func,\n",
    "    state_dim=state_dim,\n",
    "    func_args=func_args,\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_dynamic_systems=num_dynamic_systems,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    fine_integration_step=fine_integration_step,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][\n",
    "    : num_dynamic_systems * state_dim\n",
    "]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} dataset - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Noise dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "dynamics_func = sinusoids_oscillator\n",
    "state_dim = 1\n",
    "func_args = ()\n",
    "max_timesteps = 1500\n",
    "num_worms = 200\n",
    "num_signals = 302\n",
    "num_dynamic_systems = 50\n",
    "add_noise = True\n",
    "noise_std = 0.1\n",
    "random_walk = False\n",
    "fine_integration_step = 1.0\n",
    "delta_seconds = 1.0\n",
    "smooth_method = None\n",
    "transform = StandardScaler()\n",
    "dataset_name = \"WhiteNoise0000\"\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "dataset = create_synthetic_dataset_dynamics(\n",
    "    dynamics_func=dynamics_func,\n",
    "    state_dim=state_dim,\n",
    "    func_args=func_args,\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_dynamic_systems=num_dynamic_systems,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    fine_integration_step=fine_integration_step,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][\n",
    "    : num_dynamic_systems * state_dim\n",
    "]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} dataset - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
