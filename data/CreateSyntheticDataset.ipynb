{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Synthetic Datasets\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to generate simple sinusoidal synthetic datasets with a specified numbers of worms, named neurons, and other characteristics of real datasets.\n",
    "\n",
    "**Last update:** _11 December 2023_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries and helper functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union\n",
    "from datasets import load_dataset\n",
    "from scipy.integrate import odeint\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import NEURONS_302, init_random_seeds\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from preprocess._utils import CausalNormalizer, smooth_data_preprocess, reshape_calcium_data\n",
    "\n",
    "# Initialize the random seeds\n",
    "init_random_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_tokenize_and_chunk(\n",
    "    text, max_length=510\n",
    "):  # slightly less than 512 to account for special tokens\n",
    "    pre_tokenizer = WhitespaceSplit()\n",
    "    pre_tokens = pre_tokenizer.pre_tokenize_str(text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for token, (start, end) in pre_tokens:\n",
    "        if len(current_chunk) + len(token) + 1 > max_length:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = token\n",
    "        else:\n",
    "            current_chunk += \" \" + token if current_chunk else token\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(examples, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-large\")\n",
    "    tokenized_batches = {\"text\": [], \"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        chunks = pre_tokenize_and_chunk(text)\n",
    "        for chunk in chunks:\n",
    "            tokenized_output = tokenizer(\n",
    "                chunk,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            tokenized_batches[\"text\"].append(chunk)\n",
    "            tokenized_batches[\"input_ids\"].append(tokenized_output[\"input_ids\"][0].tolist())\n",
    "            tokenized_batches[\"attention_mask\"].append(\n",
    "                tokenized_output[\"attention_mask\"][0].tolist()\n",
    "            )\n",
    "\n",
    "    return tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neural_signals(data, time_tensor, neuron_idx=None, yax_limit=True, suptitle=None):\n",
    "    assert isinstance(data, torch.Tensor), \"data must be a PyTorch tensor\"\n",
    "    assert isinstance(time_tensor, torch.Tensor), \"time_tensor must be a PyTorch tensor\"\n",
    "    assert data.dim() == 2, \"data must be a 2D tensor\"\n",
    "    assert isinstance(neuron_idx, (int, list)), \"neuron_idx must be an integer or list\"\n",
    "\n",
    "    time_tensor = time_tensor.squeeze()\n",
    "    assert data.size(0) == time_tensor.size(0), \"Number of rows in data and time_tensor must match\"\n",
    "\n",
    "    num_neurons = data.size(1)\n",
    "\n",
    "    # Randomly select the column indices if not provided\n",
    "    if isinstance(neuron_idx, int):\n",
    "        assert neuron_idx <= num_neurons, \"neuron_idx cannot exceed the number of neurons\"\n",
    "        column_indices = np.random.choice(num_neurons, neuron_idx, replace=False)\n",
    "    elif isinstance(neuron_idx, list):\n",
    "        assert len(neuron_idx) <= num_neurons, \"neuron_idx cannot exceed the number of neurons\"\n",
    "        column_indices = np.array(neuron_idx)\n",
    "\n",
    "    num_columns = len(column_indices)\n",
    "\n",
    "    # Extract the selected columns from the data tensor\n",
    "    selected_columns = data[:, column_indices]\n",
    "\n",
    "    # Define the color palette using scientific colors\n",
    "    colors = sns.color_palette(\"bright\", num_columns)\n",
    "\n",
    "    # Plotting subplots vertically\n",
    "    fig, axs = plt.subplots(num_columns, 1, figsize=(15, num_columns))\n",
    "    fig.tight_layout(pad=0.0)\n",
    "\n",
    "    # If num_columns is 1, make ax iterable by wrapping it in a list\n",
    "    if num_columns == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Now your existing loop should work without modification\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.plot(time_tensor, selected_columns[:, i], color=colors[i])\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position(\"none\")\n",
    "        if yax_limit:\n",
    "            ax.set_ylim(-1.0, 1.0)\n",
    "        ax.set_ylabel(\"{}\".format(NEURONS_302[column_indices[i]]))\n",
    "\n",
    "        if i < num_columns - 1:\n",
    "            ax.set_xticks([])\n",
    "        else:\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "\n",
    "    # Add a super title to the figure if provided\n",
    "    if suptitle is not None:\n",
    "        fig.suptitle(suptitle, fontsize=16)\n",
    "\n",
    "    plt.tight_layout(pad=1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_trajectory(X, axis_labels=(\"Time\", \"Value\", \"Z Axis\"), title=\"Trajectory\", show=True):\n",
    "    \"\"\"\n",
    "    Plot a trajectory from a dataset, which can be 1D, 2D, or 3D.\n",
    "\n",
    "    Parameters:\n",
    "    - X: A 2D numpy array containing the trajectory data.\n",
    "    - axis_labels: A tuple containing the labels for the axes. Default is ('Time', 'Value', 'Z Axis').\n",
    "    - title: Title of the plot.\n",
    "    - show: If True, the plot will be displayed. If False, the plot object will be returned.\n",
    "\n",
    "    Returns:\n",
    "    - fig, ax: The figure and axis objects of the plot if show is False.\n",
    "    \"\"\"\n",
    "    max_timesteps = X.shape[0]\n",
    "    dims = X.shape[1] if len(X.shape) > 1 else 1\n",
    "\n",
    "    # Create a new figure for the plot\n",
    "    fig = plt.figure()\n",
    "    if dims >= 3:\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        plot_dims = 3\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "        plot_dims = dims\n",
    "\n",
    "    # Create a color map based on the time progression\n",
    "    norm = plt.Normalize(0, max_timesteps)\n",
    "    colors = plt.cm.viridis(norm(np.arange(max_timesteps)))\n",
    "\n",
    "    # Extract coordinates from the data and plot accordingly\n",
    "    if plot_dims == 1:\n",
    "        x = np.arange(max_timesteps)\n",
    "        y = X.flatten()\n",
    "        z = np.zeros(max_timesteps)\n",
    "        ax_labels = (\"Time\", axis_labels[0], \"Fixed Z\")\n",
    "        # Plot the 1D trajectory with a color gradient\n",
    "        for i in range(1, max_timesteps):\n",
    "            ax.plot(x[i - 1 : i + 1], y[i - 1 : i + 1], color=colors[i], lw=0.5)\n",
    "        # Mark the start and end of the trajectory\n",
    "        ax.plot(x[0], y[0], \"g*\", markersize=8)  # Start with a green star\n",
    "        ax.plot(x[-1], y[-1], \"ro\", markersize=7)  # End with a red circle\n",
    "    elif plot_dims == 2:\n",
    "        x, y = X[:, 0], X[:, 1]\n",
    "        z = np.zeros(max_timesteps)\n",
    "        ax_labels = (axis_labels[0], axis_labels[1], \"Fixed Z\")\n",
    "        # Plot the 2D trajectory\n",
    "        for i in range(1, max_timesteps):\n",
    "            ax.plot(x[i - 1 : i + 1], y[i - 1 : i + 1], color=colors[i], lw=0.5)\n",
    "        # Mark the start and end of the trajectory\n",
    "        ax.plot(x[0], y[0], \"g*\", markersize=8)  # Start with a green star\n",
    "        ax.plot(x[-1], y[-1], \"ro\", markersize=7)  # End with a red circle\n",
    "    else:  # plot_dims == 3\n",
    "        x, y, z = X[:, 0], X[:, 1], X[:, 2]\n",
    "        ax_labels = axis_labels\n",
    "        # Plot the 3D trajectory\n",
    "        for i in range(1, max_timesteps):\n",
    "            ax.plot(\n",
    "                x[i - 1 : i + 1],\n",
    "                y[i - 1 : i + 1],\n",
    "                z[i - 1 : i + 1],\n",
    "                color=colors[i],\n",
    "                lw=0.5,\n",
    "            )\n",
    "        # Mark the start and end of the trajectory\n",
    "        ax.plot(x[0], y[0], z[0], \"g*\", markersize=8)  # Start with a green star\n",
    "        ax.plot(x[-1], y[-1], z[-1], \"ro\", markersize=7)  # End with a red circle\n",
    "\n",
    "    # Set labels for the axes\n",
    "    ax.set_xlabel(ax_labels[0])\n",
    "    ax.set_ylabel(ax_labels[1])\n",
    "    if plot_dims == 3:\n",
    "        ax.set_zlabel(ax_labels[2])\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autocorrelation_and_pacf(X, neurons):\n",
    "    \"\"\"\n",
    "    Plot the autocorrelation and partial autocorrelation for each neuron's trajectory.\n",
    "\n",
    "    Parameters:\n",
    "    - X: A 2D numpy array of shape (max_timesteps, num_neurons) containing the neural trajectory data.\n",
    "    - neurons: A list or array containing the neuron identifiers.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function creates and displays a plot.\n",
    "    \"\"\"\n",
    "    # Number of neurons\n",
    "    num_neurons = X.shape[1]\n",
    "\n",
    "    # Create a figure and an array of subplots with 2 columns\n",
    "    fig, axes = plt.subplots(nrows=num_neurons, ncols=2, figsize=(10, 2 * num_neurons))\n",
    "\n",
    "    # Set a suptitle for the entire figure\n",
    "    fig.suptitle(\"Autocorrelation and Partial Autocorrelation for Neurons\")\n",
    "\n",
    "    # Iterate over the number of neurons to create individual plots\n",
    "    for i in range(num_neurons):\n",
    "        # Plot ACF on the left column (0th index)\n",
    "        ax_left = axes[i, 0] if num_neurons > 1 else axes[0]\n",
    "        plot_acf(X[:, i], ax=ax_left, use_vlines=False, marker=\"v\", linestyle=\"--\")\n",
    "        ax_left.set_title(f\"Neuron {neurons[i]} ACF\")\n",
    "\n",
    "        # Plot PACF on the right column (1st index)\n",
    "        ax_right = axes[i, 1] if num_neurons > 1 else axes[1]\n",
    "        plot_pacf(X[:, i], ax=ax_right, use_vlines=False, marker=\"*\", linestyle=\"--\")\n",
    "        ax_right.set_title(f\"Neuron {neurons[i]} PACF\")\n",
    "        ax_right.set_ylabel(\"\")  # Clear the y-axis label to prevent clutter\n",
    "\n",
    "        # Adjust y-axis limits so there is some white space around -1 and 1\n",
    "        ax_left.set_ylim(-1.1, 1.1)\n",
    "        ax_right.set_ylim(-1.1, 1.1)\n",
    "\n",
    "        # Only the bottom plots need x-axis labels\n",
    "        if i < num_neurons - 1:\n",
    "            ax_left.set_xlabel(\"\")\n",
    "            ax_right.set_xlabel(\"\")\n",
    "        else:\n",
    "            ax_left.set_xlabel(\"Lag\")\n",
    "            ax_right.set_xlabel(\"Lag\")\n",
    "\n",
    "    # Adjust the layout to not overlap plots\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    # Show the plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_delay_embedding(x, delay, dimension):\n",
    "    \"\"\"\n",
    "    Constructs a time-delay embedding matrix from time series data.\n",
    "\n",
    "    :param x: Time series data as a 1D numpy array.\n",
    "    :param delay: The delay tau between time series elements in the embedding.\n",
    "    :param dimension: The embedding dimension m.\n",
    "    :return: The time-delay embedded data as a 2D numpy array.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if n < (dimension - 1) * delay + 1:\n",
    "        raise ValueError(\"Time series data is too short for the given delay and dimension.\")\n",
    "    m = n - (dimension - 1) * delay\n",
    "    embedded_data = np.empty((m, dimension))\n",
    "    for i in range(m):\n",
    "        for j in range(dimension):\n",
    "            embedded_data[i, j] = x[i + j * delay]\n",
    "    return embedded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz(X, t, sigma=10.0, beta=8.0 / 3, rho=28.0):\n",
    "    \"\"\"The Lorenz equations.\"\"\"\n",
    "    x, y, z = X\n",
    "    dx_dt = sigma * (y - x)\n",
    "    dy_dt = x * (rho - z) - y\n",
    "    dz_dt = x * y - beta * z\n",
    "    return [dx_dt, dy_dt, dz_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def van_der_pol_oscillator(X, t, mu=1.0):\n",
    "    \"\"\"The Van der Pol oscillator equations.\"\"\"\n",
    "    x, y = X\n",
    "    dx_dt = y\n",
    "    dy_dt = mu * (1 - x**2) * y - x\n",
    "    return [dx_dt, dy_dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidsOscillator:\n",
    "    def __init__(self, sum_freqs=0, random=True):\n",
    "        self.eps = np.finfo(float).eps\n",
    "        self.sum_freqs = sum_freqs\n",
    "        self.random = random\n",
    "        # create frequencies and phases\n",
    "        if self.random:  # use random frequencies\n",
    "            self.freqs = np.round(self.eps + np.random.random(self.sum_freqs) / 10, 4)\n",
    "        else:  # use deterministic frequencies\n",
    "            self.freqs = np.round(\n",
    "                np.arange(1 / (2 * self.sum_freqs + self.eps), self.sum_freqs)\n",
    "                / (10 * self.sum_freqs),\n",
    "                4,\n",
    "            )\n",
    "        # always use random phases\n",
    "        self.phases = np.random.random(self.sum_freqs) * 2 * np.pi\n",
    "\n",
    "    def __call__(self, Y, t):\n",
    "        y = Y\n",
    "        dy_dt = -y + sum(np.cos(2 * np.pi * f * t + p) for f, p in zip(self.freqs, self.phases))\n",
    "        return dy_dt\n",
    "\n",
    "\n",
    "# create an instance of a SinusoidsOscillator dynamical system\n",
    "sinusoids_oscillator = SinusoidsOscillator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_synthetic_dataset(file_name, dataset):\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_shakespeare(\n",
    "    max_timesteps: int = 1000,\n",
    "    num_worms: int = 1,\n",
    "    num_signals: int = 302,\n",
    "    num_named_neurons: Union[None, int] = None,\n",
    "    tokenizer: Union[None, callable] = None,\n",
    "    embedding_weight: Union[None, torch.Tensor] = None,\n",
    "    smooth_method: Union[None, str] = \"ES\",\n",
    "    transform: Union[None, callable] = CausalNormalizer(), #StandardScaler(),\n",
    "    dataset_name: str = \"Shakespeare0000\",\n",
    "):\n",
    "    # Load the Shakespeare dataset\n",
    "    text_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "    # Create a tokenizer\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-large\")\n",
    "    # Apply the tokenization and chunking to each split\n",
    "    text_dataset = text_dataset.map(\n",
    "        tokenize_and_chunk, batched=True, fn_kwargs=dict(tokenizer=tokenizer)\n",
    "    )\n",
    "    # Create an embedding table\n",
    "    embedding_dim = num_signals\n",
    "    assert isinstance(embedding_weight, torch.Tensor) and embedding_weight.shape == (\n",
    "        tokenizer.vocab_size,\n",
    "        embedding_dim,\n",
    "    ), \"embedding_weight must be a PyTorch tensor of shape (tokenizer.vocab_size, embedding_dim)\"\n",
    "    embedding = torch.nn.Embedding(\n",
    "        num_embeddings=tokenizer.vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        dtype=torch.half,\n",
    "        _weight=embedding_weight,\n",
    "        _freeze=True,\n",
    "    )\n",
    "    torch.nn.init.normal_(embedding.weight)\n",
    "    # Extract all splits of the text dataset then concatenate them\n",
    "    train_tokens = text_dataset[\"train\"][\"input_ids\"]\n",
    "    validation_tokens = text_dataset[\"validation\"][\"input_ids\"]\n",
    "    test_tokens = text_dataset[\"test\"][\"input_ids\"]\n",
    "    all_tokens = train_tokens + validation_tokens + test_tokens\n",
    "    # Set up for creating the synthetic dataset\n",
    "    if num_named_neurons is None or num_named_neurons > num_signals:  # default to all neurons\n",
    "        num_named_neurons = num_signals\n",
    "    elif num_named_neurons < 0:  # default to no neurons\n",
    "        num_named_neurons = 0\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "    dataset = dict()\n",
    "    # Create data for as many worms as possible\n",
    "    worm_idx = 0\n",
    "    calcium_data = []\n",
    "    total_time = 0\n",
    "    for chunk in all_tokens:\n",
    "        if worm_idx > num_worms - 1:\n",
    "            break\n",
    "        embd_data = embedding(torch.LongTensor(chunk)).detach().numpy()\n",
    "        calcium_data.append(embd_data)\n",
    "        total_time += embd_data.shape[0]\n",
    "        if total_time >= max_timesteps:\n",
    "            calcium_data = np.vstack(calcium_data)\n",
    "            if transform:\n",
    "                calcium_data = transform.fit_transform(calcium_data)\n",
    "            time_in_seconds = np.arange(total_time).reshape(-1, 1)\n",
    "            dt = np.gradient(time_in_seconds, axis=0)\n",
    "            dt[dt == 0] = np.finfo(float).eps\n",
    "            resample_dt = np.round(np.median(dt).item(), 2)\n",
    "            residual_calcium = np.gradient(calcium_data, axis=0) / dt\n",
    "            smooth_calcium_data = smooth_data_preprocess(\n",
    "                calcium_data, time_in_seconds, smooth_method\n",
    "            )\n",
    "            smooth_residual_calcium = smooth_data_preprocess(\n",
    "                residual_calcium, time_in_seconds, smooth_method\n",
    "            )\n",
    "            named_neuron_indices = random.sample(\n",
    "                range(num_signals), num_named_neurons\n",
    "            )  # without replacement\n",
    "            named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "            neuron_to_idx = {\n",
    "                (neuron) if neuron in named_neurons else str(idx): idx\n",
    "                for idx, neuron in enumerate(NEURONS_302)\n",
    "            }\n",
    "            idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "\n",
    "            worm_data = dict()\n",
    "\n",
    "            worm_data[\"worm\"] = f\"worm{worm_idx}\"\n",
    "            worm_data[\"dataset\"] = dataset_name\n",
    "            worm_data[\"smooth_method\"] = smooth_method\n",
    "            worm_data[\"calcium_data\"] = calcium_data\n",
    "            worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "            worm_data[\"residual_calcium\"] = residual_calcium\n",
    "            worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "            worm_data[\"max_timesteps\"] = total_time\n",
    "            worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "            worm_data[\"dt\"] = dt\n",
    "            worm_data[\"resample_median_dt\"] = resample_dt\n",
    "            worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "            worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "            worm_data[\"num_neurons\"] = num_signals\n",
    "            worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "            worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "\n",
    "            worm_data = reshape_calcium_data(worm_data)\n",
    "            dataset[f\"worm{worm_idx}\"] = worm_data\n",
    "\n",
    "            worm_idx += 1\n",
    "            calcium_data = []\n",
    "            total_time = 0\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "max_timesteps = 1500\n",
    "num_worms = 200\n",
    "num_signals = 302\n",
    "num_named_neurons = 50  # None\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-large\")  # None\n",
    "embedding_weight = torch.randn(tokenizer.vocab_size, num_signals)  # None\n",
    "smooth_method = None\n",
    "transform = None\n",
    "dataset_name = \"Shakespeare0000\"\n",
    "\n",
    "# Creating and saving datasets\n",
    "dataset = create_synthetic_dataset_shakespeare(\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_named_neurons=num_named_neurons,\n",
    "    tokenizer=tokenizer,\n",
    "    embedding_weight=embedding_weight,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Get the number of worms in the dataset\n",
    "num_worms = len(dataset)\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "num_worms = len(dataset)\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][:num_named_neurons]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"smooth_calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sines dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_sines(\n",
    "    max_timesteps: int = 1000,\n",
    "    num_worms: int = 1,\n",
    "    num_signals: int = 302,\n",
    "    num_named_neurons: Union[None, int] = None,\n",
    "    add_noise: bool = False,\n",
    "    noise_std: float = 0.01,\n",
    "    random_walk: bool = False,\n",
    "    sum_frequencies: int = 1,\n",
    "    random_freqs: bool = False,\n",
    "    delta_seconds: float = 0.5,\n",
    "    smooth_method: Union[None, str] = \"ES\",\n",
    "    transform: Union[None, callable] = CausalNormalizer(), #StandardScaler(),\n",
    "    dataset_name: str = \"Sines0000\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a synthetic worm datasets using a sum of sinusoidal functions.\n",
    "    Out of the `num_signals` neurons, `num_named_neurons` neurons are chosen randomly\n",
    "    to have non-zero (i.e. \"measured\") signals. Each neuron's calcium signal is a sinusoid\n",
    "    with `sum_frequencies` frequencies added together and a random phase.\n",
    "\n",
    "    :param max_timesteps: The number of timepoints of synthetic data to generate.\n",
    "    :param num_worms: The number of synthetic worms to create datasets for.\n",
    "    :param num_signals: The number of signals corresponding to number of neurons.\n",
    "    :param num_named_neurons: The number of named neurons to create non-zero signals for.\n",
    "    :param add_noise: Whether to add Gaussian noise to the synthetic data.\n",
    "    :param noise_std: The standard deviation of the noise.\n",
    "    :param random_walk: If True, use a random walk to generate the noise. Otherwise, use iid noise.\n",
    "    :param sum_frequencies: The number of sine functions to add together to create a signal.\n",
    "    :param random_freqs: If True, use random frequencies for each neuron. Otherwise, use the same frequencies for all neurons.\n",
    "    :param delta_seconds: The constant time difference (in seconds) between each timepoint.\n",
    "    :param smooth_method: The method to use for smoothing the data.\n",
    "    :param transform: The sklearn method to scale or transform the data before use.\n",
    "    :param dataset_name: The name to give the synthetic dataset.\n",
    "    :return: A dictionary containing the synthetic worm datasets.\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    dataset = {}\n",
    "    time_in_seconds = delta_seconds * np.arange(max_timesteps).reshape(-1, 1)  # Column vector\n",
    "\n",
    "    # Create a memo of neurons to frequencies so that distinct neurons have distinct frequency signatures\n",
    "    neuron_to_frequencies = dict()  # this will be consistent across worms\n",
    "\n",
    "    # Calculation of unknown neurons\n",
    "    if num_named_neurons is None or num_named_neurons > num_signals:  # default to all neurons\n",
    "        num_named_neurons = num_signals\n",
    "    elif num_named_neurons < 0:  # default to no neurons\n",
    "        num_named_neurons = 0\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "\n",
    "    # Create data for each worm\n",
    "    for worm_idx in range(num_worms):\n",
    "        # Create a memo of neurons to phases so that distinct neurons have distinct phase signatures\n",
    "        neuron_to_phases = dict()  # this will vary across worms\n",
    "\n",
    "        # Initialize worm data\n",
    "        worm = f\"worm{worm_idx}\"\n",
    "        worm_data = {}\n",
    "        calcium_data = np.zeros((max_timesteps, num_signals))\n",
    "        named_neuron_indices = random.sample(\n",
    "            range(num_signals), num_named_neurons\n",
    "        )  # without replacement\n",
    "        named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "\n",
    "        # Create neuron to idx mapping and vice versa\n",
    "        neuron_to_idx = {\n",
    "            (neuron) if neuron in named_neurons else str(idx): idx\n",
    "            for idx, neuron in enumerate(NEURONS_302)\n",
    "        }\n",
    "        idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "\n",
    "        # Create calcium data\n",
    "        for i in named_neuron_indices:\n",
    "            neuron_index = i\n",
    "            neuron_name = NEURONS_302[neuron_index]\n",
    "\n",
    "            frequencies = neuron_to_frequencies.setdefault(\n",
    "                neuron_name,\n",
    "                np.round(\n",
    "                    eps + np.random.random(sum_frequencies) / 10, 4\n",
    "                ).tolist()  # random frequencies\n",
    "                if random_freqs\n",
    "                else np.round(\n",
    "                    np.arange(1 / (2 * sum_frequencies + eps), sum_frequencies)\n",
    "                    / (10 * sum_frequencies),\n",
    "                    4,\n",
    "                ).tolist(),  # deterministic frequencies\n",
    "            )\n",
    "\n",
    "            phases = neuron_to_phases.setdefault(\n",
    "                neuron_name,\n",
    "                [np.random.random() * 2 * np.pi for _ in range(sum_frequencies)],  # random phase\n",
    "            )\n",
    "\n",
    "            time_points = np.arange(max_timesteps)\n",
    "\n",
    "            # Add sine functions\n",
    "            for freq, phase in zip(frequencies, phases):\n",
    "                calcium_data[:, neuron_index] += np.sin(2 * np.pi * freq * time_points + phase)\n",
    "\n",
    "            # Add noise\n",
    "            if add_noise:\n",
    "                if random_walk:\n",
    "                    noise_walk = np.cumsum(\n",
    "                        [0]\n",
    "                        + np.random.normal(loc=0, scale=noise_std, size=max_timesteps - 1).tolist()\n",
    "                    )\n",
    "                    calcium_data[:, neuron_index] += noise_walk\n",
    "                else:\n",
    "                    noise_iid = np.random.normal(0, noise_std, max_timesteps)\n",
    "                    calcium_data[:, neuron_index] += noise_iid\n",
    "\n",
    "        # Normalize data\n",
    "        if transform:\n",
    "            calcium_data = transform.fit_transform(calcium_data)\n",
    "\n",
    "        # Calculate residuals\n",
    "        dt = np.gradient(time_in_seconds, axis=0)\n",
    "        dt[dt == 0] = eps\n",
    "        resample_dt = np.round(np.median(dt).item(), 2)\n",
    "        residual_calcium = np.gradient(calcium_data, axis=0) / dt\n",
    "\n",
    "        # Smooth the data\n",
    "        smooth_calcium_data = smooth_data_preprocess(calcium_data, time_in_seconds, smooth_method)\n",
    "        smooth_residual_calcium = smooth_data_preprocess(\n",
    "            residual_calcium, time_in_seconds, smooth_method\n",
    "        )\n",
    "\n",
    "        # Save the data\n",
    "        worm_data[\"worm\"] = worm\n",
    "        worm_data[\"dataset\"] = dataset_name\n",
    "        worm_data[\"smooth_method\"] = smooth_method\n",
    "        worm_data[\"calcium_data\"] = calcium_data\n",
    "        worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "        worm_data[\"residual_calcium\"] = residual_calcium\n",
    "        worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "        worm_data[\"max_timesteps\"] = max_timesteps\n",
    "        worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "        worm_data[\"dt\"] = dt\n",
    "        worm_data[\"resample_median_dt\"] = resample_dt\n",
    "        worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "        worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "        worm_data[\"num_neurons\"] = num_signals\n",
    "        worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "        worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "\n",
    "        # Reshape the data to the standardized format\n",
    "        worm_data = reshape_calcium_data(worm_data)\n",
    "\n",
    "        # Save the data\n",
    "        dataset[worm] = worm_data\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "max_timesteps = 1500\n",
    "num_worms = 200\n",
    "num_signals = 302\n",
    "num_named_neurons = 50\n",
    "add_noise = False\n",
    "noise_std = 0.1\n",
    "random_walk = False\n",
    "sum_frequencies = 1\n",
    "random_freqs = True\n",
    "delta_seconds = 1.0\n",
    "smooth_method = None\n",
    "transform = None\n",
    "dataset_name = \"Sines0000\"\n",
    "\n",
    "# Creating and saving datasets\n",
    "dataset = create_synthetic_dataset_sines(\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_named_neurons=num_named_neurons,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    sum_frequencies=sum_frequencies,\n",
    "    random_freqs=random_freqs,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][:num_named_neurons]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"smooth_calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocorrelation\n",
    "---\n",
    "Autocorrelation and partial autocorrelation are concepts used in time series analysis to examine the relationship of a variable with itself at different lags.\n",
    "\n",
    "**Autocorrelation Function (ACF):**\n",
    "- This is a correlation of a signal with a delayed copy of itself as a function of delay.\n",
    "- The autocorrelation plot (or ACF plot) displays the correlation between the time series and its lagged values.\n",
    "- The y-axis shows the autocorrelation coefficient, which ranges from -1 to 1. A value of 1 indicates perfect positive correlation, while -1 indicates perfect negative correlation.\n",
    "- The x-axis represents the lag at which the autocorrelation is computed. For example, at lag 1, you're comparing the series with itself one time step back.\n",
    "- The ACF considers the combined effect of all previous time points up to the lagged time point being calculated. It doesn't isolate the correlation at each lag.\n",
    "\n",
    "**Partial Autocorrelation Function (PACF):**\n",
    "- This measures the correlation between the time series and its lagged version after accounting for the variations already explained by the intervening comparisons. In other words, it controls for the previous lags.\n",
    "- The PACF plot shows the partial correlation of a time series with its own lagged values, controlling for the values of the time series at all shorter lags.\n",
    "- It isolates the additional correlation at a particular lag that is not explained by previous lags. This is useful to identify the actual lagged relationship, without the confounding effects of the correlations at shorter lags.\n",
    "\n",
    "In summary, while the ACF shows the cumulative effect of past data points on the current value, the PACF removes this cumulative effect and tries to show the unique contribution of each past data point.\n",
    "\n",
    "In practical applications like ARIMA (AutoRegressive Integrated Moving Average) modeling, the ACF and PACF are used to determine the AR (autoregressive) and MA (moving average) terms:\n",
    "\n",
    "* The PACF helps to identify the order of the AR part (p) by finding the lag after which the PACF cuts off (drops to zero or below the significance level).\n",
    "* The ACF is used to identify the order of the MA part (q) by finding the lag after which the ACF cuts off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting autocorrelation and partial autocorrelation\n",
    "\n",
    "# X is our data matrix (time, untis) and neurons is a list of names for each unit\n",
    "plot_autocorrelation_and_pacf(X, neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lorenz dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_lorenz(\n",
    "    max_timesteps: int = 1000,\n",
    "    num_worms: int = 1,\n",
    "    num_signals: int = 302,\n",
    "    num_lorenz_systems: int = 1,\n",
    "    add_noise: bool = False,\n",
    "    noise_std: float = 0.01,\n",
    "    random_walk: bool = False,\n",
    "    fine_integration_step: float = 0.1,\n",
    "    delta_seconds: float = 0.5,\n",
    "    smooth_method: Union[None, str] = \"ES\",\n",
    "    transform: Union[None, callable] = CausalNormalizer(), #StandardScaler(),\n",
    "    dataset_name: str = \"Lorenz0000\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a synthetic worm datasets using the Lorenz attractor.\n",
    "    Three neurons are chosen randomly to represent x, y, z trajectories from the Lorenz system.\n",
    "\n",
    "    :param max_timesteps: The number of timepoints of synthetic data to generate.\n",
    "    :param num_worms: The number of synthetic worms to create datasets for.\n",
    "    :param num_signals: The number of signals corresponding to number of neurons.\n",
    "    :param num_lorenz_systems: The number of Lorenz systems to use for creating the synthetic data.\n",
    "    :param add_noise: Whether to add Gaussian noise to the synthetic data.\n",
    "    :param noise_std: The standard deviation of the noise.\n",
    "    :param random_walk: If True, use a random walk to generate the noise. Otherwise, use iid noise.\n",
    "    :param fine_integration_step: The integration step size for the Lorenz system.\n",
    "    :param delta_seconds: The constant time difference (in seconds) between each measurement of the system.\n",
    "    :param smooth_method: The method to use for smoothing the data.\n",
    "    :param transform: The sklearn method to scale or transform the data before use.\n",
    "    :param dataset_name: The name to give the synthetic dataset.\n",
    "    :return: A dictionary containing the synthetic worm datasets.\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    dataset = {}\n",
    "\n",
    "    # Determine the timepoints for sampling the data (i.e. \"measurement\" times)\n",
    "    time_in_seconds = delta_seconds * np.arange(max_timesteps).reshape(-1, 1)  # Column vector\n",
    "\n",
    "    # Create a memo of neurons to state variable so that distinct neurons represent a distinct variable\n",
    "    neuron_to_state_var = dict()  # this will be consistent across worms\n",
    "\n",
    "    # Calculate number of named and unknown neurons\n",
    "    num_named_neurons = num_lorenz_systems * 3\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "\n",
    "    # Fine integration time points for the true underlying dynamics\n",
    "    fine_time_points = np.arange(0, max_timesteps * delta_seconds, fine_integration_step)\n",
    "\n",
    "    # Create data for each worm\n",
    "    for worm_idx in range(num_worms):\n",
    "        # Initialize worm data\n",
    "        worm = f\"worm{worm_idx}\"\n",
    "        worm_data = {}\n",
    "        calcium_data = np.zeros((max_timesteps, num_signals))\n",
    "        named_neuron_indices = random.sample(\n",
    "            range(num_signals), num_named_neurons\n",
    "        )  # without replacement\n",
    "        named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "\n",
    "        # Create neuron to idx mapping and vice versa\n",
    "        neuron_to_idx = {\n",
    "            (neuron) if neuron in named_neurons else str(idx): idx\n",
    "            for idx, neuron in enumerate(NEURONS_302)\n",
    "        }\n",
    "        idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "\n",
    "        # Create calcium data\n",
    "        calcium_data = np.zeros((max_timesteps, num_signals))\n",
    "\n",
    "        # Create multiple independent Lorenz systems\n",
    "        for system_idx in range(num_lorenz_systems):\n",
    "            # Define a new set of initial conditions for each Lorenz system\n",
    "            initial_conditions = np.random.uniform(low=-5.0, high=5.0, size=3)\n",
    "            # np.random.randn(3)  # or other method to generate initial conditions\n",
    "\n",
    "            # Integrate the Lorenz system with different initial conditions\n",
    "            fine_trajectories = odeint(lorenz, initial_conditions, fine_time_points)\n",
    "\n",
    "            # Downsample to get the \"measured\" data at delta_seconds intervals\n",
    "            downsample_indices = np.arange(\n",
    "                0, len(fine_time_points), int(delta_seconds / fine_integration_step)\n",
    "            )\n",
    "            sampled_trajectories = fine_trajectories[downsample_indices]\n",
    "\n",
    "            # Assign to the correct set of neurons for this Lorenz system\n",
    "            for i in range(3):\n",
    "                neuron_index = named_neuron_indices[system_idx * 3 + i]\n",
    "                neuron_name = NEURONS_302[neuron_index]\n",
    "\n",
    "                # Uniquely map each neuron to a committed state variable\n",
    "                var = neuron_to_state_var.setdefault(neuron_name, i)\n",
    "                calcium_data[:, neuron_index] = sampled_trajectories[:, var]\n",
    "\n",
    "                # Add noise\n",
    "                if add_noise:\n",
    "                    if random_walk:\n",
    "                        noise_walk = np.cumsum(\n",
    "                            [0]\n",
    "                            + np.random.normal(\n",
    "                                loc=0, scale=noise_std, size=max_timesteps - 1\n",
    "                            ).tolist()\n",
    "                        )\n",
    "                        calcium_data[:, neuron_index] += noise_walk\n",
    "                    else:\n",
    "                        noise_iid = np.random.normal(0, noise_std, max_timesteps)\n",
    "                        calcium_data[:, neuron_index] += noise_iid\n",
    "\n",
    "        # Normalize data\n",
    "        if transform:\n",
    "            calcium_data = transform.fit_transform(calcium_data)\n",
    "\n",
    "        # Calculate residuals\n",
    "        dt = np.gradient(time_in_seconds, axis=0)\n",
    "        dt[dt == 0] = eps\n",
    "        resample_dt = np.round(np.median(dt).item(), 2)\n",
    "        residual_calcium = np.gradient(calcium_data, axis=0) / dt\n",
    "\n",
    "        # Smooth the data\n",
    "        smooth_calcium_data = smooth_data_preprocess(calcium_data, time_in_seconds, smooth_method)\n",
    "        smooth_residual_calcium = smooth_data_preprocess(\n",
    "            residual_calcium, time_in_seconds, smooth_method\n",
    "        )\n",
    "\n",
    "        # Save the data\n",
    "        worm_data[\"dataset\"] = dataset_name\n",
    "        worm_data[\"smooth_method\"] = smooth_method\n",
    "        worm_data[\"worm\"] = worm\n",
    "        worm_data[\"calcium_data\"] = calcium_data\n",
    "        worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "        worm_data[\"residual_calcium\"] = residual_calcium\n",
    "        worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "        worm_data[\"max_timesteps\"] = max_timesteps\n",
    "        worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "        worm_data[\"dt\"] = dt\n",
    "        worm_data[\"resample_median_dt\"] = resample_dt\n",
    "        worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "        worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "        worm_data[\"num_neurons\"] = num_signals\n",
    "        worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "        worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "\n",
    "        # Reshape the data to the standardized format\n",
    "        worm_data = reshape_calcium_data(worm_data)\n",
    "\n",
    "        # Save the data\n",
    "        dataset[worm] = worm_data\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "max_timesteps = 1500\n",
    "num_worms = 200\n",
    "num_signals = 302\n",
    "num_lorenz_systems = 17\n",
    "add_noise = False\n",
    "noise_std = 0.1\n",
    "random_walk = False\n",
    "fine_integration_step = 0.04\n",
    "delta_seconds = 0.04\n",
    "smooth_method = None\n",
    "transform = None\n",
    "dataset_name = \"Lorenz0000\"\n",
    "\n",
    "# Creating and saving datasets\n",
    "dataset = create_synthetic_dataset_lorenz(\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_lorenz_systems=num_lorenz_systems,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    fine_integration_step=fine_integration_step,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][: num_lorenz_systems * 3]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} dataset - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"smooth_calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Delay Embedding\n",
    "---\n",
    "Time delay embedding is a technique used in the analysis of dynamical systems, particularly in the context of reconstructing the phase space of a system from a series of observations over time. This technique is based on Takens' Embedding Theorem, which states that the dynamics of a system can be reconstructed from the time series of a single observable of the system, under certain conditions.\n",
    "\n",
    "Here are some key points about time delay embedding:\n",
    "\n",
    "- **Time Delay,  (tau):** This is the time interval between successive observations in the reconstructed phase space. Choosing an appropriate  is crucial; too short a delay may lead to redundant information, while too long a delay may lose the dynamics of interest.\n",
    "\n",
    "- **Embedding Dimension, m:** This represents the number of delayed observations used to reconstruct the phase space. It should be high enough to unfold the dynamics, but not too high to avoid overcomplicating the model.\n",
    "\n",
    "- **Phase Space Reconstruction:** By plotting the time-delayed copies of the time series against each other, one can reconstruct the phase space, which can reveal underlying dynamical properties like attractors or limit cycles.\n",
    "\n",
    "- **Mutual Information:** To empirically choose the right , one common method is to calculate the mutual information between the time series and its delayed version, and select  at the first minimum of the mutual information function.\n",
    "\n",
    "- **False Nearest Neighbors (FNN):** The method of False Nearest Neighbors can help determine a suitable embedding dimension m by identifying when points that appear to be neighbors in lower-dimensional space are no longer neighbors in higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Let's assume you have a time series data stored in a variable called `time_series_data`.\n",
    "idx = 1\n",
    "time_series_data = X[:, idx]\n",
    "# You would first determine `tau` and `m` based on your data.\n",
    "\n",
    "tau = 1  # replace with the delay you've computed or chosen\n",
    "m = 3  # replace with the embedding dimension you've computed or chosen\n",
    "\n",
    "# You can then compute the time-delay embedding of your data as follows:\n",
    "embedded_data = time_delay_embedding(time_series_data, tau, m)\n",
    "plot_3d_trajectory(embedded_data, title=f\"Time delay embedding of {neurons[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamics datasets\n",
    "___\n",
    "\n",
    "We attempt to generalize the above approach to generating synthetic data fromarbitrary dynamical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_dynamics(\n",
    "    dynamics_func: callable,\n",
    "    state_dim: int,\n",
    "    func_args: tuple = (),\n",
    "    max_timesteps: int = 1000,\n",
    "    num_worms: int = 1,\n",
    "    num_signals: int = 302,\n",
    "    num_dynamic_systems: int = 1,\n",
    "    add_noise: bool = False,\n",
    "    random_walk: bool = False,\n",
    "    noise_std: float = 0.01,\n",
    "    fine_integration_step: float = 0.1,\n",
    "    delta_seconds: float = 0.5,\n",
    "    smooth_method: Union[None, str] = \"ES\",\n",
    "    transform: Union[None, callable] = CausalNormalizer(), #StandardScaler(),\n",
    "    dataset_name: str = \"Dynamics0000\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a synthetic worm dataset using a user-defined dynamical system.\n",
    "\n",
    "    Parameters:\n",
    "    :param dynamics_func: The function defining the dynamics, should take arguments (y, t, *args).\n",
    "    :param state_dim: The dimension of the state vector y to the dynamics function.\n",
    "    :param func_args: Any extra arguments to pass to the dynamics function.\n",
    "    :param max_timesteps: The number of timepoints of synthetic data to generate.\n",
    "    :param num_worms: The number of synthetic worms to create datasets for.\n",
    "    :param num_signals: The number of signals corresponding to number of neurons.\n",
    "    :param num_dynamic_systems: The number of independent dynamical systems to use for creating the synthetic data.\n",
    "    :param add_noise: Whether to add Gaussian noise to the synthetic data.\n",
    "    :param noise_std: The standard deviation of the noise.\n",
    "    :param fine_integration_step: The integration step size for the Lorenz system.\n",
    "    :param delta_seconds: The constant time difference (in seconds) between each measurement of the system.\n",
    "    :param smooth_method: The method to use for smoothing the data.\n",
    "    :param transform: The sklearn method to scale or transform the data before use.\n",
    "    :param dataset_name: The name to give the synthetic dataset.\n",
    "\n",
    "    :return: A dictionary containing the synthetic worm datasets.\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    dataset = {}\n",
    "\n",
    "    # Determine the timepoints for sampling the data (i.e. \"measurement\" times)\n",
    "    time_in_seconds = delta_seconds * np.arange(max_timesteps).reshape(-1, 1)  # Column vector\n",
    "\n",
    "    # Create a memo of neurons to state variable so that distinct neurons represent a distinct variable\n",
    "    neuron_to_state_var = dict()  # this will be consistent across worms\n",
    "\n",
    "    # Calculate number of named and unknown neurons\n",
    "    num_named_neurons = num_dynamic_systems * state_dim\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "\n",
    "    # Add a warmup period to allow the system to reach a steady state\n",
    "    warmup_timesteps = 10\n",
    "\n",
    "    # Fine integration time points for the true underlying dynamics\n",
    "    fine_time_points = np.arange(\n",
    "        0, (max_timesteps + warmup_timesteps) * delta_seconds, fine_integration_step\n",
    "    )\n",
    "\n",
    "    # Create data for each worm\n",
    "    for worm_idx in range(num_worms):\n",
    "        # Initialize worm data\n",
    "        worm = f\"worm{worm_idx}\"\n",
    "        worm_data = {}\n",
    "        calcium_data = np.zeros((max_timesteps, num_signals))\n",
    "        named_neuron_indices = random.sample(\n",
    "            range(num_signals), num_named_neurons\n",
    "        )  # without replacement\n",
    "        named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "\n",
    "        # Create neuron to idx mapping and vice versa\n",
    "        neuron_to_idx = {\n",
    "            (neuron) if neuron in named_neurons else str(idx): idx\n",
    "            for idx, neuron in enumerate(NEURONS_302)\n",
    "        }\n",
    "        idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "\n",
    "        # Create calcium data\n",
    "        calcium_data = np.zeros((max_timesteps, num_signals))\n",
    "\n",
    "        # Create multiple independent systems; same dynamics but different initial conditions\n",
    "        for system_idx in range(num_dynamic_systems):\n",
    "            # Define a new set of initial conditions for each system\n",
    "            initial_conditions = np.random.randn(\n",
    "                state_dim\n",
    "            )  # or other method to generate initial conditions\n",
    "\n",
    "            # Integrate the dynamical system\n",
    "            fine_trajectories = odeint(\n",
    "                dynamics_func,\n",
    "                initial_conditions,\n",
    "                fine_time_points,\n",
    "                args=func_args,\n",
    "            )\n",
    "\n",
    "            # Downsample to get the \"measured\" data at delta_seconds intervals\n",
    "            downsample_indices = np.arange(\n",
    "                0, len(fine_time_points), int(delta_seconds / fine_integration_step)\n",
    "            )\n",
    "            sampled_trajectories = fine_trajectories[downsample_indices]\n",
    "\n",
    "            # Ignore the warmup period\n",
    "            sampled_trajectories = sampled_trajectories[warmup_timesteps:]\n",
    "\n",
    "            # Assign to the correct set of neurons for this dynamical system\n",
    "            for i in range(state_dim):\n",
    "                neuron_index = named_neuron_indices[system_idx * state_dim + i]\n",
    "                neuron_name = NEURONS_302[neuron_index]\n",
    "\n",
    "                # Uniquely map each neuron to a committed state variable\n",
    "                var = neuron_to_state_var.setdefault(neuron_name, i)\n",
    "                calcium_data[:, neuron_index] = sampled_trajectories[:, var]\n",
    "\n",
    "                # Add noise\n",
    "                if add_noise:\n",
    "                    if random_walk:\n",
    "                        noise_walk = np.cumsum(\n",
    "                            [0]\n",
    "                            + np.random.normal(\n",
    "                                loc=0, scale=noise_std, size=max_timesteps - 1\n",
    "                            ).tolist()\n",
    "                        )\n",
    "                        calcium_data[:, neuron_index] += noise_walk\n",
    "                    else:\n",
    "                        noise_iid = np.random.normal(0, noise_std, max_timesteps)\n",
    "\n",
    "                        calcium_data[:, neuron_index] += noise_iid\n",
    "\n",
    "        # Normalize data\n",
    "        if transform:\n",
    "            calcium_data = transform.fit_transform(calcium_data)\n",
    "\n",
    "        # Calculate residuals\n",
    "        dt = np.gradient(time_in_seconds, axis=0)\n",
    "        dt[dt == 0] = eps\n",
    "        resample_dt = np.round(np.median(dt).item(), 2)\n",
    "        residual_calcium = np.gradient(calcium_data, axis=0) / dt\n",
    "\n",
    "        # Smooth the data\n",
    "        smooth_calcium_data = smooth_data_preprocess(calcium_data, time_in_seconds, smooth_method)\n",
    "        smooth_residual_calcium = smooth_data_preprocess(\n",
    "            residual_calcium, time_in_seconds, smooth_method\n",
    "        )\n",
    "\n",
    "        # Save the data\n",
    "        worm_data[\"dataset\"] = dataset_name\n",
    "        worm_data[\"smooth_method\"] = smooth_method\n",
    "        worm_data[\"worm\"] = worm\n",
    "        worm_data[\"calcium_data\"] = calcium_data\n",
    "        worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "        worm_data[\"residual_calcium\"] = residual_calcium\n",
    "        worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "        worm_data[\"max_timesteps\"] = max_timesteps\n",
    "        worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "        worm_data[\"dt\"] = dt\n",
    "        worm_data[\"resample_median_dt\"] = resample_dt\n",
    "        worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "        worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "        worm_data[\"num_neurons\"] = num_signals\n",
    "        worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "        worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "\n",
    "        # Reshape the data to the standardized format\n",
    "        worm_data = reshape_calcium_data(worm_data)\n",
    "\n",
    "        # Save the data\n",
    "        dataset[worm] = worm_data\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Van Der Pol dataset\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "dynamics_func = van_der_pol_oscillator\n",
    "state_dim = 2\n",
    "func_args = ()\n",
    "max_timesteps = 1500\n",
    "num_worms = 200\n",
    "num_signals = 302\n",
    "num_dynamic_systems = 25\n",
    "add_noise = False\n",
    "noise_std = 0.1\n",
    "random_walk = False\n",
    "fine_integration_step = 0.05\n",
    "delta_seconds = 0.05\n",
    "smooth_method = None\n",
    "transform = None\n",
    "dataset_name = \"VanDerPol0000\"\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "dataset = create_synthetic_dataset_dynamics(\n",
    "    dynamics_func=dynamics_func,\n",
    "    state_dim=state_dim,\n",
    "    func_args=func_args,\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_dynamic_systems=num_dynamic_systems,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    fine_integration_step=fine_integration_step,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][\n",
    "    : num_dynamic_systems * state_dim\n",
    "]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} dataset - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"smooth_calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk dataset\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "dynamics_func = sinusoids_oscillator\n",
    "state_dim = 1\n",
    "func_args = ()\n",
    "max_timesteps = 1500\n",
    "num_worms = 200\n",
    "num_signals = 302\n",
    "num_dynamic_systems = 50\n",
    "add_noise = True\n",
    "noise_std = 0.1\n",
    "random_walk = True\n",
    "fine_integration_step = 1.0\n",
    "delta_seconds = 1.0\n",
    "smooth_method = None\n",
    "transform = None\n",
    "dataset_name = \"RandWalk0000\"\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "dataset = create_synthetic_dataset_dynamics(\n",
    "    dynamics_func=dynamics_func,\n",
    "    state_dim=state_dim,\n",
    "    func_args=func_args,\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_dynamic_systems=num_dynamic_systems,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    fine_integration_step=fine_integration_step,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][\n",
    "    : num_dynamic_systems * state_dim\n",
    "]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} dataset - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"smooth_calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Noise dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "dynamics_func = sinusoids_oscillator\n",
    "state_dim = 1\n",
    "func_args = ()\n",
    "max_timesteps = 1500\n",
    "num_worms = 200\n",
    "num_signals = 302\n",
    "num_dynamic_systems = 50\n",
    "add_noise = True\n",
    "noise_std = 0.1\n",
    "random_walk = False\n",
    "fine_integration_step = 1.0\n",
    "delta_seconds = 1.0\n",
    "smooth_method = None\n",
    "transform = None\n",
    "dataset_name = \"WhiteNoise0000\"\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "dataset = create_synthetic_dataset_dynamics(\n",
    "    dynamics_func=dynamics_func,\n",
    "    state_dim=state_dim,\n",
    "    func_args=func_args,\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_dynamic_systems=num_dynamic_systems,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    fine_integration_step=fine_integration_step,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a worm and all the neurons to plot\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][\n",
    "    : num_dynamic_systems * state_dim\n",
    "]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} dataset - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = dataset[worm_idx][\"smooth_calcium_data\"]\n",
    "mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
