{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import nbimporter\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tempfile import TemporaryDirectory\n",
    "from utils import DEVICE, BLOCK_SIZE\n",
    "from models._utils import NeuralTransformer\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "from preprocess._utils import smooth_data_preprocess, reshape_calcium_data\n",
    "from CreateSyntheticDataset import (\n",
    "    save_synthetic_dataset,\n",
    "    plot_neural_signals,\n",
    "    plot_3d_trajectory,\n",
    "    tokenize_and_chunk,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling with tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title HuggingFace Tokenizers\n",
    "# @markdown Note there are two ways to call the tokenizer's encoder.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "expl_text = \"Welcome to the ðŸ¤— Tokenizers library.\"\n",
    "impl_text = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
    "expl_encode = tokenizer.encode(expl_text)\n",
    "impl_encode = tokenizer(impl_text)\n",
    "print(\n",
    "    f\"Calling `tokenizer.encode(text)`:\\n\\ttext: {expl_text}\\n\\ttokenized: {expl_encode}\\n\\tdecoded: {tokenizer.decode(expl_encode)}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Calling `tokenizer(text)`:\\n\\tobject.keys(): {impl_encode.keys()}\\n\\ttext: {impl_text}\\n\\ttokenized: {impl_encode['input_ids']}\\n\\tdecoded: {tokenizer.decode(impl_encode['input_ids'])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title HuggingFace Datasets\n",
    "\n",
    "text_dataset = load_hf_dataset(\"tiny_shakespeare\")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"train:\",\n",
    "    type(text_dataset[\"train\"][\"text\"]),\n",
    "    len(text_dataset[\"train\"][\"text\"]),\n",
    "    type(text_dataset[\"train\"][\"text\"][0]),\n",
    "    len(text_dataset[\"train\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"validation:\",\n",
    "    type(text_dataset[\"validation\"][\"text\"]),\n",
    "    len(text_dataset[\"validation\"][\"text\"]),\n",
    "    type(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    len(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"test:\",\n",
    "    type(text_dataset[\"test\"][\"text\"]),\n",
    "    len(text_dataset[\"test\"][\"text\"]),\n",
    "    type(text_dataset[\"test\"][\"text\"][0]),\n",
    "    len(text_dataset[\"test\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Tokenization and Chunking\n",
    "# @markdown Apply the tokenization and chunking to each split.\n",
    "\n",
    "text_dataset = text_dataset.map(\n",
    "    tokenize_and_chunk, batched=True, fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids']:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0][0]),\n",
    "    \"\\n\\tvalue:\",\n",
    "    text_dataset[\"train\"][\"input_ids\"][0][0],\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(f\"Original sequence (text):\\n\\t{text_dataset['train']['text'][0]}\", end=\"\\n\\n\")\n",
    "print(\n",
    "    f\"Encoded sequence (tokens):\\n\\t {text_dataset['train']['input_ids'][0]}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Decoded sequence (tokens):\\n\\t {tokenizer.decode(text_dataset['train']['input_ids'][0])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define the Transformer model\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(1, max_len, d_model)  # batch_first=True\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ntoken: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        d_hid: int,\n",
    "        nlayers: int,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = torch.nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = torch.nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape [batch_size, seq_len]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [batch_size, seq_len, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            src_mask = torch.nn.Transformer.generate_square_subsequent_mask(\n",
    "                src.size(1)  # Use src.size(1) to get the seq_len\n",
    "            ).to(\n",
    "                src.device\n",
    "            )  # Use src.device to match device of src\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: torch.LongTensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Special generate method for the Transformer model.\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Since we trained the model to directly predict the next token we take the index as the argmin\n",
    "        over the distance between the output and the embedding table.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        # Loop through time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= BLOCK_SIZE else idx[:, -BLOCK_SIZE:]\n",
    "            # forward the model to get the output\n",
    "            outputs = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = outputs[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1).view(1, 1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an instance\n",
    "\n",
    "ntokens = tokenizer.vocab_size\n",
    "emsize = 302  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention (NOTE: nhead must be a divisor of d_hid)\n",
    "dropout = 0.1  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the Transformer model\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch\n",
    "\n",
    "    num_batches = len(text_dataset[\"train\"][\"input_ids\"])\n",
    "    for batch in range(num_batches):\n",
    "        tokens = text_dataset[\"train\"][\"input_ids\"][batch]\n",
    "        # parse into input and target\n",
    "        input = (\n",
    "            torch.LongTensor(tokens[:-1]).unsqueeze(0).to(DEVICE)\n",
    "        )  # ``[batch_size=1, seq_len]``\n",
    "        target = (\n",
    "            torch.LongTensor(tokens[1:]).unsqueeze(0).reshape(-1).to(DEVICE)\n",
    "        )  # ``[batch_size=1 * seq_len]``\n",
    "        # forward pass\n",
    "        output = model(input)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        output_flat = output.view(-1, ntokens)  # ``[batch_size=1 * seq_len, ntokens]``\n",
    "        # backpropagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output_flat, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(text_dataset[\"validation\"][\"input_ids\"])\n",
    "        for batch in range(num_batches):\n",
    "            tokens = text_dataset[\"validation\"][\"input_ids\"][batch]\n",
    "            input = torch.LongTensor(tokens[:-1]).unsqueeze(0).to(DEVICE)\n",
    "            target = torch.LongTensor(tokens[1:]).unsqueeze(0).reshape(-1).to(DEVICE)\n",
    "            seq_len = input.size(1)\n",
    "            output = model(input)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += criterion(output_flat, target).item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1\n",
    "\n",
    "final_model_params_path = os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loading and saving the new best model...\")\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\"),\n",
    "    )  # save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 132]) torch.Size([1, 232])\n",
      "\n",
      "[CLS] rance ta'en as shall with either part's agreement stand? baptista : not in my house, lucentio ; for, you know, pitchers have ears, and i have many servants : besides, old gremio is hearkening still ; and happily we might be interrupted. tranio : then at my lodging, an it like you : there doth my father lie ; and there, this night, we'll pass the business privately and well. send for your daughter by your servant here : my boy shall fetch the scrivener presently. the worst is this, that, at so slender warning, you [SEP]\n",
      "\n",
      "signalello warrington phrase jack emma sacked irrigationdington aviationgp publicationrom liga knowles commits prostitution corbin nigel crisp dharma trees culminating 95 multiculturalmax rotationchal advances sighting tornado valuation microscopic ã‚ solution 1200 sermons arriving view backpack banker infinite [unused420] smack programs 129nce tian smiley shops portrayed wrestlers fangsæ‘ restore jacques novgorod irene dt solos croprted porto une luigioni mueller smiles supernatural bath browser snap vilniusâ˜‰ increasing literally exposing 26mun organicuder [unused240] andreas irelandnainched cross surrounds48owski analytics 1907 japanese properly thessaloniki carved chose confidenturi horticultural\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Generate new text using test input\n",
    "\n",
    "max_new_tokens = 100\n",
    "idx = torch.LongTensor(text_dataset[\"test\"][\"input_ids\"][0]).unsqueeze(0).to(DEVICE)\n",
    "idx_gen = model.generate(idx, max_new_tokens, top_k=5)\n",
    "\n",
    "print(idx.shape, idx_gen.shape, end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx.tolist()[0]), end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx_gen.tolist()[0][-max_new_tokens:]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  torch.Size([1, 132, 302]) torch.float32 False cpu\n",
      "\n",
      "mask: torch.Size([1, 302]) torch.bool False cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create neural datasets\n",
    "# @markdown A synthetic dataset where the neural activity is the embeddings of tokens from the tiny Shakespeare.\n",
    "\n",
    "embedding = torch.nn.Embedding(ntokens, emsize, _freeze=True)\n",
    "train_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"train\"][\"input_ids\"]\n",
    "]\n",
    "validation_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"validation\"][\"input_ids\"]\n",
    "]\n",
    "test_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"test\"][\"input_ids\"]\n",
    "]\n",
    "\n",
    "# get a test sample for an example\n",
    "data = test_dataset[0].unsqueeze(0)\n",
    "print(\"data: \", data.shape, data.dtype, data.requires_grad, data.device, end=\"\\n\\n\")\n",
    "\n",
    "mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "print(\"mask:\", mask.shape, mask.dtype, mask.requires_grad, mask.device, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 131, 302]) torch.float32 False cuda:0\n",
      "\n",
      "target: torch.Size([1, 131, 302]) torch.float32 False cuda:0\n",
      "\n",
      "output: torch.Size([1, 131, 2048]) torch.float16 True cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create a NeuralTransformer model\n",
    "\n",
    "model = NeuralTransformer(input_size=emsize, hidden_size=d_hid).to(DEVICE)\n",
    "\n",
    "# test input-output functionality\n",
    "mask = mask.to(DEVICE)\n",
    "input = data[:, :-1, :].to(DEVICE)\n",
    "target = data[:, 1:, :].to(DEVICE)\n",
    "output = model(input, mask)\n",
    "\n",
    "print(\n",
    "    \"input:\",\n",
    "    input.shape,\n",
    "    input.dtype,\n",
    "    input.requires_grad,\n",
    "    input.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"target:\",\n",
    "    target.shape,\n",
    "    target.dtype,\n",
    "    target.requires_grad,\n",
    "    target.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"output:\",\n",
    "    output.shape,\n",
    "    output.dtype,\n",
    "    output.requires_grad,\n",
    "    output.device,\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the NeuralTransformer model\n",
    "\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch\n",
    "\n",
    "    num_batches = len(train_dataset)\n",
    "    for batch in range(num_batches):\n",
    "        data = train_dataset[batch].unsqueeze(0)\n",
    "        mask = mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "        # parse into input and target\n",
    "        mask = mask.to(DEVICE)\n",
    "        input = data[:, :-1, :].to(DEVICE)\n",
    "        target = data[:, 1:, :].to(DEVICE)\n",
    "        # forward pass\n",
    "        output = model(input, mask)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        # backpropopagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss_fn()(\n",
    "            output, target\n",
    "        )  # flatens output to ``[batch_size=1 * seq_len, ntokens]``\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(validation_dataset)\n",
    "        for batch in range(num_batches):\n",
    "            data = validation_dataset[batch].unsqueeze(0)\n",
    "            mask = mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "            mask = mask.to(DEVICE)\n",
    "            input = data[:, :-1, :].to(DEVICE)\n",
    "            target = data[:, 1:, :].to(DEVICE)\n",
    "            output = model(input, mask)\n",
    "            loss = model.loss_fn()(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a previously saved model checkpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   300/ 1963 batches | lr 4.09 | ms/batch  6.27 | loss  8.21 | ppl  3681.33\n",
      "| epoch   1 |   600/ 1963 batches | lr 4.09 | ms/batch  6.36 | loss  8.30 | ppl  4030.22\n",
      "| epoch   1 |   900/ 1963 batches | lr 4.09 | ms/batch  6.29 | loss  8.69 | ppl  5915.31\n",
      "| epoch   1 |  1200/ 1963 batches | lr 4.09 | ms/batch  7.02 | loss  8.46 | ppl  4732.65\n",
      "| epoch   1 |  1500/ 1963 batches | lr 4.09 | ms/batch  6.85 | loss  8.33 | ppl  4155.44\n",
      "| epoch   1 |  1800/ 1963 batches | lr 4.09 | ms/batch  5.97 | loss  8.23 | ppl  3750.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.92s | valid loss  8.88 | valid ppl  7193.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   300/ 1963 batches | lr 4.05 | ms/batch  8.23 | loss  7.97 | ppl  2880.04\n",
      "| epoch   2 |   600/ 1963 batches | lr 4.05 | ms/batch  5.96 | loss  8.11 | ppl  3336.95\n",
      "| epoch   2 |   900/ 1963 batches | lr 4.05 | ms/batch  6.08 | loss  8.52 | ppl  5000.69\n",
      "| epoch   2 |  1200/ 1963 batches | lr 4.05 | ms/batch  6.01 | loss  8.30 | ppl  4042.73\n",
      "| epoch   2 |  1500/ 1963 batches | lr 4.05 | ms/batch  7.28 | loss  8.18 | ppl  3565.28\n",
      "| epoch   2 |  1800/ 1963 batches | lr 4.05 | ms/batch  7.13 | loss  8.07 | ppl  3187.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 13.57s | valid loss  9.30 | valid ppl 10892.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   300/ 1963 batches | lr 4.01 | ms/batch  6.23 | loss  7.79 | ppl  2424.13\n",
      "| epoch   3 |   600/ 1963 batches | lr 4.01 | ms/batch  6.02 | loss  7.94 | ppl  2800.13\n",
      "| epoch   3 |   900/ 1963 batches | lr 4.01 | ms/batch  9.00 | loss  8.31 | ppl  4073.96\n",
      "| epoch   3 |  1200/ 1963 batches | lr 4.01 | ms/batch 12.39 | loss  8.64 | ppl  5637.75\n",
      "| epoch   3 |  1500/ 1963 batches | lr 4.01 | ms/batch  9.11 | loss  7.99 | ppl  2964.74\n",
      "| epoch   3 |  1800/ 1963 batches | lr 4.01 | ms/batch 10.70 | loss  7.90 | ppl  2708.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 17.51s | valid loss  8.65 | valid ppl  5708.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   300/ 1963 batches | lr 3.97 | ms/batch  6.13 | loss  7.66 | ppl  2126.49\n",
      "| epoch   4 |   600/ 1963 batches | lr 3.97 | ms/batch  6.08 | loss  7.77 | ppl  2359.39\n",
      "| epoch   4 |   900/ 1963 batches | lr 3.97 | ms/batch  6.05 | loss  8.16 | ppl  3509.04\n",
      "| epoch   4 |  1200/ 1963 batches | lr 3.97 | ms/batch  6.17 | loss  7.98 | ppl  2918.36\n",
      "| epoch   4 |  1500/ 1963 batches | lr 3.97 | ms/batch  6.11 | loss  7.87 | ppl  2608.11\n",
      "| epoch   4 |  1800/ 1963 batches | lr 3.97 | ms/batch  6.11 | loss  7.75 | ppl  2325.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 12.30s | valid loss  8.99 | valid ppl  7994.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   300/ 1963 batches | lr 3.93 | ms/batch  6.08 | loss  7.56 | ppl  1912.26\n",
      "| epoch   5 |   600/ 1963 batches | lr 3.93 | ms/batch  6.22 | loss  7.68 | ppl  2160.65\n",
      "| epoch   5 |   900/ 1963 batches | lr 3.93 | ms/batch  6.04 | loss  8.06 | ppl  3160.51\n",
      "| epoch   5 |  1200/ 1963 batches | lr 3.93 | ms/batch  6.11 | loss  7.80 | ppl  2435.49\n",
      "| epoch   5 |  1500/ 1963 batches | lr 3.93 | ms/batch  6.20 | loss  7.69 | ppl  2195.93\n",
      "| epoch   5 |  1800/ 1963 batches | lr 3.93 | ms/batch  6.38 | loss  7.59 | ppl  1986.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 12.40s | valid loss  8.52 | valid ppl  4993.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   300/ 1963 batches | lr 3.89 | ms/batch  6.03 | loss  7.43 | ppl  1682.61\n",
      "| epoch   6 |   600/ 1963 batches | lr 3.89 | ms/batch  5.91 | loss  7.54 | ppl  1875.05\n",
      "| epoch   6 |   900/ 1963 batches | lr 3.89 | ms/batch  6.02 | loss  7.88 | ppl  2649.39\n",
      "| epoch   6 |  1200/ 1963 batches | lr 3.89 | ms/batch  5.95 | loss  7.68 | ppl  2164.11\n",
      "| epoch   6 |  1500/ 1963 batches | lr 3.89 | ms/batch  5.98 | loss  7.66 | ppl  2129.20\n",
      "| epoch   6 |  1800/ 1963 batches | lr 3.89 | ms/batch  5.90 | loss  7.48 | ppl  1778.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 12.00s | valid loss  8.69 | valid ppl  5948.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   300/ 1963 batches | lr 3.85 | ms/batch  6.73 | loss  7.27 | ppl  1432.74\n",
      "| epoch   7 |   600/ 1963 batches | lr 3.85 | ms/batch  5.73 | loss  7.44 | ppl  1697.26\n",
      "| epoch   7 |   900/ 1963 batches | lr 3.85 | ms/batch  5.75 | loss  7.79 | ppl  2422.37\n",
      "| epoch   7 |  1200/ 1963 batches | lr 3.85 | ms/batch  5.81 | loss  7.56 | ppl  1921.90\n",
      "| epoch   7 |  1500/ 1963 batches | lr 3.85 | ms/batch  5.76 | loss  7.48 | ppl  1771.27\n",
      "| epoch   7 |  1800/ 1963 batches | lr 3.85 | ms/batch  5.93 | loss  7.37 | ppl  1593.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 11.97s | valid loss  8.12 | valid ppl  3358.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   300/ 1963 batches | lr 3.81 | ms/batch  6.03 | loss  7.13 | ppl  1254.74\n",
      "| epoch   8 |   600/ 1963 batches | lr 3.81 | ms/batch  5.96 | loss  7.25 | ppl  1404.97\n",
      "| epoch   8 |   900/ 1963 batches | lr 3.81 | ms/batch  5.85 | loss  7.69 | ppl  2188.88\n",
      "| epoch   8 |  1200/ 1963 batches | lr 3.81 | ms/batch  5.90 | loss  7.44 | ppl  1704.39\n",
      "| epoch   8 |  1500/ 1963 batches | lr 3.81 | ms/batch  5.93 | loss  7.34 | ppl  1541.27\n",
      "| epoch   8 |  1800/ 1963 batches | lr 3.81 | ms/batch  5.95 | loss  7.27 | ppl  1434.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 11.95s | valid loss  7.81 | valid ppl  2456.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   300/ 1963 batches | lr 3.77 | ms/batch  6.05 | loss  7.05 | ppl  1148.77\n",
      "| epoch   9 |   600/ 1963 batches | lr 3.77 | ms/batch  5.92 | loss  7.24 | ppl  1392.03\n",
      "| epoch   9 |   900/ 1963 batches | lr 3.77 | ms/batch  5.77 | loss  7.54 | ppl  1876.35\n",
      "| epoch   9 |  1200/ 1963 batches | lr 3.77 | ms/batch  5.99 | loss  7.37 | ppl  1586.52\n",
      "| epoch   9 |  1500/ 1963 batches | lr 3.77 | ms/batch  5.94 | loss  7.30 | ppl  1476.30\n",
      "| epoch   9 |  1800/ 1963 batches | lr 3.77 | ms/batch  5.87 | loss  7.20 | ppl  1335.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 11.96s | valid loss  8.24 | valid ppl  3777.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   300/ 1963 batches | lr 3.74 | ms/batch  5.96 | loss  6.99 | ppl  1088.69\n",
      "| epoch  10 |   600/ 1963 batches | lr 3.74 | ms/batch  5.73 | loss  7.15 | ppl  1276.36\n",
      "| epoch  10 |   900/ 1963 batches | lr 3.74 | ms/batch  5.91 | loss  7.47 | ppl  1751.94\n",
      "| epoch  10 |  1200/ 1963 batches | lr 3.74 | ms/batch  5.89 | loss  7.36 | ppl  1568.87\n",
      "| epoch  10 |  1500/ 1963 batches | lr 3.74 | ms/batch  5.87 | loss  7.21 | ppl  1346.44\n",
      "| epoch  10 |  1800/ 1963 batches | lr 3.74 | ms/batch  5.75 | loss  7.08 | ppl  1188.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 11.83s | valid loss  8.24 | valid ppl  3774.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   300/ 1963 batches | lr 3.70 | ms/batch  5.96 | loss  6.92 | ppl  1008.61\n",
      "| epoch  11 |   600/ 1963 batches | lr 3.70 | ms/batch  5.70 | loss  7.04 | ppl  1140.67\n",
      "| epoch  11 |   900/ 1963 batches | lr 3.70 | ms/batch  6.40 | loss  7.39 | ppl  1613.31\n",
      "| epoch  11 |  1200/ 1963 batches | lr 3.70 | ms/batch  5.97 | loss  7.24 | ppl  1396.16\n",
      "| epoch  11 |  1500/ 1963 batches | lr 3.70 | ms/batch  5.80 | loss  7.09 | ppl  1203.55\n",
      "| epoch  11 |  1800/ 1963 batches | lr 3.70 | ms/batch  6.18 | loss  7.03 | ppl  1126.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 12.39s | valid loss  8.32 | valid ppl  4123.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   300/ 1963 batches | lr 3.66 | ms/batch  6.12 | loss  6.88 | ppl   968.12\n",
      "| epoch  12 |   600/ 1963 batches | lr 3.66 | ms/batch  5.68 | loss  6.98 | ppl  1077.97\n",
      "| epoch  12 |   900/ 1963 batches | lr 3.66 | ms/batch  5.86 | loss  7.30 | ppl  1480.09\n",
      "| epoch  12 |  1200/ 1963 batches | lr 3.66 | ms/batch  8.30 | loss  7.02 | ppl  1113.74\n",
      "| epoch  12 |  1500/ 1963 batches | lr 3.66 | ms/batch 12.87 | loss  7.03 | ppl  1131.75\n",
      "| epoch  12 |  1800/ 1963 batches | lr 3.66 | ms/batch 10.84 | loss  6.91 | ppl  1005.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 16.63s | valid loss  7.50 | valid ppl  1815.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   300/ 1963 batches | lr 3.62 | ms/batch 10.08 | loss  6.80 | ppl   897.24\n",
      "| epoch  13 |   600/ 1963 batches | lr 3.62 | ms/batch  5.75 | loss  6.89 | ppl   981.14\n",
      "| epoch  13 |   900/ 1963 batches | lr 3.62 | ms/batch  5.74 | loss  7.19 | ppl  1330.86\n",
      "| epoch  13 |  1200/ 1963 batches | lr 3.62 | ms/batch  5.85 | loss  7.05 | ppl  1152.15\n",
      "| epoch  13 |  1500/ 1963 batches | lr 3.62 | ms/batch  5.88 | loss  7.01 | ppl  1107.32\n",
      "| epoch  13 |  1800/ 1963 batches | lr 3.62 | ms/batch  5.76 | loss  6.85 | ppl   945.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 12.99s | valid loss  8.22 | valid ppl  3731.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   300/ 1963 batches | lr 3.59 | ms/batch  5.91 | loss  6.72 | ppl   828.43\n",
      "| epoch  14 |   600/ 1963 batches | lr 3.59 | ms/batch  5.71 | loss  6.83 | ppl   925.88\n",
      "| epoch  14 |   900/ 1963 batches | lr 3.59 | ms/batch  5.89 | loss  7.16 | ppl  1293.31\n",
      "| epoch  14 |  1200/ 1963 batches | lr 3.59 | ms/batch  5.94 | loss  6.99 | ppl  1085.10\n",
      "| epoch  14 |  1500/ 1963 batches | lr 3.59 | ms/batch  5.77 | loss  6.89 | ppl   979.41\n",
      "| epoch  14 |  1800/ 1963 batches | lr 3.59 | ms/batch  5.82 | loss  6.80 | ppl   896.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 11.78s | valid loss  7.74 | valid ppl  2303.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   300/ 1963 batches | lr 3.55 | ms/batch  5.85 | loss  6.72 | ppl   828.38\n",
      "| epoch  15 |   600/ 1963 batches | lr 3.55 | ms/batch  5.89 | loss  6.76 | ppl   865.61\n",
      "| epoch  15 |   900/ 1963 batches | lr 3.55 | ms/batch  5.86 | loss  7.11 | ppl  1226.17\n",
      "| epoch  15 |  1200/ 1963 batches | lr 3.55 | ms/batch  5.87 | loss  6.99 | ppl  1081.31\n",
      "| epoch  15 |  1500/ 1963 batches | lr 3.55 | ms/batch  5.83 | loss  6.83 | ppl   921.93\n",
      "| epoch  15 |  1800/ 1963 batches | lr 3.55 | ms/batch  5.96 | loss  6.72 | ppl   825.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 11.86s | valid loss  8.92 | valid ppl  7499.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   300/ 1963 batches | lr 3.52 | ms/batch  5.98 | loss  6.64 | ppl   767.75\n",
      "| epoch  16 |   600/ 1963 batches | lr 3.52 | ms/batch  5.84 | loss  6.69 | ppl   800.48\n",
      "| epoch  16 |   900/ 1963 batches | lr 3.52 | ms/batch  5.82 | loss  6.98 | ppl  1070.42\n",
      "| epoch  16 |  1200/ 1963 batches | lr 3.52 | ms/batch  5.80 | loss  6.88 | ppl   975.80\n",
      "| epoch  16 |  1500/ 1963 batches | lr 3.52 | ms/batch  5.84 | loss  6.76 | ppl   862.47\n",
      "| epoch  16 |  1800/ 1963 batches | lr 3.52 | ms/batch  5.88 | loss  6.71 | ppl   821.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 11.80s | valid loss  7.11 | valid ppl  1225.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   300/ 1963 batches | lr 3.48 | ms/batch  5.92 | loss  6.56 | ppl   706.31\n",
      "| epoch  17 |   600/ 1963 batches | lr 3.48 | ms/batch  5.79 | loss  6.67 | ppl   785.33\n",
      "| epoch  17 |   900/ 1963 batches | lr 3.48 | ms/batch  5.73 | loss  6.91 | ppl  1003.81\n",
      "| epoch  17 |  1200/ 1963 batches | lr 3.48 | ms/batch  5.91 | loss  6.77 | ppl   872.65\n",
      "| epoch  17 |  1500/ 1963 batches | lr 3.48 | ms/batch  5.82 | loss  6.75 | ppl   851.02\n",
      "| epoch  17 |  1800/ 1963 batches | lr 3.48 | ms/batch  5.87 | loss  6.63 | ppl   754.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 11.77s | valid loss  7.27 | valid ppl  1435.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   300/ 1963 batches | lr 3.45 | ms/batch  5.92 | loss  6.55 | ppl   699.06\n",
      "| epoch  18 |   600/ 1963 batches | lr 3.45 | ms/batch  5.91 | loss  6.56 | ppl   708.39\n",
      "| epoch  18 |   900/ 1963 batches | lr 3.45 | ms/batch  5.76 | loss  6.88 | ppl   971.94\n",
      "| epoch  18 |  1200/ 1963 batches | lr 3.45 | ms/batch  5.88 | loss  6.67 | ppl   785.03\n",
      "| epoch  18 |  1500/ 1963 batches | lr 3.45 | ms/batch  5.84 | loss  6.68 | ppl   795.13\n",
      "| epoch  18 |  1800/ 1963 batches | lr 3.45 | ms/batch  5.76 | loss  6.60 | ppl   738.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 11.81s | valid loss  7.81 | valid ppl  2456.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   300/ 1963 batches | lr 3.41 | ms/batch  5.90 | loss  6.44 | ppl   624.26\n",
      "| epoch  19 |   600/ 1963 batches | lr 3.41 | ms/batch  5.70 | loss  6.54 | ppl   690.67\n",
      "| epoch  19 |   900/ 1963 batches | lr 3.41 | ms/batch  5.83 | loss  6.76 | ppl   862.57\n",
      "| epoch  19 |  1200/ 1963 batches | lr 3.41 | ms/batch  5.89 | loss  6.61 | ppl   744.53\n",
      "| epoch  19 |  1500/ 1963 batches | lr 3.41 | ms/batch  6.16 | loss  6.55 | ppl   702.71\n",
      "| epoch  19 |  1800/ 1963 batches | lr 3.41 | ms/batch  5.89 | loss  6.48 | ppl   651.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 11.92s | valid loss  7.98 | valid ppl  2918.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   300/ 1963 batches | lr 3.38 | ms/batch  5.85 | loss  6.39 | ppl   598.31\n",
      "| epoch  20 |   600/ 1963 batches | lr 3.38 | ms/batch  5.86 | loss  6.48 | ppl   649.80\n",
      "| epoch  20 |   900/ 1963 batches | lr 3.38 | ms/batch  5.86 | loss  6.77 | ppl   869.16\n",
      "| epoch  20 |  1200/ 1963 batches | lr 3.38 | ms/batch  5.97 | loss  6.62 | ppl   750.07\n",
      "| epoch  20 |  1500/ 1963 batches | lr 3.38 | ms/batch  5.76 | loss  6.50 | ppl   662.45\n",
      "| epoch  20 |  1800/ 1963 batches | lr 3.38 | ms/batch  5.93 | loss  6.41 | ppl   607.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 11.83s | valid loss  7.98 | valid ppl  2929.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "Loading and saving the new best model...\n"
     ]
    }
   ],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 20\n",
    "\n",
    "final_model_params_path = os.path.join(\"../models/\", \"neural_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loading and saving the new best model...\")\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        os.path.join(\"../models/\", \"neural_transformer_model.pt\"),\n",
    "    )  # save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 132, 302]) torch.Size([1, 232, 302])\n",
      "\n",
      "torch.Size([1, 132, 30522])\n",
      "\n",
      "[101, 2743, 3401, 11937, 1005, 4372, 2004, 4618, 2007, 2593, 2112, 1005, 1055, 3820, 3233, 1029, 7550, 2050, 1024, 2025, 1999, 2026, 2160, 1010, 19913, 16778, 2080, 1025, 2005, 1010, 2017, 2113, 1010, 23232, 2031, 5551, 1010, 1998, 1045, 2031, 2116, 8858, 1024, 4661, 1010, 2214, 24665, 23238, 2080, 2003, 2963, 7520, 2075, 2145, 1025, 1998, 11361, 2057, 2453, 2022, 7153, 1012, 25283, 3695, 1024, 2059, 2012, 2026, 26859, 1010, 2019, 2009, 2066, 2017, 1024, 2045, 11089, 2232, 2026, 2269, 4682, 1025, 1998, 2045, 1010, 2023, 2305, 1010, 2057, 1005, 2222, 3413, 1996, 2449, 9139, 1998, 2092, 1012, 4604, 2005, 2115, 2684, 2011, 2115, 7947, 2182, 1024, 2026, 2879, 4618, 18584, 1996, 8040, 3089, 8159, 2121, 12825, 1012, 1996, 5409, 2003, 2023, 1010, 2008, 1010, 2012, 2061, 10944, 5432, 1010, 2017, 102]\n",
      "\n",
      "[CLS] rance ta'en as shall with either part's agreement stand? baptista : not in my house, lucentio ; for, you know, pitchers have ears, and i have many servants : besides, old gremio is hearkening still ; and happily we might be interrupted. tranio : then at my lodging, an it like you : there doth my father lie ; and there, this night, we'll pass the business privately and well. send for your daughter by your servant here : my boy shall fetch the scrivener presently. the worst is this, that, at so slender warning, you [SEP]\n",
      "\n",
      "[101, 2743, 3401, 11937, 1005, 4372, 2004, 4618, 2007, 2593, 2112, 1005, 1055, 3820, 3233, 1029, 7550, 2050, 1024, 2025, 1999, 2026, 2160, 1010, 19913, 16778, 2080, 1025, 2005, 1010, 2017, 2113, 1010, 23232, 2031, 5551, 1010, 1998, 1045, 2031, 2116, 8858, 1024, 4661, 1010, 2214, 24665, 23238, 2080, 2003, 2963, 7520, 2075, 2145, 1025, 1998, 11361, 2057, 2453, 2022, 7153, 1012, 25283, 3695, 1024, 2059, 2012, 2026, 26859, 1010, 2019, 2009, 2066, 2017, 1024, 2045, 11089, 2232, 2026, 2269, 4682, 1025, 1998, 2045, 1010, 2023, 2305, 1010, 2057, 1005, 2222, 3413, 1996, 2449, 9139, 1998, 2092, 1012, 4604, 2005, 2115, 2684, 2011, 2115, 7947, 2182, 1024, 2026, 2879, 4618, 18584, 1996, 8040, 3089, 8159, 2121, 12825, 1012, 1996, 5409, 2003, 2023, 1010, 2008, 1010, 2012, 2061, 10944, 5432, 1010, 2017, 102]\n",
      "\n",
      "torch.Size([1, 100, 30522])\n",
      "\n",
      "[1155, 7195, 15077, 9828, 8768, 7276, 7044, 3798, 5901, 875, 8032, 3339, 7276, 6870, 12800, 6559, 21478, 17677, 9237, 19893, 8768, 11259, 21770, 17677, 9607, 7044, 14537, 29390, 29077, 2132, 18531, 19310, 737, 12800, 4512, 10603, 5901, 25489, 15077, 19310, 23327, 7044, 17037, 4599, 23327, 8768, 29077, 9952, 13466, 10603, 5479, 10762, 23327, 1096, 13545, 20270, 5417, 26872, 26603, 10603, 5479, 663, 12800, 21734, 10603, 5479, 10762, 7899, 26250, 6559, 19310, 23327, 3864, 23327, 3864, 23327, 9916, 5479, 10762, 21734, 10519, 9237, 13891, 10603, 5479, 10762, 17037, 10603, 5479, 10762, 17037, 21734, 15446, 5901, 1096, 5479, 10762, 17676, 1096, 5479]\n",
      "\n",
      "Î± revenge consuming lied blanket select liberty levels rapidly [unused870] vocalist 1975 select wildlife circaok orphans wander quarterfinals spruce blanket subtle het wander veins liberty jeannesystems 1651 head usafÃŸ [unused732] circa conversationcks rapidly westchester consumingÃŸ shards liberty fest fans shards blanket 1651 moses higginscks runnerigan shards ÃŸ fond landowner fifteen fragmented goghcks runner [unused658] circa steamedcks runnerigan instructionoireokÃŸ shards elections shards elections shards murders runnerigan steamed airborne quarterfinals lumbercks runnerigan festcks runnerigan fest steamed nightmares rapidly ÃŸ runnerigan deprived ÃŸ runner\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Generate new data using test input\n",
    "\n",
    "data = test_dataset[0].unsqueeze(0).to(DEVICE)\n",
    "mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0).to(DEVICE)\n",
    "embedding = embedding.to(DEVICE)\n",
    "\n",
    "max_new_tokens = 100\n",
    "data_gen = model.transformer_generate(data, mask, max_new_tokens)\n",
    "\n",
    "print(data.shape, data_gen.shape, end=\"\\n\\n\")\n",
    "\n",
    "# @markdown We want to find the nearest token to each generated embedding.\n",
    "# We can do this by finding the nearest embedding to each generated embedding\n",
    "# and then finding the index corresponding to that embedding.\n",
    "\n",
    "# First run a test on data we know what the true token output should be\n",
    "with torch.no_grad():\n",
    "    sequence_expanded = data.unsqueeze(2)\n",
    "    matrix_expanded = embedding.weight.unsqueeze(0).unsqueeze(0)\n",
    "    dist = torch.linalg.vector_norm(sequence_expanded - matrix_expanded, dim=3)\n",
    "    print(dist.shape, end=\"\\n\\n\")\n",
    "    tokens = dist.argmin(dim=2)\n",
    "    print(tokens.squeeze().tolist(), end=\"\\n\\n\")\n",
    "    # decode into text\n",
    "    print(tokenizer.decode(tokens.squeeze().tolist()), end=\"\\n\\n\")\n",
    "\n",
    "# If correct this should match\n",
    "print(text_dataset[\"test\"][\"input_ids\"][0], end=\"\\n\\n\")\n",
    "\n",
    "# Now do the same thing on the newly generated data\n",
    "with torch.no_grad():\n",
    "    sequence_expanded = data_gen[:, -max_new_tokens:, :].unsqueeze(2)\n",
    "    matrix_expanded = embedding.weight.unsqueeze(0).unsqueeze(0)\n",
    "    dist = torch.linalg.vector_norm(sequence_expanded - matrix_expanded, dim=3)\n",
    "    print(dist.shape, end=\"\\n\\n\")\n",
    "    tokens = dist.argmin(dim=2)\n",
    "    print(tokens.squeeze().tolist(), end=\"\\n\\n\")\n",
    "    # decode into text\n",
    "    print(tokenizer.decode(tokens.squeeze().tolist()), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
