{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device found.\n"
     ]
    }
   ],
   "source": [
    "# @title Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import nbimporter\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tempfile import TemporaryDirectory\n",
    "from utils import DEVICE, BLOCK_SIZE\n",
    "from models._utils import NeuralTransformer\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "from CreateSyntheticDataset import tokenize_and_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling with tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `tokenizer.encode(text)`:\n",
      "\ttext: Welcome to the ðŸ¤— Tokenizers library.\n",
      "\ttokenized: [101, 6160, 2000, 1996, 100, 19204, 17629, 2015, 3075, 1012, 102]\n",
      "\tdecoded: [CLS] welcome to the [UNK] tokenizers library. [SEP]\n",
      "\n",
      "Calling `tokenizer(text)`:\n",
      "\tobject.keys(): dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\ttext: We are very happy to show you the ðŸ¤— Transformers library.\n",
      "\ttokenized: [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102]\n",
      "\tdecoded: [CLS] we are very happy to show you the [UNK] transformers library. [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Tokenizers\n",
    "# @markdown Note there are two ways to call the tokenizer's encoder.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "expl_text = \"Welcome to the ðŸ¤— Tokenizers library.\"\n",
    "impl_text = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
    "expl_encode = tokenizer.encode(expl_text)\n",
    "impl_encode = tokenizer(impl_text)\n",
    "print(\n",
    "    f\"Calling `tokenizer.encode(text)`:\\n\\ttext: {expl_text}\\n\\ttokenized: {expl_encode}\\n\\tdecoded: {tokenizer.decode(expl_encode)}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Calling `tokenizer(text)`:\\n\\tobject.keys(): {impl_encode.keys()}\\n\\ttext: {impl_text}\\n\\ttokenized: {impl_encode['input_ids']}\\n\\tdecoded: {tokenizer.decode(impl_encode['input_ids'])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "train: <class 'list'> 1 <class 'str'> 1003854\n",
      "\n",
      "validation: <class 'list'> 1 <class 'str'> 55770\n",
      "\n",
      "test: <class 'list'> 1 <class 'str'> 55770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Datasets\n",
    "\n",
    "text_dataset = load_hf_dataset(\"tiny_shakespeare\")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"train:\",\n",
    "    type(text_dataset[\"train\"][\"text\"]),\n",
    "    len(text_dataset[\"train\"][\"text\"]),\n",
    "    type(text_dataset[\"train\"][\"text\"][0]),\n",
    "    len(text_dataset[\"train\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"validation:\",\n",
    "    type(text_dataset[\"validation\"][\"text\"]),\n",
    "    len(text_dataset[\"validation\"][\"text\"]),\n",
    "    type(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    len(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"test:\",\n",
    "    type(text_dataset[\"test\"][\"text\"]),\n",
    "    len(text_dataset[\"test\"][\"text\"]),\n",
    "    type(text_dataset[\"test\"][\"text\"][0]),\n",
    "    len(text_dataset[\"test\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1963\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "})\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "text_dataset['train']['input_ids']:\n",
      " \ttype: <class 'list'> \n",
      "\tlength: 1963\n",
      "\n",
      "text_dataset['train']['input_ids'][0]:\n",
      " \ttype: <class 'list'> \n",
      "\tlength: 135\n",
      "\n",
      "text_dataset['train']['input_ids'][0][0]:\n",
      " \ttype: <class 'int'> \n",
      "\tvalue: 101\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Original sequence (text):\n",
      "\tFirst Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? All: Resolved. resolved. First Citizen: First, you know Caius Marcius is chief enemy to the people. All: We know't, we know't. First Citizen: Let us kill him, and we'll have corn at our own price. Is't a verdict? All: No more talking on't; let it be done: away, away! Second Citizen: One word, good citizens. First Citizen: We are accounted poor citizens, the\n",
      "\n",
      "Encoded sequence (tokens):\n",
      "\t [101, 2034, 6926, 1024, 2077, 2057, 10838, 2151, 2582, 1010, 2963, 2033, 3713, 1012, 2035, 1024, 3713, 1010, 3713, 1012, 2034, 6926, 1024, 2017, 2024, 2035, 10395, 2738, 2000, 3280, 2084, 2000, 6904, 15630, 2232, 1029, 2035, 1024, 10395, 1012, 10395, 1012, 2034, 6926, 1024, 2034, 1010, 2017, 2113, 29080, 2271, 7871, 4173, 2003, 2708, 4099, 2000, 1996, 2111, 1012, 2035, 1024, 2057, 2113, 1005, 1056, 1010, 2057, 2113, 1005, 1056, 1012, 2034, 6926, 1024, 2292, 2149, 3102, 2032, 1010, 1998, 2057, 1005, 2222, 2031, 9781, 2012, 2256, 2219, 3976, 1012, 2003, 1005, 1056, 1037, 14392, 1029, 2035, 1024, 2053, 2062, 3331, 2006, 1005, 1056, 1025, 2292, 2009, 2022, 2589, 1024, 2185, 1010, 2185, 999, 2117, 6926, 1024, 2028, 2773, 1010, 2204, 4480, 1012, 2034, 6926, 1024, 2057, 2024, 14729, 3532, 4480, 1010, 1996, 102]\n",
      "\n",
      "Decoded sequence (tokens):\n",
      "\t [CLS] first citizen : before we proceed any further, hear me speak. all : speak, speak. first citizen : you are all resolved rather to die than to famish? all : resolved. resolved. first citizen : first, you know caius marcius is chief enemy to the people. all : we know't, we know't. first citizen : let us kill him, and we'll have corn at our own price. is't a verdict? all : no more talking on't ; let it be done : away, away! second citizen : one word, good citizens. first citizen : we are accounted poor citizens, the [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Tokenization and Chunking\n",
    "# @markdown Apply the tokenization and chunking to each split.\n",
    "\n",
    "text_dataset = text_dataset.map(\n",
    "    tokenize_and_chunk, batched=True, fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids']:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0][0]),\n",
    "    \"\\n\\tvalue:\",\n",
    "    text_dataset[\"train\"][\"input_ids\"][0][0],\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(f\"Original sequence (text):\\n\\t{text_dataset['train']['text'][0]}\", end=\"\\n\\n\")\n",
    "print(\n",
    "    f\"Encoded sequence (tokens):\\n\\t {text_dataset['train']['input_ids'][0]}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Decoded sequence (tokens):\\n\\t {tokenizer.decode(text_dataset['train']['input_ids'][0])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define the Transformer model\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(1, max_len, d_model)  # batch_first=True\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ntoken: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        d_hid: int,\n",
    "        nlayers: int,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = torch.nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = torch.nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape [batch_size, seq_len]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [batch_size, seq_len, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            src_mask = torch.nn.Transformer.generate_square_subsequent_mask(\n",
    "                src.size(1)  # Use src.size(1) to get the seq_len\n",
    "            ).to(\n",
    "                src.device\n",
    "            )  # Use src.device to match device of src\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: torch.LongTensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Special generate method for the Transformer model.\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Since we trained the model to directly predict the next token we take the index as the argmin\n",
    "        over the distance between the output and the embedding table.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        # Loop through time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= BLOCK_SIZE else idx[:, -BLOCK_SIZE:]\n",
    "            # forward the model to get the output\n",
    "            outputs = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = outputs[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1).view(1, 1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an instance\n",
    "\n",
    "ntokens = tokenizer.vocab_size\n",
    "emsize = 302  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention (NOTE: nhead must be a divisor of d_hid)\n",
    "dropout = 0.1  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the Transformer model\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch\n",
    "\n",
    "    num_batches = len(text_dataset[\"train\"][\"input_ids\"])\n",
    "    for batch in range(num_batches):\n",
    "        tokens = text_dataset[\"train\"][\"input_ids\"][batch]\n",
    "        # parse into input and target\n",
    "        input = (\n",
    "            torch.LongTensor(tokens[:-1]).unsqueeze(0).to(DEVICE)\n",
    "        )  # ``[batch_size=1, seq_len]``\n",
    "        target = (\n",
    "            torch.LongTensor(tokens[1:]).unsqueeze(0).reshape(-1).to(DEVICE)\n",
    "        )  # ``[batch_size=1 * seq_len]``\n",
    "        # forward pass\n",
    "        output = model(input)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        output_flat = output.view(-1, ntokens)  # ``[batch_size=1 * seq_len, ntokens]``\n",
    "        # backpropagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output_flat, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(text_dataset[\"validation\"][\"input_ids\"])\n",
    "        for batch in range(num_batches):\n",
    "            tokens = text_dataset[\"validation\"][\"input_ids\"][batch]\n",
    "            input = torch.LongTensor(tokens[:-1]).unsqueeze(0).to(DEVICE)\n",
    "            target = torch.LongTensor(tokens[1:]).unsqueeze(0).reshape(-1).to(DEVICE)\n",
    "            seq_len = input.size(1)\n",
    "            output = model(input)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += criterion(output_flat, target).item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1\n",
    "\n",
    "final_model_params_path = os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loading and saving the new best model...\")\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\"),\n",
    "    )  # save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate new text using test input\n",
    "\n",
    "max_new_tokens = 100\n",
    "idx = torch.LongTensor(text_dataset[\"test\"][\"input_ids\"][0]).unsqueeze(0).to(DEVICE)\n",
    "idx_gen = model.generate(idx, max_new_tokens, top_k=5)\n",
    "\n",
    "print(idx.shape, idx_gen.shape, end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx.tolist()[0]), end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx_gen.tolist()[0][-max_new_tokens:]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  torch.Size([1, 132, 302]) torch.float32 False cpu\n",
      "\n",
      "mask: torch.Size([1, 302]) torch.bool False cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create neural datasets\n",
    "# @markdown A synthetic dataset where the neural activity is the embeddings of tokens from the tiny Shakespeare.\n",
    "\n",
    "ntokens = tokenizer.vocab_size\n",
    "emsize = 302\n",
    "d_hid = 512\n",
    "embedding = torch.nn.Embedding(ntokens, emsize, _freeze=True)\n",
    "\n",
    "train_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"train\"][\"input_ids\"]\n",
    "]\n",
    "validation_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"validation\"][\"input_ids\"]\n",
    "]\n",
    "test_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"test\"][\"input_ids\"]\n",
    "]\n",
    "\n",
    "# get a test sample for an example\n",
    "data = test_dataset[0].unsqueeze(0)\n",
    "print(\"data: \", data.shape, data.dtype, data.requires_grad, data.device, end=\"\\n\\n\")\n",
    "\n",
    "mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "print(\"mask:\", mask.shape, mask.dtype, mask.requires_grad, mask.device, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 131, 302]) torch.float32 False cuda:0\n",
      "\n",
      "target: torch.Size([1, 131, 302]) torch.float32 False cuda:0\n",
      "\n",
      "output: torch.Size([1, 131, 16384]) torch.float16 True cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create a NeuralTransformer model\n",
    "\n",
    "model = NeuralTransformer(input_size=emsize, hidden_size=d_hid).to(DEVICE)\n",
    "\n",
    "# test input-output functionality\n",
    "mask = mask.to(DEVICE)\n",
    "input = data[:, :-1, :].to(DEVICE)\n",
    "target = data[:, 1:, :].to(DEVICE)\n",
    "output = model(input, mask)\n",
    "\n",
    "print(\n",
    "    \"input:\",\n",
    "    input.shape,\n",
    "    input.dtype,\n",
    "    input.requires_grad,\n",
    "    input.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"target:\",\n",
    "    target.shape,\n",
    "    target.dtype,\n",
    "    target.requires_grad,\n",
    "    target.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"output:\",\n",
    "    output.shape,\n",
    "    output.dtype,\n",
    "    output.requires_grad,\n",
    "    output.device,\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the NeuralTransformer model\n",
    "\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch\n",
    "\n",
    "    num_batches = len(train_dataset)\n",
    "    for batch in range(num_batches):\n",
    "        data = train_dataset[batch].unsqueeze(0)\n",
    "        mask = mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "        # parse into input and target\n",
    "        mask = mask.to(DEVICE)\n",
    "        input = data[:, :-1, :].to(DEVICE)\n",
    "        target = data[:, 1:, :].to(DEVICE)\n",
    "        # forward pass\n",
    "        output = model(input, mask)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        # backpropopagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss_fn()(\n",
    "            output, target, mask\n",
    "        )  # flatens output to ``[batch_size=1 * seq_len, ntokens]``\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(validation_dataset)\n",
    "        for batch in range(num_batches):\n",
    "            data = validation_dataset[batch].unsqueeze(0)\n",
    "            mask = mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "            mask = mask.to(DEVICE)\n",
    "            input = data[:, :-1, :].to(DEVICE)\n",
    "            target = data[:, 1:, :].to(DEVICE)\n",
    "            output = model(input, mask)\n",
    "            loss = model.loss_fn()(output, target, mask)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "random_projection \t torch.Size([16384, 302])\n",
      "token_neural_map \t torch.Size([16384, 302])\n",
      "input_hidden.0.weight \t torch.Size([16384, 512])\n",
      "input_hidden.1.pe \t torch.Size([1, 5000, 512])\n",
      "hidden_hidden.transformer.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "hidden_hidden.transformer.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "hidden_hidden.transformer.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "hidden_hidden.transformer.self_attn.out_proj.bias \t torch.Size([512])\n",
      "hidden_hidden.transformer.linear1.weight \t torch.Size([512, 512])\n",
      "hidden_hidden.transformer.linear1.bias \t torch.Size([512])\n",
      "hidden_hidden.transformer.linear2.weight \t torch.Size([512, 512])\n",
      "hidden_hidden.transformer.linear2.bias \t torch.Size([512])\n",
      "hidden_hidden.transformer.norm1.weight \t torch.Size([512])\n",
      "hidden_hidden.transformer.norm1.bias \t torch.Size([512])\n",
      "hidden_hidden.transformer.norm2.weight \t torch.Size([512])\n",
      "hidden_hidden.transformer.norm2.bias \t torch.Size([512])\n",
      "inner_hidden_model.hidden_hidden.transformer.self_attn.in_proj_weight \t torch.Size([1536, 512])\n",
      "inner_hidden_model.hidden_hidden.transformer.self_attn.in_proj_bias \t torch.Size([1536])\n",
      "inner_hidden_model.hidden_hidden.transformer.self_attn.out_proj.weight \t torch.Size([512, 512])\n",
      "inner_hidden_model.hidden_hidden.transformer.self_attn.out_proj.bias \t torch.Size([512])\n",
      "inner_hidden_model.hidden_hidden.transformer.linear1.weight \t torch.Size([512, 512])\n",
      "inner_hidden_model.hidden_hidden.transformer.linear1.bias \t torch.Size([512])\n",
      "inner_hidden_model.hidden_hidden.transformer.linear2.weight \t torch.Size([512, 512])\n",
      "inner_hidden_model.hidden_hidden.transformer.linear2.bias \t torch.Size([512])\n",
      "inner_hidden_model.hidden_hidden.transformer.norm1.weight \t torch.Size([512])\n",
      "inner_hidden_model.hidden_hidden.transformer.norm1.bias \t torch.Size([512])\n",
      "inner_hidden_model.hidden_hidden.transformer.norm2.weight \t torch.Size([512])\n",
      "inner_hidden_model.hidden_hidden.transformer.norm2.bias \t torch.Size([512])\n",
      "linear.weight \t torch.Size([16384, 512])\n",
      "linear.bias \t torch.Size([16384])\n",
      "layer_norm.weight \t torch.Size([512])\n",
      "layer_norm.bias \t torch.Size([512])\n",
      "embedding.weight \t torch.Size([16384, 512])\n",
      "positional_encoding.pe \t torch.Size([1, 5000, 512])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'),\n",
       " tensor([-1.4615e+00,  1.0513e+00, -6.4477e-03, -2.7760e+00, -1.1088e+00,\n",
       "          4.6590e-01, -1.2727e+00, -2.1451e-03,  1.9926e-01,  2.8776e-01],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_neural_map[0][:10], model.random_projection[0, :][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a previously saved model checkpoint...\n",
      "| epoch   1 |   300/ 1963 batches | lr 5.00 | ms/batch 38.49 | loss 76.58 | ppl 1810019763954201031114621590700032.00\n",
      "| epoch   1 |   600/ 1963 batches | lr 5.00 | ms/batch 45.25 | loss 67.44 | ppl 194475590375382478026849648640.00\n",
      "| epoch   1 |   900/ 1963 batches | lr 5.00 | ms/batch 52.34 | loss 62.74 | ppl 1775824796635758649780731904.00\n",
      "| epoch   1 |  1200/ 1963 batches | lr 5.00 | ms/batch 40.24 | loss 58.78 | ppl 33828075759793970316574720.00\n",
      "| epoch   1 |  1500/ 1963 batches | lr 5.00 | ms/batch 36.52 | loss 55.37 | ppl 1111682279145350600065024.00\n",
      "| epoch   1 |  1800/ 1963 batches | lr 5.00 | ms/batch 39.12 | loss 51.73 | ppl 29376417123173587222528.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 87.38s | valid loss 44.89 | valid ppl 31359799825478328320.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   2 |   300/ 1963 batches | lr 4.95 | ms/batch 40.09 | loss 47.70 | ppl 521602511768101060608.00\n",
      "| epoch   2 |   600/ 1963 batches | lr 4.95 | ms/batch 40.64 | loss 47.76 | ppl 554000862039384195072.00\n",
      "| epoch   2 |   900/ 1963 batches | lr 4.95 | ms/batch 38.64 | loss 47.44 | ppl 399343717239784341504.00\n",
      "| epoch   2 |  1200/ 1963 batches | lr 4.95 | ms/batch 46.96 | loss 45.25 | ppl 44753813521186045952.00\n",
      "| epoch   2 |  1500/ 1963 batches | lr 4.95 | ms/batch 37.45 | loss 43.99 | ppl 12781509754246293504.00\n",
      "| epoch   2 |  1800/ 1963 batches | lr 4.95 | ms/batch 40.05 | loss 41.64 | ppl 1209475840526245120.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 85.13s | valid loss 36.11 | valid ppl 4806769373758330.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   3 |   300/ 1963 batches | lr 4.90 | ms/batch 40.50 | loss 39.65 | ppl 165364261886325312.00\n",
      "| epoch   3 |   600/ 1963 batches | lr 4.90 | ms/batch 35.53 | loss 40.11 | ppl 262386171364686208.00\n",
      "| epoch   3 |   900/ 1963 batches | lr 4.90 | ms/batch 35.98 | loss 40.56 | ppl 410240854994544960.00\n",
      "| epoch   3 |  1200/ 1963 batches | lr 4.90 | ms/batch 39.38 | loss 38.81 | ppl 71788450393745840.00\n",
      "| epoch   3 |  1500/ 1963 batches | lr 4.90 | ms/batch 40.54 | loss 37.90 | ppl 28907127425244700.00\n",
      "| epoch   3 |  1800/ 1963 batches | lr 4.90 | ms/batch 39.67 | loss 36.56 | ppl 7569206107399904.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 81.49s | valid loss 32.64 | valid ppl 149502080046699.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   4 |   300/ 1963 batches | lr 4.85 | ms/batch 40.00 | loss 35.17 | ppl 1875410880638371.50\n",
      "| epoch   4 |   600/ 1963 batches | lr 4.85 | ms/batch 34.28 | loss 35.81 | ppl 3571159281378247.00\n",
      "| epoch   4 |   900/ 1963 batches | lr 4.85 | ms/batch 35.21 | loss 36.42 | ppl 6577267847882178.00\n",
      "| epoch   4 |  1200/ 1963 batches | lr 4.85 | ms/batch 39.22 | loss 34.84 | ppl 1353624899998128.25\n",
      "| epoch   4 |  1500/ 1963 batches | lr 4.85 | ms/batch 35.99 | loss 34.37 | ppl 844961770288387.88\n",
      "| epoch   4 |  1800/ 1963 batches | lr 4.85 | ms/batch 38.97 | loss 33.20 | ppl 262562539859226.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 78.86s | valid loss 27.97 | valid ppl 1401760412307.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   5 |   300/ 1963 batches | lr 4.80 | ms/batch 39.63 | loss 32.04 | ppl 82455613804784.77\n",
      "| epoch   5 |   600/ 1963 batches | lr 4.80 | ms/batch 33.51 | loss 32.66 | ppl 152014951945575.56\n",
      "| epoch   5 |   900/ 1963 batches | lr 4.80 | ms/batch 34.98 | loss 33.23 | ppl 270784319247442.59\n",
      "| epoch   5 |  1200/ 1963 batches | lr 4.80 | ms/batch 44.36 | loss 31.95 | ppl 74842442268539.30\n",
      "| epoch   5 |  1500/ 1963 batches | lr 4.80 | ms/batch 47.00 | loss 31.48 | ppl 46905997734380.88\n",
      "| epoch   5 |  1800/ 1963 batches | lr 4.80 | ms/batch 54.63 | loss 30.55 | ppl 18612299657269.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 87.98s | valid loss 26.90 | valid ppl 482011012460.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   6 |   300/ 1963 batches | lr 4.75 | ms/batch 39.44 | loss 29.51 | ppl 6576206443504.79\n",
      "| epoch   6 |   600/ 1963 batches | lr 4.75 | ms/batch 33.50 | loss 30.01 | ppl 10770850607655.43\n",
      "| epoch   6 |   900/ 1963 batches | lr 4.75 | ms/batch 35.91 | loss 30.53 | ppl 18225706648770.23\n",
      "| epoch   6 |  1200/ 1963 batches | lr 4.75 | ms/batch 38.95 | loss 29.30 | ppl 5321138144480.75\n",
      "| epoch   6 |  1500/ 1963 batches | lr 4.75 | ms/batch 36.12 | loss 28.81 | ppl 3245977002103.47\n",
      "| epoch   6 |  1800/ 1963 batches | lr 4.75 | ms/batch 42.64 | loss 27.99 | ppl 1430971929059.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 81.57s | valid loss 24.99 | valid ppl 71061527740.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   7 |   300/ 1963 batches | lr 4.71 | ms/batch 39.96 | loss 27.10 | ppl 587269696410.23\n",
      "| epoch   7 |   600/ 1963 batches | lr 4.71 | ms/batch 40.80 | loss 27.37 | ppl 772554775499.49\n",
      "| epoch   7 |   900/ 1963 batches | lr 4.71 | ms/batch 35.08 | loss 27.97 | ppl 1399863479753.78\n",
      "| epoch   7 |  1200/ 1963 batches | lr 4.71 | ms/batch 38.94 | loss 26.93 | ppl 493835720879.11\n",
      "| epoch   7 |  1500/ 1963 batches | lr 4.71 | ms/batch 36.03 | loss 26.42 | ppl 296808850241.09\n",
      "| epoch   7 |  1800/ 1963 batches | lr 4.71 | ms/batch 39.30 | loss 25.62 | ppl 133260662648.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 80.83s | valid loss 23.05 | valid ppl 10260890892.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   8 |   300/ 1963 batches | lr 4.66 | ms/batch 39.93 | loss 24.88 | ppl 63729708142.38\n",
      "| epoch   8 |   600/ 1963 batches | lr 4.66 | ms/batch 33.66 | loss 25.20 | ppl 87768521654.03\n",
      "| epoch   8 |   900/ 1963 batches | lr 4.66 | ms/batch 35.07 | loss 25.67 | ppl 140513261382.96\n",
      "| epoch   8 |  1200/ 1963 batches | lr 4.66 | ms/batch 39.25 | loss 24.64 | ppl 50377601572.56\n",
      "| epoch   8 |  1500/ 1963 batches | lr 4.66 | ms/batch 36.54 | loss 24.24 | ppl 33555221892.46\n",
      "| epoch   8 |  1800/ 1963 batches | lr 4.66 | ms/batch 40.01 | loss 23.46 | ppl 15389528152.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 79.19s | valid loss 20.13 | valid ppl 550434466.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   9 |   300/ 1963 batches | lr 4.61 | ms/batch 39.79 | loss 22.66 | ppl 6963752894.84\n",
      "| epoch   9 |   600/ 1963 batches | lr 4.61 | ms/batch 33.56 | loss 23.14 | ppl 11184116249.66\n",
      "| epoch   9 |   900/ 1963 batches | lr 4.61 | ms/batch 35.00 | loss 23.52 | ppl 16424356952.21\n",
      "| epoch   9 |  1200/ 1963 batches | lr 4.61 | ms/batch 38.99 | loss 22.72 | ppl 7364772214.92\n",
      "| epoch   9 |  1500/ 1963 batches | lr 4.61 | ms/batch 35.97 | loss 22.31 | ppl 4884960729.12\n",
      "| epoch   9 |  1800/ 1963 batches | lr 4.61 | ms/batch 39.32 | loss 21.77 | ppl 2856207168.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 78.61s | valid loss 18.69 | valid ppl 130795881.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch  10 |   300/ 1963 batches | lr 4.57 | ms/batch 39.62 | loss 21.06 | ppl 1400658518.50\n",
      "| epoch  10 |   600/ 1963 batches | lr 4.57 | ms/batch 33.57 | loss 21.48 | ppl 2135582346.65\n",
      "| epoch  10 |   900/ 1963 batches | lr 4.57 | ms/batch 35.20 | loss 21.92 | ppl 3306535054.34\n",
      "| epoch  10 |  1200/ 1963 batches | lr 4.57 | ms/batch 39.02 | loss 21.13 | ppl 1502608973.71\n",
      "| epoch  10 |  1500/ 1963 batches | lr 4.57 | ms/batch 35.84 | loss 20.84 | ppl 1128953903.37\n",
      "| epoch  10 |  1800/ 1963 batches | lr 4.57 | ms/batch 39.12 | loss 20.26 | ppl 629503963.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 78.47s | valid loss 18.20 | valid ppl 79896632.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch  11 |   300/ 1963 batches | lr 4.52 | ms/batch 39.70 | loss 19.80 | ppl 398795109.66\n",
      "| epoch  11 |   600/ 1963 batches | lr 4.52 | ms/batch 33.45 | loss 20.06 | ppl 513251246.07\n",
      "| epoch  11 |   900/ 1963 batches | lr 4.52 | ms/batch 34.99 | loss 20.50 | ppl 802322202.04\n",
      "| epoch  11 |  1200/ 1963 batches | lr 4.52 | ms/batch 38.79 | loss 19.83 | ppl 409914161.41\n",
      "| epoch  11 |  1500/ 1963 batches | lr 4.52 | ms/batch 36.03 | loss 19.60 | ppl 326004543.06\n",
      "| epoch  11 |  1800/ 1963 batches | lr 4.52 | ms/batch 39.23 | loss 19.13 | ppl 203838493.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 78.45s | valid loss 16.91 | valid ppl 22092869.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch  12 |   300/ 1963 batches | lr 4.48 | ms/batch 39.68 | loss 18.61 | ppl 120927666.22\n",
      "| epoch  12 |   600/ 1963 batches | lr 4.48 | ms/batch 33.47 | loss 18.94 | ppl 167467872.24\n",
      "| epoch  12 |   900/ 1963 batches | lr 4.48 | ms/batch 35.03 | loss 19.44 | ppl 275791650.74\n",
      "| epoch  12 |  1200/ 1963 batches | lr 4.48 | ms/batch 38.82 | loss 18.77 | ppl 141191121.20\n",
      "| epoch  12 |  1500/ 1963 batches | lr 4.48 | ms/batch 35.80 | loss 18.56 | ppl 115353931.47\n",
      "| epoch  12 |  1800/ 1963 batches | lr 4.48 | ms/batch 39.03 | loss 18.12 | ppl 74064190.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 78.36s | valid loss 17.05 | valid ppl 25319616.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   300/ 1963 batches | lr 4.43 | ms/batch 39.06 | loss 17.72 | ppl 49392665.53\n",
      "| epoch  13 |   600/ 1963 batches | lr 4.43 | ms/batch 32.74 | loss 18.00 | ppl 65987372.66\n",
      "| epoch  13 |   900/ 1963 batches | lr 4.43 | ms/batch 34.34 | loss 18.46 | ppl 104333072.34\n",
      "| epoch  13 |  1200/ 1963 batches | lr 4.43 | ms/batch 38.08 | loss 17.89 | ppl 58754611.90\n",
      "| epoch  13 |  1500/ 1963 batches | lr 4.43 | ms/batch 35.28 | loss 17.67 | ppl 47056000.87\n",
      "| epoch  13 |  1800/ 1963 batches | lr 4.43 | ms/batch 38.53 | loss 17.28 | ppl 32018451.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 77.08s | valid loss 14.88 | valid ppl 2894839.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch  14 |   300/ 1963 batches | lr 4.39 | ms/batch 39.12 | loss 16.84 | ppl 20569025.28\n",
      "| epoch  14 |   600/ 1963 batches | lr 4.39 | ms/batch 32.98 | loss 17.24 | ppl 30582469.18\n",
      "| epoch  14 |   900/ 1963 batches | lr 4.39 | ms/batch 34.62 | loss 17.68 | ppl 47897953.32\n",
      "| epoch  14 |  1200/ 1963 batches | lr 4.39 | ms/batch 38.29 | loss 17.10 | ppl 26614830.78\n",
      "| epoch  14 |  1500/ 1963 batches | lr 4.39 | ms/batch 35.43 | loss 16.97 | ppl 23529135.04\n",
      "| epoch  14 |  1800/ 1963 batches | lr 4.39 | ms/batch 38.60 | loss 16.56 | ppl 15565989.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 77.36s | valid loss 14.03 | valid ppl 1240245.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch  15 |   300/ 1963 batches | lr 4.34 | ms/batch 39.09 | loss 16.27 | ppl 11634427.71\n",
      "| epoch  15 |   600/ 1963 batches | lr 4.34 | ms/batch 32.98 | loss 16.62 | ppl 16537147.83\n",
      "| epoch  15 |   900/ 1963 batches | lr 4.34 | ms/batch 34.52 | loss 16.88 | ppl 21373368.66\n",
      "| epoch  15 |  1200/ 1963 batches | lr 4.34 | ms/batch 38.32 | loss 16.43 | ppl 13728712.49\n",
      "| epoch  15 |  1500/ 1963 batches | lr 4.34 | ms/batch 35.41 | loss 16.33 | ppl 12312749.56\n",
      "| epoch  15 |  1800/ 1963 batches | lr 4.34 | ms/batch 38.42 | loss 16.02 | ppl 9050053.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 77.24s | valid loss 13.29 | valid ppl 593075.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch  16 |   300/ 1963 batches | lr 4.30 | ms/batch 38.79 | loss 15.62 | ppl 6068960.62\n",
      "| epoch  16 |   600/ 1963 batches | lr 4.30 | ms/batch 32.75 | loss 15.99 | ppl 8835576.16\n",
      "| epoch  16 |   900/ 1963 batches | lr 4.30 | ms/batch 34.49 | loss 16.36 | ppl 12702274.97\n",
      "| epoch  16 |  1200/ 1963 batches | lr 4.30 | ms/batch 38.09 | loss 15.89 | ppl 7959237.41\n",
      "| epoch  16 |  1500/ 1963 batches | lr 4.30 | ms/batch 35.19 | loss 15.83 | ppl 7506471.26\n",
      "| epoch  16 |  1800/ 1963 batches | lr 4.30 | ms/batch 38.40 | loss 15.36 | ppl 4707593.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 76.90s | valid loss 13.39 | valid ppl 653282.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   300/ 1963 batches | lr 4.26 | ms/batch 38.81 | loss 15.10 | ppl 3628002.24\n",
      "| epoch  17 |   600/ 1963 batches | lr 4.26 | ms/batch 32.60 | loss 15.44 | ppl 5077148.87\n",
      "| epoch  17 |   900/ 1963 batches | lr 4.26 | ms/batch 34.21 | loss 15.86 | ppl 7735077.35\n",
      "| epoch  17 |  1200/ 1963 batches | lr 4.26 | ms/batch 38.12 | loss 15.47 | ppl 5243364.65\n",
      "| epoch  17 |  1500/ 1963 batches | lr 4.26 | ms/batch 35.19 | loss 15.31 | ppl 4452651.93\n",
      "| epoch  17 |  1800/ 1963 batches | lr 4.26 | ms/batch 51.78 | loss 15.01 | ppl 3296802.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 81.00s | valid loss 13.00 | valid ppl 442001.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch  18 |   300/ 1963 batches | lr 4.21 | ms/batch 39.21 | loss 14.62 | ppl 2240333.77\n",
      "| epoch  18 |   600/ 1963 batches | lr 4.21 | ms/batch 36.51 | loss 15.06 | ppl 3477586.09\n",
      "| epoch  18 |   900/ 1963 batches | lr 4.21 | ms/batch 34.83 | loss 15.39 | ppl 4815969.42\n",
      "| epoch  18 |  1200/ 1963 batches | lr 4.21 | ms/batch 45.61 | loss 15.04 | ppl 3403491.93\n",
      "| epoch  18 |  1500/ 1963 batches | lr 4.21 | ms/batch 58.29 | loss 14.88 | ppl 2887829.17\n",
      "| epoch  18 |  1800/ 1963 batches | lr 4.21 | ms/batch 57.19 | loss 14.54 | ppl 2067119.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 98.98s | valid loss 13.06 | valid ppl 469733.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   300/ 1963 batches | lr 4.17 | ms/batch 52.65 | loss 14.29 | ppl 1609245.58\n",
      "| epoch  19 |   600/ 1963 batches | lr 4.17 | ms/batch 46.12 | loss 14.59 | ppl 2176047.76\n",
      "| epoch  19 |   900/ 1963 batches | lr 4.17 | ms/batch 64.08 | loss 14.93 | ppl 3040955.37\n",
      "| epoch  19 |  1200/ 1963 batches | lr 4.17 | ms/batch 66.17 | loss 14.55 | ppl 2091486.08\n",
      "| epoch  19 |  1500/ 1963 batches | lr 4.17 | ms/batch 46.84 | loss 14.48 | ppl 1937232.24\n",
      "| epoch  19 |  1800/ 1963 batches | lr 4.17 | ms/batch 39.22 | loss 14.17 | ppl 1428388.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 106.14s | valid loss 12.77 | valid ppl 351561.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch  20 |   300/ 1963 batches | lr 4.13 | ms/batch 38.92 | loss 13.94 | ppl 1130036.42\n",
      "| epoch  20 |   600/ 1963 batches | lr 4.13 | ms/batch 53.25 | loss 14.24 | ppl 1522333.95\n",
      "| epoch  20 |   900/ 1963 batches | lr 4.13 | ms/batch 89.91 | loss 14.59 | ppl 2162433.56\n",
      "| epoch  20 |  1200/ 1963 batches | lr 4.13 | ms/batch 86.22 | loss 14.18 | ppl 1442105.25\n",
      "| epoch  20 |  1500/ 1963 batches | lr 4.13 | ms/batch 59.90 | loss 14.15 | ppl 1396462.94\n",
      "| epoch  20 |  1800/ 1963 batches | lr 4.13 | ms/batch 60.25 | loss 13.92 | ppl 1111503.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 128.28s | valid loss 11.91 | valid ppl 148328.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "Loading the best overall model...\n",
      "Saving the best overall model...\n"
     ]
    }
   ],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 20\n",
    "\n",
    "final_model_params_path = os.path.join(\"../models/\", \"neural_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"Saving the new best model...\")\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loading the best overall model...\")\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "    print(f\"Saving the best overall model...\")\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        os.path.join(\"../models/\", \"neural_transformer_model.pt\"),\n",
    "    )  # save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.3579, -1.2398, -1.8815,  0.1538,  1.1946, -0.4549, -0.2907, -0.3557,\n",
       "         -1.4055,  0.2469], device='cuda:0'),\n",
       " tensor([-0.1848, -0.9811, -0.0892,  0.7338, -0.2960,  0.0940, -0.1746, -0.0378,\n",
       "         -0.2782,  1.1651], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_neural_map[0][:10], model.random_projection[0, :][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 302]) torch.Size([1, 135, 302]) torch.Size([1, 235, 302]) torch.Size([30522, 302])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Generate new data using test input\n",
    "\n",
    "data = train_dataset[0].unsqueeze(0).to(DEVICE)\n",
    "# data = test_dataset[0].unsqueeze(0).to(DEVICE)\n",
    "mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0).to(DEVICE)\n",
    "embedding = embedding.to(DEVICE)\n",
    "\n",
    "max_new_tokens = 100\n",
    "data_gen = model.generate(data, mask, max_new_tokens, top_k=1)\n",
    "\n",
    "print(mask.shape, data.shape, data_gen.shape, embedding.weight.shape, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2034, 6926, 1024, 2077, 2057, 10838, 2151, 2582, 1010, 2963, 2033, 3713, 1012, 2035, 1024, 3713, 1010, 3713, 1012, 2034, 6926, 1024, 2017, 2024, 2035, 10395, 2738, 2000, 3280, 2084, 2000, 6904, 15630, 2232, 1029, 2035, 1024, 10395, 1012, 10395, 1012, 2034, 6926, 1024, 2034, 1010, 2017, 2113, 29080, 2271, 7871, 4173, 2003, 2708, 4099, 2000, 1996, 2111, 1012, 2035, 1024, 2057, 2113, 1005, 1056, 1010, 2057, 2113, 1005, 1056, 1012, 2034, 6926, 1024, 2292, 2149, 3102, 2032, 1010, 1998, 2057, 1005, 2222, 2031, 9781, 2012, 2256, 2219, 3976, 1012, 2003, 1005, 1056, 1037, 14392, 1029, 2035, 1024, 2053, 2062, 3331, 2006, 1005, 1056, 1025, 2292, 2009, 2022, 2589, 1024, 2185, 1010, 2185, 999, 2117, 6926, 1024, 2028, 2773, 1010, 2204, 4480, 1012, 2034, 6926, 1024, 2057, 2024, 14729, 3532, 4480, 1010, 1996, 102]\n",
      "\n",
      "[CLS] first citizen : before we proceed any further, hear me speak. all : speak, speak. first citizen : you are all resolved rather to die than to famish? all : resolved. resolved. first citizen : first, you know caius marcius is chief enemy to the people. all : we know't, we know't. first citizen : let us kill him, and we'll have corn at our own price. is't a verdict? all : no more talking on't ; let it be done : away, away! second citizen : one word, good citizens. first citizen : we are accounted poor citizens, the [SEP]\n",
      "\n",
      "[101, 2743, 3401, 11937, 1005, 4372, 2004, 4618, 2007, 2593, 2112, 1005, 1055, 3820, 3233, 1029, 7550, 2050, 1024, 2025, 1999, 2026, 2160, 1010, 19913, 16778, 2080, 1025, 2005, 1010, 2017, 2113, 1010, 23232, 2031, 5551, 1010, 1998, 1045, 2031, 2116, 8858, 1024, 4661, 1010, 2214, 24665, 23238, 2080, 2003, 2963, 7520, 2075, 2145, 1025, 1998, 11361, 2057, 2453, 2022, 7153, 1012, 25283, 3695, 1024, 2059, 2012, 2026, 26859, 1010, 2019, 2009, 2066, 2017, 1024, 2045, 11089, 2232, 2026, 2269, 4682, 1025, 1998, 2045, 1010, 2023, 2305, 1010, 2057, 1005, 2222, 3413, 1996, 2449, 9139, 1998, 2092, 1012, 4604, 2005, 2115, 2684, 2011, 2115, 7947, 2182, 1024, 2026, 2879, 4618, 18584, 1996, 8040, 3089, 8159, 2121, 12825, 1012, 1996, 5409, 2003, 2023, 1010, 2008, 1010, 2012, 2061, 10944, 5432, 1010, 2017, 102]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @markdown We want to find the nearest token to each generated embedding.\n",
    "# We can do this by finding the nearest embedding to each generated embedding\n",
    "# and then finding the index corresponding to that embedding.\n",
    "\n",
    "# First run a test on data we know what the true token output should be\n",
    "with torch.no_grad():\n",
    "    sequence = data\n",
    "    tokens = model.tokenize_neural_data(\n",
    "        neural_sequence=data, feature_mask=mask, token_matrix=embedding.weight\n",
    "    )\n",
    "    print(tokens.squeeze().tolist(), end=\"\\n\\n\")\n",
    "    # decode into text\n",
    "    print(tokenizer.decode(tokens.squeeze().tolist()), end=\"\\n\\n\")\n",
    "\n",
    "# If correct this should match\n",
    "print(text_dataset[\"test\"][\"input_ids\"][0], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15223, 21146, 4618, 2022, 2993, 2404, 101, 5897, 1029, 5551, 2050, 1024, 2023, 2014, 1010, 2002, 2063, 2014, 2175, 3367, 3917, 3913, 3602, 1997, 2014, 2175, 2014, 2175, 2014, 1997, 18094, 18674, 14992, 1996, 7350, 3102, 2014, 29603, 2025, 2005, 4757, 1998, 2870, 2004, 6904, 2022, 1029, 2033, 2045, 29080, 3102, 2010, 2102, 2022, 1029, 5551, 2050, 1024, 1045, 1998, 11818, 2015, 1024, 2993, 2404, 7174, 2004, 2012, 2004, 2005, 6916, 4355, 1012, 2033, 2097, 7174, 1012, 24665, 6904, 1997, 3233, 3098, 3102, 2045, 2035, 4826, 2182, 1005, 5605, 2290, 6904, 6904, 2004, 2684, 7947, 1024, 1012, 24665, 2097, 1045]\n",
      "\n",
      "thou sha shall be itself put [CLS]nce? earsa : this her, hee her gostler lay note of her go her go her of endurenous thee the derby kill her dowry not forss and myself as fa be? me there cai kill hist be? earsa : i anddges : itself putllo as at as forttiest. me willllo. gr fa of stand opening kill there all tomorrow here 'hipg fa fa as daughter servant :. gr will i\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the same thing on the newly generated data\n",
    "with torch.no_grad():\n",
    "    sequence = data_gen[:, -max_new_tokens:, :]\n",
    "    tokens = model.tokenize_neural_data(\n",
    "        neural_sequence=sequence,\n",
    "        feature_mask=mask,\n",
    "        token_matrix=embedding.weight,\n",
    "    )\n",
    "    print(tokens.squeeze().tolist(), end=\"\\n\\n\")\n",
    "    # decode into text\n",
    "    print(tokenizer.decode(tokens.squeeze().tolist()), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
