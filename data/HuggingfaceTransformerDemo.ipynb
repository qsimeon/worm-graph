{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device found.\n"
     ]
    }
   ],
   "source": [
    "# @title Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import nbimporter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tempfile import TemporaryDirectory\n",
    "from models._utils import NeuralTransformer\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "from utils import DEVICE, BLOCK_SIZE, NUM_TOKENS, init_random_seeds\n",
    "from CreateSyntheticDataset import tokenize_and_chunk  # works because of nbimporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling with tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `tokenizer.encode(text)`:\n",
      "\ttext: Welcome to the ðŸ¤— Tokenizers library.\n",
      "\ttokenized: [90, 104, 111, 102, 114, 112, 104, 35, 119, 114, 35, 119, 107, 104, 35, 243, 162, 167, 154, 35, 87, 114, 110, 104, 113, 108, 125, 104, 117, 118, 35, 111, 108, 101, 117, 100, 117, 124, 49, 1]\n",
      "\tdecoded: Welcome to the ðŸ¤— Tokenizers library.</s>\n",
      "\n",
      "Calling `tokenizer(text)`:\n",
      "\tobject.keys(): dict_keys(['input_ids', 'attention_mask'])\n",
      "\ttext: We are very happy to show you the ðŸ¤— Transformers library.\n",
      "\ttokenized: [90, 104, 35, 100, 117, 104, 35, 121, 104, 117, 124, 35, 107, 100, 115, 115, 124, 35, 119, 114, 35, 118, 107, 114, 122, 35, 124, 114, 120, 35, 119, 107, 104, 35, 243, 162, 167, 154, 35, 87, 117, 100, 113, 118, 105, 114, 117, 112, 104, 117, 118, 35, 111, 108, 101, 117, 100, 117, 124, 49, 1]\n",
      "\tdecoded: We are very happy to show you the ðŸ¤— Transformers library.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Tokenizers\n",
    "# @markdown Note there are two ways to call the tokenizer's encoder.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-large\")\n",
    "\n",
    "expl_text = \"Welcome to the ðŸ¤— Tokenizers library.\"\n",
    "impl_text = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
    "expl_encode = tokenizer.encode(expl_text)\n",
    "impl_encode = tokenizer(impl_text)\n",
    "print(\n",
    "    f\"Calling `tokenizer.encode(text)`:\\n\\ttext: {expl_text}\\n\\ttokenized: {expl_encode}\\n\\tdecoded: {tokenizer.decode(expl_encode)}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Calling `tokenizer(text)`:\\n\\tobject.keys(): {impl_encode.keys()}\\n\\ttext: {impl_text}\\n\\ttokenized: {impl_encode['input_ids']}\\n\\tdecoded: {tokenizer.decode(impl_encode['input_ids'])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "train: <class 'list'> 1 <class 'str'> 1003854\n",
      "\n",
      "validation: <class 'list'> 1 <class 'str'> 55770\n",
      "\n",
      "test: <class 'list'> 1 <class 'str'> 55770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Datasets\n",
    "\n",
    "text_dataset = load_hf_dataset(\"tiny_shakespeare\")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"train:\",\n",
    "    type(text_dataset[\"train\"][\"text\"]),\n",
    "    len(text_dataset[\"train\"][\"text\"]),\n",
    "    type(text_dataset[\"train\"][\"text\"][0]),\n",
    "    len(text_dataset[\"train\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"validation:\",\n",
    "    type(text_dataset[\"validation\"][\"text\"]),\n",
    "    len(text_dataset[\"validation\"][\"text\"]),\n",
    "    type(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    len(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"test:\",\n",
    "    type(text_dataset[\"test\"][\"text\"]),\n",
    "    len(text_dataset[\"test\"][\"text\"]),\n",
    "    type(text_dataset[\"test\"][\"text\"][0]),\n",
    "    len(text_dataset[\"test\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1963\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "})\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "text_dataset['train']['input_ids']:\n",
      " \ttype: <class 'list'> \n",
      "\tlength: 1963\n",
      "\n",
      "text_dataset['train']['input_ids'][0]:\n",
      " \ttype: <class 'list'> \n",
      "\tlength: 506\n",
      "\n",
      "text_dataset['train']['input_ids'][0][0]:\n",
      " \ttype: <class 'int'> \n",
      "\tvalue: 73\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Original sequence (text):\n",
      "\tFirst Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? All: Resolved. resolved. First Citizen: First, you know Caius Marcius is chief enemy to the people. All: We know't, we know't. First Citizen: Let us kill him, and we'll have corn at our own price. Is't a verdict? All: No more talking on't; let it be done: away, away! Second Citizen: One word, good citizens. First Citizen: We are accounted poor citizens, the\n",
      "\n",
      "Encoded sequence (tokens):\n",
      "\t [73, 108, 117, 118, 119, 35, 70, 108, 119, 108, 125, 104, 113, 61, 35, 69, 104, 105, 114, 117, 104, 35, 122, 104, 35, 115, 117, 114, 102, 104, 104, 103, 35, 100, 113, 124, 35, 105, 120, 117, 119, 107, 104, 117, 47, 35, 107, 104, 100, 117, 35, 112, 104, 35, 118, 115, 104, 100, 110, 49, 35, 68, 111, 111, 61, 35, 86, 115, 104, 100, 110, 47, 35, 118, 115, 104, 100, 110, 49, 35, 73, 108, 117, 118, 119, 35, 70, 108, 119, 108, 125, 104, 113, 61, 35, 92, 114, 120, 35, 100, 117, 104, 35, 100, 111, 111, 35, 117, 104, 118, 114, 111, 121, 104, 103, 35, 117, 100, 119, 107, 104, 117, 35, 119, 114, 35, 103, 108, 104, 35, 119, 107, 100, 113, 35, 119, 114, 35, 105, 100, 112, 108, 118, 107, 66, 35, 68, 111, 111, 61, 35, 85, 104, 118, 114, 111, 121, 104, 103, 49, 35, 117, 104, 118, 114, 111, 121, 104, 103, 49, 35, 73, 108, 117, 118, 119, 35, 70, 108, 119, 108, 125, 104, 113, 61, 35, 73, 108, 117, 118, 119, 47, 35, 124, 114, 120, 35, 110, 113, 114, 122, 35, 70, 100, 108, 120, 118, 35, 80, 100, 117, 102, 108, 120, 118, 35, 108, 118, 35, 102, 107, 108, 104, 105, 35, 104, 113, 104, 112, 124, 35, 119, 114, 35, 119, 107, 104, 35, 115, 104, 114, 115, 111, 104, 49, 35, 68, 111, 111, 61, 35, 90, 104, 35, 110, 113, 114, 122, 42, 119, 47, 35, 122, 104, 35, 110, 113, 114, 122, 42, 119, 49, 35, 73, 108, 117, 118, 119, 35, 70, 108, 119, 108, 125, 104, 113, 61, 35, 79, 104, 119, 35, 120, 118, 35, 110, 108, 111, 111, 35, 107, 108, 112, 47, 35, 100, 113, 103, 35, 122, 104, 42, 111, 111, 35, 107, 100, 121, 104, 35, 102, 114, 117, 113, 35, 100, 119, 35, 114, 120, 117, 35, 114, 122, 113, 35, 115, 117, 108, 102, 104, 49, 35, 76, 118, 42, 119, 35, 100, 35, 121, 104, 117, 103, 108, 102, 119, 66, 35, 68, 111, 111, 61, 35, 81, 114, 35, 112, 114, 117, 104, 35, 119, 100, 111, 110, 108, 113, 106, 35, 114, 113, 42, 119, 62, 35, 111, 104, 119, 35, 108, 119, 35, 101, 104, 35, 103, 114, 113, 104, 61, 35, 100, 122, 100, 124, 47, 35, 100, 122, 100, 124, 36, 35, 86, 104, 102, 114, 113, 103, 35, 70, 108, 119, 108, 125, 104, 113, 61, 35, 82, 113, 104, 35, 122, 114, 117, 103, 47, 35, 106, 114, 114, 103, 35, 102, 108, 119, 108, 125, 104, 113, 118, 49, 35, 73, 108, 117, 118, 119, 35, 70, 108, 119, 108, 125, 104, 113, 61, 35, 90, 104, 35, 100, 117, 104, 35, 100, 102, 102, 114, 120, 113, 119, 104, 103, 35, 115, 114, 114, 117, 35, 102, 108, 119, 108, 125, 104, 113, 118, 47, 35, 119, 107, 104, 1]\n",
      "\n",
      "Decoded sequence (tokens):\n",
      "\t First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? All: Resolved. resolved. First Citizen: First, you know Caius Marcius is chief enemy to the people. All: We know't, we know't. First Citizen: Let us kill him, and we'll have corn at our own price. Is't a verdict? All: No more talking on't; let it be done: away, away! Second Citizen: One word, good citizens. First Citizen: We are accounted poor citizens, the</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Tokenization and Chunking\n",
    "# @markdown Apply the tokenization and chunking to each split.\n",
    "\n",
    "text_dataset = text_dataset.map(\n",
    "    tokenize_and_chunk, batched=True, fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids']:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0][0]),\n",
    "    \"\\n\\tvalue:\",\n",
    "    text_dataset[\"train\"][\"input_ids\"][0][0],\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(f\"Original sequence (text):\\n\\t{text_dataset['train']['text'][0]}\", end=\"\\n\\n\")\n",
    "print(\n",
    "    f\"Encoded sequence (tokens):\\n\\t {text_dataset['train']['input_ids'][0]}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Decoded sequence (tokens):\\n\\t {tokenizer.decode(text_dataset['train']['input_ids'][0])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define the Transformer model\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)  # batch_first=True\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ntoken: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        d_hid: int,\n",
    "        nlayers: int,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.d_model = d_model\n",
    "        self.embedding = torch.nn.Embedding(ntoken, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.linear = torch.nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape [batch_size, seq_len]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [batch_size, seq_len, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            src_mask = torch.nn.Transformer.generate_square_subsequent_mask(\n",
    "                src.size(1)  # Use src.size(1) to get the seq_len\n",
    "            ).to(\n",
    "                src.device\n",
    "            )  # Use src.device to match device of src\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: torch.LongTensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Special generate method for the Transformer model.\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Since we trained the model to directly predict the next token we take the index as the argmin\n",
    "        over the distance between the output and the embedding table.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        # Loop through time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= BLOCK_SIZE else idx[:, -BLOCK_SIZE:]\n",
    "            # forward the model to get the output\n",
    "            outputs = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = outputs[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1).view(1, 1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  torch.Size([1, 511]) torch.int64 False cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create token datasets\n",
    "\n",
    "train_dataset = torch.nested.nested_tensor(text_dataset[\"train\"][\"input_ids\"], dtype=torch.long)\n",
    "validation_dataset = torch.nested.nested_tensor(\n",
    "    text_dataset[\"validation\"][\"input_ids\"], dtype=torch.long\n",
    ")\n",
    "test_dataset = torch.nested.nested_tensor(text_dataset[\"test\"][\"input_ids\"], dtype=torch.long)\n",
    "\n",
    "# get a test sample for an example\n",
    "data = test_dataset[0].unsqueeze(0)\n",
    "print(\"data: \", data.shape, data.dtype, data.requires_grad, data.device, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (i.e. num tokens) = 256\n",
      "Number of attn heads = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Instantiate a TransformerModel\n",
    "\n",
    "ntokens = tokenizer.vocab_size\n",
    "emsize = 302  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention (NOTE: nhead must be a divisor of d_hid)\n",
    "dropout = 0.1  # dropout probability\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(DEVICE)\n",
    "print(f\"Vocab size (i.e. num tokens) = {ntokens}\")\n",
    "print(f\"Number of attn heads = {nhead}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the Transformer model\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch, optimizer, scheduler, criterion\n",
    "\n",
    "    num_batches = train_dataset.size(0)\n",
    "    for batch in range(num_batches):\n",
    "        # tokens = text_dataset[\"train\"][\"input_ids\"][batch]\n",
    "        tokens = train_dataset[batch].unsqueeze(0)\n",
    "        # parse into input and target\n",
    "        input = tokens[:, :-1].to(DEVICE)  # ``[batch_size=1, seq_len]``\n",
    "        target = tokens[:, 1:].reshape(-1).to(DEVICE)  # ``[batch_size=1 * seq_len]``\n",
    "        # forward pass\n",
    "        output = model(input)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        output_flat = output.view(-1, ntokens)  # ``[batch_size=1 * seq_len, ntokens]``\n",
    "        # backpropagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output_flat, target)\n",
    "        # check if the computed loss requires gradient\n",
    "        if loss.requires_grad:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = validation_dataset.size(0)\n",
    "        for batch in range(num_batches):\n",
    "            tokens = validation_dataset[batch].unsqueeze(0)\n",
    "            input = tokens[:, :-1].to(DEVICE)\n",
    "            target = tokens[:, 1:].reshape(-1).to(DEVICE)\n",
    "            output = model(input)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += criterion(output_flat, target).item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epoch(s)...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   300/ 1963 batches | lr 5.00 | ms/batch  8.85 | loss  3.47 | ppl    32.01\n",
      "| epoch   1 |   600/ 1963 batches | lr 5.00 | ms/batch  5.90 | loss  2.64 | ppl    14.03\n",
      "| epoch   1 |   900/ 1963 batches | lr 5.00 | ms/batch  5.70 | loss  2.60 | ppl    13.53\n",
      "| epoch   1 |  1200/ 1963 batches | lr 5.00 | ms/batch  8.45 | loss  2.56 | ppl    12.91\n",
      "| epoch   1 |  1500/ 1963 batches | lr 5.00 | ms/batch  6.62 | loss  2.54 | ppl    12.65\n",
      "| epoch   1 |  1800/ 1963 batches | lr 5.00 | ms/batch  6.39 | loss  2.51 | ppl    12.33\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 13.80s | valid loss  2.60 | valid ppl    13.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch   2 |   300/ 1963 batches | lr 4.95 | ms/batch  6.17 | loss  2.50 | ppl    12.23\n",
      "| epoch   2 |   600/ 1963 batches | lr 4.95 | ms/batch  6.74 | loss  2.52 | ppl    12.44\n",
      "| epoch   2 |   900/ 1963 batches | lr 4.95 | ms/batch  6.12 | loss  2.55 | ppl    12.81\n",
      "| epoch   2 |  1200/ 1963 batches | lr 4.95 | ms/batch  6.09 | loss  2.54 | ppl    12.65\n",
      "| epoch   2 |  1500/ 1963 batches | lr 4.95 | ms/batch  6.14 | loss  2.52 | ppl    12.40\n",
      "| epoch   2 |  1800/ 1963 batches | lr 4.95 | ms/batch  6.40 | loss  2.50 | ppl    12.24\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 12.64s | valid loss  2.55 | valid ppl    12.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch   3 |   300/ 1963 batches | lr 4.90 | ms/batch  6.84 | loss  2.49 | ppl    12.08\n",
      "| epoch   3 |   600/ 1963 batches | lr 4.90 | ms/batch  6.29 | loss  2.51 | ppl    12.35\n",
      "| epoch   3 |   900/ 1963 batches | lr 4.90 | ms/batch  7.12 | loss  2.54 | ppl    12.65\n",
      "| epoch   3 |  1200/ 1963 batches | lr 4.90 | ms/batch  6.89 | loss  2.52 | ppl    12.39\n",
      "| epoch   3 |  1500/ 1963 batches | lr 4.90 | ms/batch  5.93 | loss  2.51 | ppl    12.27\n",
      "| epoch   3 |  1800/ 1963 batches | lr 4.90 | ms/batch  6.54 | loss  2.49 | ppl    12.08\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 13.29s | valid loss  2.56 | valid ppl    12.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   300/ 1963 batches | lr 4.85 | ms/batch 10.68 | loss  2.50 | ppl    12.16\n",
      "| epoch   4 |   600/ 1963 batches | lr 4.85 | ms/batch  5.97 | loss  2.50 | ppl    12.23\n",
      "| epoch   4 |   900/ 1963 batches | lr 4.85 | ms/batch  6.11 | loss  2.53 | ppl    12.57\n",
      "| epoch   4 |  1200/ 1963 batches | lr 4.85 | ms/batch  6.52 | loss  2.51 | ppl    12.32\n",
      "| epoch   4 |  1500/ 1963 batches | lr 4.85 | ms/batch  5.74 | loss  2.51 | ppl    12.26\n",
      "| epoch   4 |  1800/ 1963 batches | lr 4.85 | ms/batch  5.98 | loss  2.48 | ppl    11.92\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 13.47s | valid loss  2.55 | valid ppl    12.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch   5 |   300/ 1963 batches | lr 4.80 | ms/batch  6.08 | loss  2.48 | ppl    11.95\n",
      "| epoch   5 |   600/ 1963 batches | lr 4.80 | ms/batch  5.95 | loss  2.50 | ppl    12.15\n",
      "| epoch   5 |   900/ 1963 batches | lr 4.80 | ms/batch  5.70 | loss  2.53 | ppl    12.55\n",
      "| epoch   5 |  1200/ 1963 batches | lr 4.80 | ms/batch  5.62 | loss  2.51 | ppl    12.24\n",
      "| epoch   5 |  1500/ 1963 batches | lr 4.80 | ms/batch  5.94 | loss  2.50 | ppl    12.17\n",
      "| epoch   5 |  1800/ 1963 batches | lr 4.80 | ms/batch  5.70 | loss  2.47 | ppl    11.87\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 11.61s | valid loss  2.55 | valid ppl    12.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   300/ 1963 batches | lr 4.75 | ms/batch  5.57 | loss  2.48 | ppl    11.89\n",
      "| epoch   6 |   600/ 1963 batches | lr 4.75 | ms/batch  5.73 | loss  2.51 | ppl    12.35\n",
      "| epoch   6 |   900/ 1963 batches | lr 4.75 | ms/batch  5.66 | loss  2.53 | ppl    12.55\n",
      "| epoch   6 |  1200/ 1963 batches | lr 4.75 | ms/batch  5.58 | loss  2.50 | ppl    12.23\n",
      "| epoch   6 |  1500/ 1963 batches | lr 4.75 | ms/batch  5.59 | loss  2.50 | ppl    12.21\n",
      "| epoch   6 |  1800/ 1963 batches | lr 4.75 | ms/batch  5.62 | loss  2.51 | ppl    12.30\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 11.25s | valid loss  2.53 | valid ppl    12.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch   7 |   300/ 1963 batches | lr 4.71 | ms/batch  5.63 | loss  2.47 | ppl    11.87\n",
      "| epoch   7 |   600/ 1963 batches | lr 4.71 | ms/batch  5.58 | loss  2.49 | ppl    12.03\n",
      "| epoch   7 |   900/ 1963 batches | lr 4.71 | ms/batch  5.57 | loss  2.52 | ppl    12.48\n",
      "| epoch   7 |  1200/ 1963 batches | lr 4.71 | ms/batch  5.59 | loss  2.50 | ppl    12.17\n",
      "| epoch   7 |  1500/ 1963 batches | lr 4.71 | ms/batch  5.60 | loss  2.49 | ppl    12.07\n",
      "| epoch   7 |  1800/ 1963 batches | lr 4.71 | ms/batch  5.59 | loss  2.47 | ppl    11.82\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 11.29s | valid loss  2.53 | valid ppl    12.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch   8 |   300/ 1963 batches | lr 4.66 | ms/batch  9.56 | loss  2.47 | ppl    11.85\n",
      "| epoch   8 |   600/ 1963 batches | lr 4.66 | ms/batch  5.74 | loss  2.49 | ppl    12.12\n",
      "| epoch   8 |   900/ 1963 batches | lr 4.66 | ms/batch  5.70 | loss  2.52 | ppl    12.41\n",
      "| epoch   8 |  1200/ 1963 batches | lr 4.66 | ms/batch  5.80 | loss  2.51 | ppl    12.32\n",
      "| epoch   8 |  1500/ 1963 batches | lr 4.66 | ms/batch  5.59 | loss  2.50 | ppl    12.20\n",
      "| epoch   8 |  1800/ 1963 batches | lr 4.66 | ms/batch  5.75 | loss  2.47 | ppl    11.79\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.59s | valid loss  2.55 | valid ppl    12.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   300/ 1963 batches | lr 4.61 | ms/batch  5.69 | loss  2.47 | ppl    11.82\n",
      "| epoch   9 |   600/ 1963 batches | lr 4.61 | ms/batch  5.63 | loss  2.49 | ppl    12.04\n",
      "| epoch   9 |   900/ 1963 batches | lr 4.61 | ms/batch  5.62 | loss  2.52 | ppl    12.37\n",
      "| epoch   9 |  1200/ 1963 batches | lr 4.61 | ms/batch  5.64 | loss  2.50 | ppl    12.13\n",
      "| epoch   9 |  1500/ 1963 batches | lr 4.61 | ms/batch  5.94 | loss  2.49 | ppl    12.05\n",
      "| epoch   9 |  1800/ 1963 batches | lr 4.61 | ms/batch  5.92 | loss  2.47 | ppl    11.78\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 11.49s | valid loss  2.53 | valid ppl    12.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  10 |   300/ 1963 batches | lr 4.57 | ms/batch  5.77 | loss  2.47 | ppl    11.79\n",
      "| epoch  10 |   600/ 1963 batches | lr 4.57 | ms/batch  5.77 | loss  2.48 | ppl    11.98\n",
      "| epoch  10 |   900/ 1963 batches | lr 4.57 | ms/batch  5.77 | loss  2.52 | ppl    12.39\n",
      "| epoch  10 |  1200/ 1963 batches | lr 4.57 | ms/batch  5.76 | loss  2.49 | ppl    12.10\n",
      "| epoch  10 |  1500/ 1963 batches | lr 4.57 | ms/batch  5.81 | loss  2.49 | ppl    12.08\n",
      "| epoch  10 |  1800/ 1963 batches | lr 4.57 | ms/batch  5.57 | loss  2.47 | ppl    11.77\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 11.44s | valid loss  2.52 | valid ppl    12.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  11 |   300/ 1963 batches | lr 4.52 | ms/batch  5.60 | loss  2.47 | ppl    11.78\n",
      "| epoch  11 |   600/ 1963 batches | lr 4.52 | ms/batch  5.58 | loss  2.48 | ppl    11.97\n",
      "| epoch  11 |   900/ 1963 batches | lr 4.52 | ms/batch  5.72 | loss  2.51 | ppl    12.35\n",
      "| epoch  11 |  1200/ 1963 batches | lr 4.52 | ms/batch  5.68 | loss  2.49 | ppl    12.08\n",
      "| epoch  11 |  1500/ 1963 batches | lr 4.52 | ms/batch  5.63 | loss  2.49 | ppl    12.00\n",
      "| epoch  11 |  1800/ 1963 batches | lr 4.52 | ms/batch  5.64 | loss  2.46 | ppl    11.73\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 11.29s | valid loss  2.54 | valid ppl    12.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   300/ 1963 batches | lr 4.48 | ms/batch  5.74 | loss  2.46 | ppl    11.76\n",
      "| epoch  12 |   600/ 1963 batches | lr 4.48 | ms/batch  5.76 | loss  2.48 | ppl    11.95\n",
      "| epoch  12 |   900/ 1963 batches | lr 4.48 | ms/batch  5.62 | loss  2.51 | ppl    12.33\n",
      "| epoch  12 |  1200/ 1963 batches | lr 4.48 | ms/batch  5.62 | loss  2.49 | ppl    12.09\n",
      "| epoch  12 |  1500/ 1963 batches | lr 4.48 | ms/batch  5.63 | loss  2.49 | ppl    12.01\n",
      "| epoch  12 |  1800/ 1963 batches | lr 4.48 | ms/batch  5.65 | loss  2.47 | ppl    11.82\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 11.33s | valid loss  2.53 | valid ppl    12.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   300/ 1963 batches | lr 4.43 | ms/batch  5.87 | loss  2.46 | ppl    11.74\n",
      "| epoch  13 |   600/ 1963 batches | lr 4.43 | ms/batch  6.06 | loss  2.48 | ppl    11.93\n",
      "| epoch  13 |   900/ 1963 batches | lr 4.43 | ms/batch  5.84 | loss  2.51 | ppl    12.32\n",
      "| epoch  13 |  1200/ 1963 batches | lr 4.43 | ms/batch  6.00 | loss  2.50 | ppl    12.13\n",
      "| epoch  13 |  1500/ 1963 batches | lr 4.43 | ms/batch  5.79 | loss  2.48 | ppl    11.94\n",
      "| epoch  13 |  1800/ 1963 batches | lr 4.43 | ms/batch  5.76 | loss  2.46 | ppl    11.72\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 11.77s | valid loss  2.53 | valid ppl    12.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   300/ 1963 batches | lr 4.39 | ms/batch  5.69 | loss  2.46 | ppl    11.70\n",
      "| epoch  14 |   600/ 1963 batches | lr 4.39 | ms/batch  5.67 | loss  2.48 | ppl    11.91\n",
      "| epoch  14 |   900/ 1963 batches | lr 4.39 | ms/batch  5.82 | loss  2.51 | ppl    12.30\n",
      "| epoch  14 |  1200/ 1963 batches | lr 4.39 | ms/batch  5.79 | loss  2.49 | ppl    12.02\n",
      "| epoch  14 |  1500/ 1963 batches | lr 4.39 | ms/batch  5.69 | loss  2.48 | ppl    11.93\n",
      "| epoch  14 |  1800/ 1963 batches | lr 4.39 | ms/batch  5.96 | loss  2.46 | ppl    11.73\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 11.59s | valid loss  2.52 | valid ppl    12.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  15 |   300/ 1963 batches | lr 4.34 | ms/batch  6.19 | loss  2.46 | ppl    11.70\n",
      "| epoch  15 |   600/ 1963 batches | lr 4.34 | ms/batch  6.24 | loss  2.48 | ppl    11.89\n",
      "| epoch  15 |   900/ 1963 batches | lr 4.34 | ms/batch  5.83 | loss  2.51 | ppl    12.29\n",
      "| epoch  15 |  1200/ 1963 batches | lr 4.34 | ms/batch  6.21 | loss  2.49 | ppl    12.01\n",
      "| epoch  15 |  1500/ 1963 batches | lr 4.34 | ms/batch  5.74 | loss  2.48 | ppl    11.96\n",
      "| epoch  15 |  1800/ 1963 batches | lr 4.34 | ms/batch  5.80 | loss  2.46 | ppl    11.69\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 11.95s | valid loss  2.52 | valid ppl    12.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   300/ 1963 batches | lr 4.30 | ms/batch  5.86 | loss  2.46 | ppl    11.69\n",
      "| epoch  16 |   600/ 1963 batches | lr 4.30 | ms/batch  5.85 | loss  2.48 | ppl    11.89\n",
      "| epoch  16 |   900/ 1963 batches | lr 4.30 | ms/batch  6.78 | loss  2.51 | ppl    12.26\n",
      "| epoch  16 |  1200/ 1963 batches | lr 4.30 | ms/batch  6.87 | loss  2.48 | ppl    11.99\n",
      "| epoch  16 |  1500/ 1963 batches | lr 4.30 | ms/batch 12.65 | loss  2.48 | ppl    11.90\n",
      "| epoch  16 |  1800/ 1963 batches | lr 4.30 | ms/batch  6.46 | loss  2.46 | ppl    11.69\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 15.29s | valid loss  2.51 | valid ppl    12.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  17 |   300/ 1963 batches | lr 4.26 | ms/batch  7.59 | loss  2.46 | ppl    11.67\n",
      "| epoch  17 |   600/ 1963 batches | lr 4.26 | ms/batch  5.67 | loss  2.48 | ppl    11.96\n",
      "| epoch  17 |   900/ 1963 batches | lr 4.26 | ms/batch  5.59 | loss  2.50 | ppl    12.23\n",
      "| epoch  17 |  1200/ 1963 batches | lr 4.26 | ms/batch  5.62 | loss  2.48 | ppl    11.99\n",
      "| epoch  17 |  1500/ 1963 batches | lr 4.26 | ms/batch  5.60 | loss  2.48 | ppl    11.88\n",
      "| epoch  17 |  1800/ 1963 batches | lr 4.26 | ms/batch  5.61 | loss  2.46 | ppl    11.65\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 12.51s | valid loss  2.51 | valid ppl    12.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  18 |   300/ 1963 batches | lr 4.21 | ms/batch  8.05 | loss  2.45 | ppl    11.64\n",
      "| epoch  18 |   600/ 1963 batches | lr 4.21 | ms/batch  5.66 | loss  2.47 | ppl    11.84\n",
      "| epoch  18 |   900/ 1963 batches | lr 4.21 | ms/batch  5.63 | loss  2.51 | ppl    12.25\n",
      "| epoch  18 |  1200/ 1963 batches | lr 4.21 | ms/batch  5.59 | loss  2.48 | ppl    11.96\n",
      "| epoch  18 |  1500/ 1963 batches | lr 4.21 | ms/batch  5.60 | loss  2.47 | ppl    11.86\n",
      "| epoch  18 |  1800/ 1963 batches | lr 4.21 | ms/batch  5.58 | loss  2.45 | ppl    11.64\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 11.97s | valid loss  2.51 | valid ppl    12.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   300/ 1963 batches | lr 4.17 | ms/batch  5.77 | loss  2.45 | ppl    11.63\n",
      "| epoch  19 |   600/ 1963 batches | lr 4.17 | ms/batch  5.69 | loss  2.47 | ppl    11.82\n",
      "| epoch  19 |   900/ 1963 batches | lr 4.17 | ms/batch  5.63 | loss  2.50 | ppl    12.23\n",
      "| epoch  19 |  1200/ 1963 batches | lr 4.17 | ms/batch  5.86 | loss  2.48 | ppl    11.95\n",
      "| epoch  19 |  1500/ 1963 batches | lr 4.17 | ms/batch  5.86 | loss  2.47 | ppl    11.84\n",
      "| epoch  19 |  1800/ 1963 batches | lr 4.17 | ms/batch  5.74 | loss  2.45 | ppl    11.62\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 11.50s | valid loss  2.51 | valid ppl    12.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   300/ 1963 batches | lr 4.13 | ms/batch  5.73 | loss  2.45 | ppl    11.63\n",
      "| epoch  20 |   600/ 1963 batches | lr 4.13 | ms/batch  5.67 | loss  2.47 | ppl    11.84\n",
      "| epoch  20 |   900/ 1963 batches | lr 4.13 | ms/batch  5.71 | loss  2.50 | ppl    12.20\n",
      "| epoch  20 |  1200/ 1963 batches | lr 4.13 | ms/batch  5.73 | loss  2.48 | ppl    11.93\n",
      "| epoch  20 |  1500/ 1963 batches | lr 4.13 | ms/batch  5.61 | loss  2.47 | ppl    11.85\n",
      "| epoch  20 |  1800/ 1963 batches | lr 4.13 | ms/batch  5.60 | loss  2.45 | ppl    11.63\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 11.37s | valid loss  2.50 | valid ppl    12.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  21 |   300/ 1963 batches | lr 4.09 | ms/batch  5.95 | loss  2.45 | ppl    11.61\n",
      "| epoch  21 |   600/ 1963 batches | lr 4.09 | ms/batch  5.63 | loss  2.47 | ppl    11.79\n",
      "| epoch  21 |   900/ 1963 batches | lr 4.09 | ms/batch  5.79 | loss  2.50 | ppl    12.21\n",
      "| epoch  21 |  1200/ 1963 batches | lr 4.09 | ms/batch  5.61 | loss  2.48 | ppl    11.94\n",
      "| epoch  21 |  1500/ 1963 batches | lr 4.09 | ms/batch  5.85 | loss  2.47 | ppl    11.83\n",
      "| epoch  21 |  1800/ 1963 batches | lr 4.09 | ms/batch  5.99 | loss  2.45 | ppl    11.63\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 11.63s | valid loss  2.51 | valid ppl    12.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   300/ 1963 batches | lr 4.05 | ms/batch  5.79 | loss  2.45 | ppl    11.58\n",
      "| epoch  22 |   600/ 1963 batches | lr 4.05 | ms/batch  5.74 | loss  2.47 | ppl    11.78\n",
      "| epoch  22 |   900/ 1963 batches | lr 4.05 | ms/batch  5.97 | loss  2.50 | ppl    12.16\n",
      "| epoch  22 |  1200/ 1963 batches | lr 4.05 | ms/batch  5.88 | loss  2.48 | ppl    11.91\n",
      "| epoch  22 |  1500/ 1963 batches | lr 4.05 | ms/batch  6.16 | loss  2.47 | ppl    11.82\n",
      "| epoch  22 |  1800/ 1963 batches | lr 4.05 | ms/batch  6.08 | loss  2.45 | ppl    11.59\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 11.94s | valid loss  2.51 | valid ppl    12.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   300/ 1963 batches | lr 4.01 | ms/batch  6.48 | loss  2.45 | ppl    11.59\n",
      "| epoch  23 |   600/ 1963 batches | lr 4.01 | ms/batch  5.99 | loss  2.47 | ppl    11.79\n",
      "| epoch  23 |   900/ 1963 batches | lr 4.01 | ms/batch  6.03 | loss  2.50 | ppl    12.20\n",
      "| epoch  23 |  1200/ 1963 batches | lr 4.01 | ms/batch  5.53 | loss  2.48 | ppl    11.91\n",
      "| epoch  23 |  1500/ 1963 batches | lr 4.01 | ms/batch  5.55 | loss  2.47 | ppl    11.79\n",
      "| epoch  23 |  1800/ 1963 batches | lr 4.01 | ms/batch  5.58 | loss  2.45 | ppl    11.58\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 11.69s | valid loss  2.50 | valid ppl    12.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   300/ 1963 batches | lr 3.97 | ms/batch  5.67 | loss  2.45 | ppl    11.58\n",
      "| epoch  24 |   600/ 1963 batches | lr 3.97 | ms/batch  5.73 | loss  2.46 | ppl    11.76\n",
      "| epoch  24 |   900/ 1963 batches | lr 3.97 | ms/batch  5.81 | loss  2.50 | ppl    12.13\n",
      "| epoch  24 |  1200/ 1963 batches | lr 3.97 | ms/batch  5.80 | loss  2.48 | ppl    11.89\n",
      "| epoch  24 |  1500/ 1963 batches | lr 3.97 | ms/batch  5.98 | loss  2.47 | ppl    11.80\n",
      "| epoch  24 |  1800/ 1963 batches | lr 3.97 | ms/batch  6.35 | loss  2.45 | ppl    11.55\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 11.83s | valid loss  2.50 | valid ppl    12.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  25 |   300/ 1963 batches | lr 3.93 | ms/batch  5.85 | loss  2.45 | ppl    11.56\n",
      "| epoch  25 |   600/ 1963 batches | lr 3.93 | ms/batch  5.74 | loss  2.47 | ppl    11.86\n",
      "| epoch  25 |   900/ 1963 batches | lr 3.93 | ms/batch  5.71 | loss  2.50 | ppl    12.13\n",
      "| epoch  25 |  1200/ 1963 batches | lr 3.93 | ms/batch  5.73 | loss  2.48 | ppl    11.89\n",
      "| epoch  25 |  1500/ 1963 batches | lr 3.93 | ms/batch  5.92 | loss  2.46 | ppl    11.76\n",
      "| epoch  25 |  1800/ 1963 batches | lr 3.93 | ms/batch  6.06 | loss  2.47 | ppl    11.77\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 11.71s | valid loss  2.51 | valid ppl    12.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   300/ 1963 batches | lr 3.89 | ms/batch 10.30 | loss  2.45 | ppl    11.56\n",
      "| epoch  26 |   600/ 1963 batches | lr 3.89 | ms/batch  5.68 | loss  2.46 | ppl    11.74\n",
      "| epoch  26 |   900/ 1963 batches | lr 3.89 | ms/batch  6.52 | loss  2.50 | ppl    12.14\n",
      "| epoch  26 |  1200/ 1963 batches | lr 3.89 | ms/batch  5.87 | loss  2.48 | ppl    11.88\n",
      "| epoch  26 |  1500/ 1963 batches | lr 3.89 | ms/batch  5.88 | loss  2.47 | ppl    11.76\n",
      "| epoch  26 |  1800/ 1963 batches | lr 3.89 | ms/batch  6.02 | loss  2.44 | ppl    11.53\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 13.25s | valid loss  2.50 | valid ppl    12.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   300/ 1963 batches | lr 3.85 | ms/batch  5.70 | loss  2.45 | ppl    11.55\n",
      "| epoch  27 |   600/ 1963 batches | lr 3.85 | ms/batch  5.90 | loss  2.46 | ppl    11.74\n",
      "| epoch  27 |   900/ 1963 batches | lr 3.85 | ms/batch  6.01 | loss  2.49 | ppl    12.11\n",
      "| epoch  27 |  1200/ 1963 batches | lr 3.85 | ms/batch  5.63 | loss  2.47 | ppl    11.86\n",
      "| epoch  27 |  1500/ 1963 batches | lr 3.85 | ms/batch  5.74 | loss  2.47 | ppl    11.77\n",
      "| epoch  27 |  1800/ 1963 batches | lr 3.85 | ms/batch  5.71 | loss  2.45 | ppl    11.53\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 11.56s | valid loss  2.51 | valid ppl    12.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   300/ 1963 batches | lr 3.81 | ms/batch  6.03 | loss  2.45 | ppl    11.53\n",
      "| epoch  28 |   600/ 1963 batches | lr 3.81 | ms/batch  5.93 | loss  2.46 | ppl    11.73\n",
      "| epoch  28 |   900/ 1963 batches | lr 3.81 | ms/batch  5.85 | loss  2.49 | ppl    12.10\n",
      "| epoch  28 |  1200/ 1963 batches | lr 3.81 | ms/batch  5.93 | loss  2.47 | ppl    11.86\n",
      "| epoch  28 |  1500/ 1963 batches | lr 3.81 | ms/batch  5.85 | loss  2.46 | ppl    11.75\n",
      "| epoch  28 |  1800/ 1963 batches | lr 3.81 | ms/batch  5.73 | loss  2.44 | ppl    11.51\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 11.75s | valid loss  2.50 | valid ppl    12.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   300/ 1963 batches | lr 3.77 | ms/batch  5.76 | loss  2.45 | ppl    11.53\n",
      "| epoch  29 |   600/ 1963 batches | lr 3.77 | ms/batch  5.64 | loss  2.46 | ppl    11.73\n",
      "| epoch  29 |   900/ 1963 batches | lr 3.77 | ms/batch  5.73 | loss  2.50 | ppl    12.20\n",
      "| epoch  29 |  1200/ 1963 batches | lr 3.77 | ms/batch  5.89 | loss  2.47 | ppl    11.84\n",
      "| epoch  29 |  1500/ 1963 batches | lr 3.77 | ms/batch  6.15 | loss  2.46 | ppl    11.74\n",
      "| epoch  29 |  1800/ 1963 batches | lr 3.77 | ms/batch  6.47 | loss  2.44 | ppl    11.51\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 11.91s | valid loss  2.50 | valid ppl    12.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  30 |   300/ 1963 batches | lr 3.74 | ms/batch  5.79 | loss  2.44 | ppl    11.51\n",
      "| epoch  30 |   600/ 1963 batches | lr 3.74 | ms/batch  5.79 | loss  2.46 | ppl    11.70\n",
      "| epoch  30 |   900/ 1963 batches | lr 3.74 | ms/batch  5.80 | loss  2.49 | ppl    12.06\n",
      "| epoch  30 |  1200/ 1963 batches | lr 3.74 | ms/batch  5.78 | loss  2.47 | ppl    11.83\n",
      "| epoch  30 |  1500/ 1963 batches | lr 3.74 | ms/batch  6.03 | loss  2.46 | ppl    11.74\n",
      "| epoch  30 |  1800/ 1963 batches | lr 3.74 | ms/batch  5.95 | loss  2.44 | ppl    11.49\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 11.73s | valid loss  2.50 | valid ppl    12.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   300/ 1963 batches | lr 3.70 | ms/batch  5.91 | loss  2.44 | ppl    11.51\n",
      "| epoch  31 |   600/ 1963 batches | lr 3.70 | ms/batch  6.03 | loss  2.46 | ppl    11.70\n",
      "| epoch  31 |   900/ 1963 batches | lr 3.70 | ms/batch  6.17 | loss  2.49 | ppl    12.06\n",
      "| epoch  31 |  1200/ 1963 batches | lr 3.70 | ms/batch  5.87 | loss  2.47 | ppl    11.80\n",
      "| epoch  31 |  1500/ 1963 batches | lr 3.70 | ms/batch  5.71 | loss  2.46 | ppl    11.71\n",
      "| epoch  31 |  1800/ 1963 batches | lr 3.70 | ms/batch  5.77 | loss  2.45 | ppl    11.53\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 11.87s | valid loss  2.50 | valid ppl    12.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  32 |   300/ 1963 batches | lr 3.66 | ms/batch  6.07 | loss  2.44 | ppl    11.50\n",
      "| epoch  32 |   600/ 1963 batches | lr 3.66 | ms/batch  6.11 | loss  2.46 | ppl    11.67\n",
      "| epoch  32 |   900/ 1963 batches | lr 3.66 | ms/batch  6.30 | loss  2.49 | ppl    12.09\n",
      "| epoch  32 |  1200/ 1963 batches | lr 3.66 | ms/batch  6.26 | loss  2.47 | ppl    11.82\n",
      "| epoch  32 |  1500/ 1963 batches | lr 3.66 | ms/batch  6.15 | loss  2.46 | ppl    11.72\n",
      "| epoch  32 |  1800/ 1963 batches | lr 3.66 | ms/batch  6.00 | loss  2.44 | ppl    11.47\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 12.27s | valid loss  2.50 | valid ppl    12.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   300/ 1963 batches | lr 3.62 | ms/batch  6.34 | loss  2.44 | ppl    11.50\n",
      "| epoch  33 |   600/ 1963 batches | lr 3.62 | ms/batch  5.98 | loss  2.47 | ppl    11.84\n",
      "| epoch  33 |   900/ 1963 batches | lr 3.62 | ms/batch  5.68 | loss  2.49 | ppl    12.04\n",
      "| epoch  33 |  1200/ 1963 batches | lr 3.62 | ms/batch  5.68 | loss  2.47 | ppl    11.80\n",
      "| epoch  33 |  1500/ 1963 batches | lr 3.62 | ms/batch  5.74 | loss  2.46 | ppl    11.70\n",
      "| epoch  33 |  1800/ 1963 batches | lr 3.62 | ms/batch  5.80 | loss  2.44 | ppl    11.47\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 11.75s | valid loss  2.50 | valid ppl    12.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   300/ 1963 batches | lr 3.59 | ms/batch  5.94 | loss  2.44 | ppl    11.47\n",
      "| epoch  34 |   600/ 1963 batches | lr 3.59 | ms/batch  5.80 | loss  2.46 | ppl    11.66\n",
      "| epoch  34 |   900/ 1963 batches | lr 3.59 | ms/batch  5.56 | loss  2.49 | ppl    12.05\n",
      "| epoch  34 |  1200/ 1963 batches | lr 3.59 | ms/batch  5.85 | loss  2.47 | ppl    11.78\n",
      "| epoch  34 |  1500/ 1963 batches | lr 3.59 | ms/batch  6.11 | loss  2.46 | ppl    11.70\n",
      "| epoch  34 |  1800/ 1963 batches | lr 3.59 | ms/batch  6.08 | loss  2.44 | ppl    11.46\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 11.80s | valid loss  2.50 | valid ppl    12.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |   300/ 1963 batches | lr 3.55 | ms/batch  5.82 | loss  2.44 | ppl    11.46\n",
      "| epoch  35 |   600/ 1963 batches | lr 3.55 | ms/batch  5.68 | loss  2.46 | ppl    11.65\n",
      "| epoch  35 |   900/ 1963 batches | lr 3.55 | ms/batch  5.70 | loss  2.49 | ppl    12.02\n",
      "| epoch  35 |  1200/ 1963 batches | lr 3.55 | ms/batch  5.69 | loss  2.47 | ppl    11.80\n",
      "| epoch  35 |  1500/ 1963 batches | lr 3.55 | ms/batch  5.83 | loss  2.46 | ppl    11.67\n",
      "| epoch  35 |  1800/ 1963 batches | lr 3.55 | ms/batch  5.95 | loss  2.44 | ppl    11.45\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 11.60s | valid loss  2.49 | valid ppl    12.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  36 |   300/ 1963 batches | lr 3.52 | ms/batch  5.93 | loss  2.44 | ppl    11.45\n",
      "| epoch  36 |   600/ 1963 batches | lr 3.52 | ms/batch  5.89 | loss  2.45 | ppl    11.64\n",
      "| epoch  36 |   900/ 1963 batches | lr 3.52 | ms/batch  6.22 | loss  2.49 | ppl    12.01\n",
      "| epoch  36 |  1200/ 1963 batches | lr 3.52 | ms/batch  6.31 | loss  2.47 | ppl    11.77\n",
      "| epoch  36 |  1500/ 1963 batches | lr 3.52 | ms/batch  5.74 | loss  2.46 | ppl    11.67\n",
      "| epoch  36 |  1800/ 1963 batches | lr 3.52 | ms/batch  5.98 | loss  2.44 | ppl    11.46\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 11.94s | valid loss  2.53 | valid ppl    12.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |   300/ 1963 batches | lr 3.48 | ms/batch  5.81 | loss  2.44 | ppl    11.45\n",
      "| epoch  37 |   600/ 1963 batches | lr 3.48 | ms/batch  5.81 | loss  2.45 | ppl    11.63\n",
      "| epoch  37 |   900/ 1963 batches | lr 3.48 | ms/batch  5.86 | loss  2.49 | ppl    12.02\n",
      "| epoch  37 |  1200/ 1963 batches | lr 3.48 | ms/batch  5.84 | loss  2.46 | ppl    11.76\n",
      "| epoch  37 |  1500/ 1963 batches | lr 3.48 | ms/batch  6.01 | loss  2.46 | ppl    11.72\n",
      "| epoch  37 |  1800/ 1963 batches | lr 3.48 | ms/batch  5.72 | loss  2.44 | ppl    11.43\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 11.67s | valid loss  2.50 | valid ppl    12.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |   300/ 1963 batches | lr 3.45 | ms/batch  5.83 | loss  2.44 | ppl    11.44\n",
      "| epoch  38 |   600/ 1963 batches | lr 3.45 | ms/batch  6.02 | loss  2.45 | ppl    11.62\n",
      "| epoch  38 |   900/ 1963 batches | lr 3.45 | ms/batch  5.97 | loss  2.48 | ppl    11.99\n",
      "| epoch  38 |  1200/ 1963 batches | lr 3.45 | ms/batch  5.84 | loss  2.46 | ppl    11.75\n",
      "| epoch  38 |  1500/ 1963 batches | lr 3.45 | ms/batch  5.82 | loss  2.45 | ppl    11.64\n",
      "| epoch  38 |  1800/ 1963 batches | lr 3.45 | ms/batch  5.86 | loss  2.44 | ppl    11.43\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 11.78s | valid loss  2.50 | valid ppl    12.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   300/ 1963 batches | lr 3.41 | ms/batch  5.82 | loss  2.44 | ppl    11.43\n",
      "| epoch  39 |   600/ 1963 batches | lr 3.41 | ms/batch  5.77 | loss  2.45 | ppl    11.61\n",
      "| epoch  39 |   900/ 1963 batches | lr 3.41 | ms/batch  5.87 | loss  2.48 | ppl    11.98\n",
      "| epoch  39 |  1200/ 1963 batches | lr 3.41 | ms/batch  5.77 | loss  2.46 | ppl    11.74\n",
      "| epoch  39 |  1500/ 1963 batches | lr 3.41 | ms/batch  5.81 | loss  2.45 | ppl    11.63\n",
      "| epoch  39 |  1800/ 1963 batches | lr 3.41 | ms/batch  6.03 | loss  2.44 | ppl    11.44\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 11.70s | valid loss  2.49 | valid ppl    12.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |   300/ 1963 batches | lr 3.38 | ms/batch  5.74 | loss  2.43 | ppl    11.41\n",
      "| epoch  40 |   600/ 1963 batches | lr 3.38 | ms/batch  5.55 | loss  2.45 | ppl    11.61\n",
      "| epoch  40 |   900/ 1963 batches | lr 3.38 | ms/batch  5.62 | loss  2.48 | ppl    11.97\n",
      "| epoch  40 |  1200/ 1963 batches | lr 3.38 | ms/batch  5.80 | loss  2.46 | ppl    11.72\n",
      "| epoch  40 |  1500/ 1963 batches | lr 3.38 | ms/batch  5.96 | loss  2.45 | ppl    11.63\n",
      "| epoch  40 |  1800/ 1963 batches | lr 3.38 | ms/batch  5.68 | loss  2.44 | ppl    11.42\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 11.45s | valid loss  2.50 | valid ppl    12.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |   300/ 1963 batches | lr 3.34 | ms/batch  5.94 | loss  2.43 | ppl    11.41\n",
      "| epoch  41 |   600/ 1963 batches | lr 3.34 | ms/batch  5.94 | loss  2.45 | ppl    11.61\n",
      "| epoch  41 |   900/ 1963 batches | lr 3.34 | ms/batch  6.25 | loss  2.48 | ppl    11.96\n",
      "| epoch  41 |  1200/ 1963 batches | lr 3.34 | ms/batch  6.47 | loss  2.46 | ppl    11.71\n",
      "| epoch  41 |  1500/ 1963 batches | lr 3.34 | ms/batch  6.22 | loss  2.45 | ppl    11.61\n",
      "| epoch  41 |  1800/ 1963 batches | lr 3.34 | ms/batch  5.70 | loss  2.44 | ppl    11.42\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 12.27s | valid loss  2.49 | valid ppl    12.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  42 |   300/ 1963 batches | lr 3.31 | ms/batch  5.77 | loss  2.43 | ppl    11.41\n",
      "| epoch  42 |   600/ 1963 batches | lr 3.31 | ms/batch  5.70 | loss  2.45 | ppl    11.59\n",
      "| epoch  42 |   900/ 1963 batches | lr 3.31 | ms/batch  6.02 | loss  2.48 | ppl    11.95\n",
      "| epoch  42 |  1200/ 1963 batches | lr 3.31 | ms/batch  6.20 | loss  2.46 | ppl    11.72\n",
      "| epoch  42 |  1500/ 1963 batches | lr 3.31 | ms/batch  5.63 | loss  2.45 | ppl    11.61\n",
      "| epoch  42 |  1800/ 1963 batches | lr 3.31 | ms/batch  5.47 | loss  2.43 | ppl    11.42\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 11.52s | valid loss  2.49 | valid ppl    12.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |   300/ 1963 batches | lr 3.28 | ms/batch  5.53 | loss  2.43 | ppl    11.40\n",
      "| epoch  43 |   600/ 1963 batches | lr 3.28 | ms/batch  5.55 | loss  2.45 | ppl    11.58\n",
      "| epoch  43 |   900/ 1963 batches | lr 3.28 | ms/batch  5.55 | loss  2.48 | ppl    11.94\n",
      "| epoch  43 |  1200/ 1963 batches | lr 3.28 | ms/batch  5.54 | loss  2.46 | ppl    11.69\n",
      "| epoch  43 |  1500/ 1963 batches | lr 3.28 | ms/batch  5.55 | loss  2.45 | ppl    11.61\n",
      "| epoch  43 |  1800/ 1963 batches | lr 3.28 | ms/batch  5.55 | loss  2.43 | ppl    11.40\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 11.08s | valid loss  2.49 | valid ppl    12.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |   300/ 1963 batches | lr 3.25 | ms/batch  5.55 | loss  2.43 | ppl    11.39\n",
      "| epoch  44 |   600/ 1963 batches | lr 3.25 | ms/batch  5.55 | loss  2.45 | ppl    11.58\n",
      "| epoch  44 |   900/ 1963 batches | lr 3.25 | ms/batch  5.54 | loss  2.48 | ppl    11.94\n",
      "| epoch  44 |  1200/ 1963 batches | lr 3.25 | ms/batch  5.55 | loss  2.46 | ppl    11.69\n",
      "| epoch  44 |  1500/ 1963 batches | lr 3.25 | ms/batch  5.55 | loss  2.45 | ppl    11.58\n",
      "| epoch  44 |  1800/ 1963 batches | lr 3.25 | ms/batch  5.54 | loss  2.43 | ppl    11.38\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 11.08s | valid loss  2.49 | valid ppl    12.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  45 |   300/ 1963 batches | lr 3.21 | ms/batch  5.56 | loss  2.43 | ppl    11.39\n",
      "| epoch  45 |   600/ 1963 batches | lr 3.21 | ms/batch  5.55 | loss  2.45 | ppl    11.56\n",
      "| epoch  45 |   900/ 1963 batches | lr 3.21 | ms/batch  5.54 | loss  2.48 | ppl    11.94\n",
      "| epoch  45 |  1200/ 1963 batches | lr 3.21 | ms/batch  5.55 | loss  2.46 | ppl    11.68\n",
      "| epoch  45 |  1500/ 1963 batches | lr 3.21 | ms/batch  5.55 | loss  2.45 | ppl    11.58\n",
      "| epoch  45 |  1800/ 1963 batches | lr 3.21 | ms/batch  5.54 | loss  2.43 | ppl    11.38\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 11.09s | valid loss  2.49 | valid ppl    12.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |   300/ 1963 batches | lr 3.18 | ms/batch  5.54 | loss  2.43 | ppl    11.38\n",
      "| epoch  46 |   600/ 1963 batches | lr 3.18 | ms/batch  5.54 | loss  2.45 | ppl    11.55\n",
      "| epoch  46 |   900/ 1963 batches | lr 3.18 | ms/batch  5.56 | loss  2.48 | ppl    11.92\n",
      "| epoch  46 |  1200/ 1963 batches | lr 3.18 | ms/batch  5.55 | loss  2.46 | ppl    11.67\n",
      "| epoch  46 |  1500/ 1963 batches | lr 3.18 | ms/batch  5.55 | loss  2.45 | ppl    11.57\n",
      "| epoch  46 |  1800/ 1963 batches | lr 3.18 | ms/batch  5.54 | loss  2.43 | ppl    11.37\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 11.09s | valid loss  2.48 | valid ppl    11.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  47 |   300/ 1963 batches | lr 3.15 | ms/batch  5.56 | loss  2.43 | ppl    11.37\n",
      "| epoch  47 |   600/ 1963 batches | lr 3.15 | ms/batch  5.55 | loss  2.45 | ppl    11.55\n",
      "| epoch  47 |   900/ 1963 batches | lr 3.15 | ms/batch  5.55 | loss  2.48 | ppl    11.93\n",
      "| epoch  47 |  1200/ 1963 batches | lr 3.15 | ms/batch  5.54 | loss  2.46 | ppl    11.67\n",
      "| epoch  47 |  1500/ 1963 batches | lr 3.15 | ms/batch  5.55 | loss  2.45 | ppl    11.58\n",
      "| epoch  47 |  1800/ 1963 batches | lr 3.15 | ms/batch  5.56 | loss  2.43 | ppl    11.36\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 11.09s | valid loss  2.49 | valid ppl    12.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |   300/ 1963 batches | lr 3.12 | ms/batch  5.55 | loss  2.43 | ppl    11.36\n",
      "| epoch  48 |   600/ 1963 batches | lr 3.12 | ms/batch  5.53 | loss  2.45 | ppl    11.57\n",
      "| epoch  48 |   900/ 1963 batches | lr 3.12 | ms/batch  5.53 | loss  2.48 | ppl    11.92\n",
      "| epoch  48 |  1200/ 1963 batches | lr 3.12 | ms/batch  5.54 | loss  2.46 | ppl    11.66\n",
      "| epoch  48 |  1500/ 1963 batches | lr 3.12 | ms/batch  5.55 | loss  2.45 | ppl    11.58\n",
      "| epoch  48 |  1800/ 1963 batches | lr 3.12 | ms/batch  5.54 | loss  2.43 | ppl    11.35\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 11.07s | valid loss  2.50 | valid ppl    12.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |   300/ 1963 batches | lr 3.09 | ms/batch  5.55 | loss  2.43 | ppl    11.36\n",
      "| epoch  49 |   600/ 1963 batches | lr 3.09 | ms/batch  5.55 | loss  2.45 | ppl    11.53\n",
      "| epoch  49 |   900/ 1963 batches | lr 3.09 | ms/batch  5.55 | loss  2.48 | ppl    11.90\n",
      "| epoch  49 |  1200/ 1963 batches | lr 3.09 | ms/batch  5.55 | loss  2.46 | ppl    11.66\n",
      "| epoch  49 |  1500/ 1963 batches | lr 3.09 | ms/batch  5.54 | loss  2.45 | ppl    11.56\n",
      "| epoch  49 |  1800/ 1963 batches | lr 3.09 | ms/batch  5.54 | loss  2.43 | ppl    11.35\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 11.08s | valid loss  2.49 | valid ppl    12.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |   300/ 1963 batches | lr 3.06 | ms/batch  5.56 | loss  2.43 | ppl    11.35\n",
      "| epoch  50 |   600/ 1963 batches | lr 3.06 | ms/batch  5.54 | loss  2.44 | ppl    11.52\n",
      "| epoch  50 |   900/ 1963 batches | lr 3.06 | ms/batch  5.55 | loss  2.48 | ppl    11.90\n",
      "| epoch  50 |  1200/ 1963 batches | lr 3.06 | ms/batch  5.55 | loss  2.46 | ppl    11.65\n",
      "| epoch  50 |  1500/ 1963 batches | lr 3.06 | ms/batch  5.55 | loss  2.45 | ppl    11.55\n",
      "| epoch  50 |  1800/ 1963 batches | lr 3.06 | ms/batch  5.54 | loss  2.43 | ppl    11.34\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 11.09s | valid loss  2.49 | valid ppl    12.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |   300/ 1963 batches | lr 3.03 | ms/batch  5.56 | loss  2.43 | ppl    11.35\n",
      "| epoch  51 |   600/ 1963 batches | lr 3.03 | ms/batch  5.55 | loss  2.44 | ppl    11.52\n",
      "| epoch  51 |   900/ 1963 batches | lr 3.03 | ms/batch  5.55 | loss  2.48 | ppl    11.90\n",
      "| epoch  51 |  1200/ 1963 batches | lr 3.03 | ms/batch  5.55 | loss  2.45 | ppl    11.64\n",
      "| epoch  51 |  1500/ 1963 batches | lr 3.03 | ms/batch  5.54 | loss  2.45 | ppl    11.54\n",
      "| epoch  51 |  1800/ 1963 batches | lr 3.03 | ms/batch  5.54 | loss  2.43 | ppl    11.34\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 11.09s | valid loss  2.49 | valid ppl    12.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |   300/ 1963 batches | lr 2.99 | ms/batch  5.57 | loss  2.43 | ppl    11.34\n",
      "| epoch  52 |   600/ 1963 batches | lr 2.99 | ms/batch  5.56 | loss  2.44 | ppl    11.50\n",
      "| epoch  52 |   900/ 1963 batches | lr 2.99 | ms/batch  5.70 | loss  2.48 | ppl    11.89\n",
      "| epoch  52 |  1200/ 1963 batches | lr 2.99 | ms/batch  5.95 | loss  2.45 | ppl    11.63\n",
      "| epoch  52 |  1500/ 1963 batches | lr 2.99 | ms/batch  5.77 | loss  2.45 | ppl    11.53\n",
      "| epoch  52 |  1800/ 1963 batches | lr 2.99 | ms/batch  5.73 | loss  2.43 | ppl    11.34\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 11.47s | valid loss  2.49 | valid ppl    12.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |   300/ 1963 batches | lr 2.96 | ms/batch  5.67 | loss  2.43 | ppl    11.33\n",
      "| epoch  53 |   600/ 1963 batches | lr 2.96 | ms/batch  6.01 | loss  2.44 | ppl    11.49\n",
      "| epoch  53 |   900/ 1963 batches | lr 2.96 | ms/batch  5.93 | loss  2.47 | ppl    11.87\n",
      "| epoch  53 |  1200/ 1963 batches | lr 2.96 | ms/batch  5.91 | loss  2.45 | ppl    11.63\n",
      "| epoch  53 |  1500/ 1963 batches | lr 2.96 | ms/batch  5.77 | loss  2.44 | ppl    11.53\n",
      "| epoch  53 |  1800/ 1963 batches | lr 2.96 | ms/batch  5.72 | loss  2.43 | ppl    11.33\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 11.74s | valid loss  2.48 | valid ppl    11.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  54 |   300/ 1963 batches | lr 2.94 | ms/batch  6.33 | loss  2.43 | ppl    11.33\n",
      "| epoch  54 |   600/ 1963 batches | lr 2.94 | ms/batch  5.93 | loss  2.44 | ppl    11.50\n",
      "| epoch  54 |   900/ 1963 batches | lr 2.94 | ms/batch  5.95 | loss  2.47 | ppl    11.87\n",
      "| epoch  54 |  1200/ 1963 batches | lr 2.94 | ms/batch  6.11 | loss  2.45 | ppl    11.63\n",
      "| epoch  54 |  1500/ 1963 batches | lr 2.94 | ms/batch  6.29 | loss  2.44 | ppl    11.52\n",
      "| epoch  54 |  1800/ 1963 batches | lr 2.94 | ms/batch  6.79 | loss  2.43 | ppl    11.32\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 12.51s | valid loss  2.49 | valid ppl    12.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |   300/ 1963 batches | lr 2.91 | ms/batch  6.65 | loss  2.43 | ppl    11.32\n",
      "| epoch  55 |   600/ 1963 batches | lr 2.91 | ms/batch  6.57 | loss  2.44 | ppl    11.48\n",
      "| epoch  55 |   900/ 1963 batches | lr 2.91 | ms/batch  6.42 | loss  2.47 | ppl    11.87\n",
      "| epoch  55 |  1200/ 1963 batches | lr 2.91 | ms/batch  5.97 | loss  2.45 | ppl    11.61\n",
      "| epoch  55 |  1500/ 1963 batches | lr 2.91 | ms/batch  6.54 | loss  2.44 | ppl    11.53\n",
      "| epoch  55 |  1800/ 1963 batches | lr 2.91 | ms/batch  6.81 | loss  2.43 | ppl    11.31\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 13.07s | valid loss  2.48 | valid ppl    11.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |   300/ 1963 batches | lr 2.88 | ms/batch  6.79 | loss  2.43 | ppl    11.31\n",
      "| epoch  56 |   600/ 1963 batches | lr 2.88 | ms/batch  6.47 | loss  2.44 | ppl    11.48\n",
      "| epoch  56 |   900/ 1963 batches | lr 2.88 | ms/batch  6.47 | loss  2.47 | ppl    11.87\n",
      "| epoch  56 |  1200/ 1963 batches | lr 2.88 | ms/batch  6.30 | loss  2.45 | ppl    11.61\n",
      "| epoch  56 |  1500/ 1963 batches | lr 2.88 | ms/batch  6.40 | loss  2.44 | ppl    11.51\n",
      "| epoch  56 |  1800/ 1963 batches | lr 2.88 | ms/batch  6.95 | loss  2.43 | ppl    11.32\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 13.19s | valid loss  2.48 | valid ppl    11.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  57 |   300/ 1963 batches | lr 2.85 | ms/batch  7.00 | loss  2.43 | ppl    11.31\n",
      "| epoch  57 |   600/ 1963 batches | lr 2.85 | ms/batch  6.85 | loss  2.44 | ppl    11.47\n",
      "| epoch  57 |   900/ 1963 batches | lr 2.85 | ms/batch  6.80 | loss  2.47 | ppl    11.85\n",
      "| epoch  57 |  1200/ 1963 batches | lr 2.85 | ms/batch  6.58 | loss  2.45 | ppl    11.62\n",
      "| epoch  57 |  1500/ 1963 batches | lr 2.85 | ms/batch  6.65 | loss  2.44 | ppl    11.51\n",
      "| epoch  57 |  1800/ 1963 batches | lr 2.85 | ms/batch  6.08 | loss  2.43 | ppl    11.31\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 13.30s | valid loss  2.48 | valid ppl    11.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |   300/ 1963 batches | lr 2.82 | ms/batch  6.55 | loss  2.42 | ppl    11.30\n",
      "| epoch  58 |   600/ 1963 batches | lr 2.82 | ms/batch  6.66 | loss  2.44 | ppl    11.48\n",
      "| epoch  58 |   900/ 1963 batches | lr 2.82 | ms/batch  6.26 | loss  2.47 | ppl    11.83\n",
      "| epoch  58 |  1200/ 1963 batches | lr 2.82 | ms/batch  6.49 | loss  2.45 | ppl    11.61\n",
      "| epoch  58 |  1500/ 1963 batches | lr 2.82 | ms/batch  6.97 | loss  2.44 | ppl    11.51\n",
      "| epoch  58 |  1800/ 1963 batches | lr 2.82 | ms/batch  6.34 | loss  2.42 | ppl    11.29\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 13.07s | valid loss  2.48 | valid ppl    11.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  59 |   300/ 1963 batches | lr 2.79 | ms/batch  6.46 | loss  2.43 | ppl    11.30\n",
      "| epoch  59 |   600/ 1963 batches | lr 2.79 | ms/batch  6.34 | loss  2.44 | ppl    11.45\n",
      "| epoch  59 |   900/ 1963 batches | lr 2.79 | ms/batch  6.21 | loss  2.47 | ppl    11.83\n",
      "| epoch  59 |  1200/ 1963 batches | lr 2.79 | ms/batch  6.38 | loss  2.45 | ppl    11.60\n",
      "| epoch  59 |  1500/ 1963 batches | lr 2.79 | ms/batch  5.95 | loss  2.44 | ppl    11.49\n",
      "| epoch  59 |  1800/ 1963 batches | lr 2.79 | ms/batch  6.06 | loss  2.42 | ppl    11.29\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 12.53s | valid loss  2.48 | valid ppl    11.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |   300/ 1963 batches | lr 2.76 | ms/batch  6.64 | loss  2.42 | ppl    11.28\n",
      "| epoch  60 |   600/ 1963 batches | lr 2.76 | ms/batch  5.95 | loss  2.44 | ppl    11.46\n",
      "| epoch  60 |   900/ 1963 batches | lr 2.76 | ms/batch  6.14 | loss  2.47 | ppl    11.84\n",
      "| epoch  60 |  1200/ 1963 batches | lr 2.76 | ms/batch  6.36 | loss  2.45 | ppl    11.59\n",
      "| epoch  60 |  1500/ 1963 batches | lr 2.76 | ms/batch  6.16 | loss  2.44 | ppl    11.49\n",
      "| epoch  60 |  1800/ 1963 batches | lr 2.76 | ms/batch  6.03 | loss  2.42 | ppl    11.28\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 12.48s | valid loss  2.48 | valid ppl    11.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |   300/ 1963 batches | lr 2.74 | ms/batch  6.37 | loss  2.42 | ppl    11.28\n",
      "| epoch  61 |   600/ 1963 batches | lr 2.74 | ms/batch  6.19 | loss  2.44 | ppl    11.45\n",
      "| epoch  61 |   900/ 1963 batches | lr 2.74 | ms/batch  6.33 | loss  2.47 | ppl    11.82\n",
      "| epoch  61 |  1200/ 1963 batches | lr 2.74 | ms/batch  6.39 | loss  2.45 | ppl    11.58\n",
      "| epoch  61 |  1500/ 1963 batches | lr 2.74 | ms/batch  6.09 | loss  2.44 | ppl    11.50\n",
      "| epoch  61 |  1800/ 1963 batches | lr 2.74 | ms/batch  6.55 | loss  2.42 | ppl    11.29\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 12.62s | valid loss  2.48 | valid ppl    11.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |   300/ 1963 batches | lr 2.71 | ms/batch  5.79 | loss  2.42 | ppl    11.29\n",
      "| epoch  62 |   600/ 1963 batches | lr 2.71 | ms/batch  6.31 | loss  2.44 | ppl    11.45\n",
      "| epoch  62 |   900/ 1963 batches | lr 2.71 | ms/batch  6.40 | loss  2.47 | ppl    11.81\n",
      "| epoch  62 |  1200/ 1963 batches | lr 2.71 | ms/batch  6.15 | loss  2.45 | ppl    11.59\n",
      "| epoch  62 |  1500/ 1963 batches | lr 2.71 | ms/batch  5.66 | loss  2.44 | ppl    11.48\n",
      "| epoch  62 |  1800/ 1963 batches | lr 2.71 | ms/batch  5.89 | loss  2.42 | ppl    11.28\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 12.16s | valid loss  2.48 | valid ppl    11.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |   300/ 1963 batches | lr 2.68 | ms/batch  6.52 | loss  2.42 | ppl    11.28\n",
      "| epoch  63 |   600/ 1963 batches | lr 2.68 | ms/batch  6.06 | loss  2.44 | ppl    11.45\n",
      "| epoch  63 |   900/ 1963 batches | lr 2.68 | ms/batch  6.63 | loss  2.47 | ppl    11.82\n",
      "| epoch  63 |  1200/ 1963 batches | lr 2.68 | ms/batch  6.74 | loss  2.45 | ppl    11.58\n",
      "| epoch  63 |  1500/ 1963 batches | lr 2.68 | ms/batch  6.58 | loss  2.44 | ppl    11.49\n",
      "| epoch  63 |  1800/ 1963 batches | lr 2.68 | ms/batch  6.71 | loss  2.42 | ppl    11.27\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 13.08s | valid loss  2.48 | valid ppl    11.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |   300/ 1963 batches | lr 2.65 | ms/batch  5.99 | loss  2.42 | ppl    11.29\n",
      "| epoch  64 |   600/ 1963 batches | lr 2.65 | ms/batch  6.25 | loss  2.44 | ppl    11.44\n",
      "| epoch  64 |   900/ 1963 batches | lr 2.65 | ms/batch  6.22 | loss  2.47 | ppl    11.81\n",
      "| epoch  64 |  1200/ 1963 batches | lr 2.65 | ms/batch  6.29 | loss  2.45 | ppl    11.59\n",
      "| epoch  64 |  1500/ 1963 batches | lr 2.65 | ms/batch  6.11 | loss  2.44 | ppl    11.48\n",
      "| epoch  64 |  1800/ 1963 batches | lr 2.65 | ms/batch  5.95 | loss  2.42 | ppl    11.28\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 12.35s | valid loss  2.47 | valid ppl    11.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  65 |   300/ 1963 batches | lr 2.63 | ms/batch  5.94 | loss  2.42 | ppl    11.27\n",
      "| epoch  65 |   600/ 1963 batches | lr 2.63 | ms/batch  6.18 | loss  2.44 | ppl    11.43\n",
      "| epoch  65 |   900/ 1963 batches | lr 2.63 | ms/batch  6.77 | loss  2.47 | ppl    11.80\n",
      "| epoch  65 |  1200/ 1963 batches | lr 2.63 | ms/batch  6.32 | loss  2.45 | ppl    11.59\n",
      "| epoch  65 |  1500/ 1963 batches | lr 2.63 | ms/batch  6.39 | loss  2.44 | ppl    11.48\n",
      "| epoch  65 |  1800/ 1963 batches | lr 2.63 | ms/batch  6.44 | loss  2.42 | ppl    11.27\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 12.70s | valid loss  2.48 | valid ppl    11.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |   300/ 1963 batches | lr 2.60 | ms/batch  6.35 | loss  2.42 | ppl    11.27\n",
      "| epoch  66 |   600/ 1963 batches | lr 2.60 | ms/batch  6.19 | loss  2.44 | ppl    11.42\n",
      "| epoch  66 |   900/ 1963 batches | lr 2.60 | ms/batch  6.49 | loss  2.47 | ppl    11.80\n",
      "| epoch  66 |  1200/ 1963 batches | lr 2.60 | ms/batch  6.91 | loss  2.45 | ppl    11.57\n",
      "| epoch  66 |  1500/ 1963 batches | lr 2.60 | ms/batch  6.93 | loss  2.44 | ppl    11.48\n",
      "| epoch  66 |  1800/ 1963 batches | lr 2.60 | ms/batch  7.07 | loss  2.42 | ppl    11.26\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 13.52s | valid loss  2.48 | valid ppl    11.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |   300/ 1963 batches | lr 2.58 | ms/batch  6.74 | loss  2.42 | ppl    11.26\n",
      "| epoch  67 |   600/ 1963 batches | lr 2.58 | ms/batch  6.76 | loss  2.43 | ppl    11.42\n",
      "| epoch  67 |   900/ 1963 batches | lr 2.58 | ms/batch  6.41 | loss  2.47 | ppl    11.79\n",
      "| epoch  67 |  1200/ 1963 batches | lr 2.58 | ms/batch  5.76 | loss  2.45 | ppl    11.57\n",
      "| epoch  67 |  1500/ 1963 batches | lr 2.58 | ms/batch  6.88 | loss  2.44 | ppl    11.47\n",
      "| epoch  67 |  1800/ 1963 batches | lr 2.58 | ms/batch  6.82 | loss  2.42 | ppl    11.25\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 13.18s | valid loss  2.48 | valid ppl    11.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |   300/ 1963 batches | lr 2.55 | ms/batch  6.97 | loss  2.42 | ppl    11.26\n",
      "| epoch  68 |   600/ 1963 batches | lr 2.55 | ms/batch  6.70 | loss  2.44 | ppl    11.42\n",
      "| epoch  68 |   900/ 1963 batches | lr 2.55 | ms/batch  6.27 | loss  2.47 | ppl    11.79\n",
      "| epoch  68 |  1200/ 1963 batches | lr 2.55 | ms/batch  6.51 | loss  2.45 | ppl    11.56\n",
      "| epoch  68 |  1500/ 1963 batches | lr 2.55 | ms/batch  6.53 | loss  2.44 | ppl    11.45\n",
      "| epoch  68 |  1800/ 1963 batches | lr 2.55 | ms/batch  6.53 | loss  2.42 | ppl    11.26\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 12.95s | valid loss  2.51 | valid ppl    12.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |   300/ 1963 batches | lr 2.52 | ms/batch  5.50 | loss  2.42 | ppl    11.25\n",
      "| epoch  69 |   600/ 1963 batches | lr 2.52 | ms/batch  5.53 | loss  2.43 | ppl    11.41\n",
      "| epoch  69 |   900/ 1963 batches | lr 2.52 | ms/batch  5.53 | loss  2.47 | ppl    11.79\n",
      "| epoch  69 |  1200/ 1963 batches | lr 2.52 | ms/batch  5.54 | loss  2.45 | ppl    11.55\n",
      "| epoch  69 |  1500/ 1963 batches | lr 2.52 | ms/batch  5.54 | loss  2.44 | ppl    11.45\n",
      "| epoch  69 |  1800/ 1963 batches | lr 2.52 | ms/batch  5.55 | loss  2.42 | ppl    11.24\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 11.06s | valid loss  2.48 | valid ppl    11.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |   300/ 1963 batches | lr 2.50 | ms/batch  5.58 | loss  2.42 | ppl    11.24\n",
      "| epoch  70 |   600/ 1963 batches | lr 2.50 | ms/batch  5.56 | loss  2.44 | ppl    11.42\n",
      "| epoch  70 |   900/ 1963 batches | lr 2.50 | ms/batch  5.55 | loss  2.47 | ppl    11.78\n",
      "| epoch  70 |  1200/ 1963 batches | lr 2.50 | ms/batch  5.53 | loss  2.45 | ppl    11.53\n",
      "| epoch  70 |  1500/ 1963 batches | lr 2.50 | ms/batch  5.56 | loss  2.44 | ppl    11.44\n",
      "| epoch  70 |  1800/ 1963 batches | lr 2.50 | ms/batch  5.55 | loss  2.42 | ppl    11.25\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 11.10s | valid loss  2.48 | valid ppl    11.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 |   300/ 1963 batches | lr 2.47 | ms/batch  5.59 | loss  2.42 | ppl    11.25\n",
      "| epoch  71 |   600/ 1963 batches | lr 2.47 | ms/batch  5.60 | loss  2.43 | ppl    11.41\n",
      "| epoch  71 |   900/ 1963 batches | lr 2.47 | ms/batch  5.61 | loss  2.47 | ppl    11.80\n",
      "| epoch  71 |  1200/ 1963 batches | lr 2.47 | ms/batch  5.69 | loss  2.45 | ppl    11.54\n",
      "| epoch  71 |  1500/ 1963 batches | lr 2.47 | ms/batch  5.63 | loss  2.44 | ppl    11.45\n",
      "| epoch  71 |  1800/ 1963 batches | lr 2.47 | ms/batch  5.63 | loss  2.42 | ppl    11.24\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 11.24s | valid loss  2.47 | valid ppl    11.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  72 |   300/ 1963 batches | lr 2.45 | ms/batch  5.60 | loss  2.42 | ppl    11.24\n",
      "| epoch  72 |   600/ 1963 batches | lr 2.45 | ms/batch  5.55 | loss  2.43 | ppl    11.41\n",
      "| epoch  72 |   900/ 1963 batches | lr 2.45 | ms/batch  5.55 | loss  2.47 | ppl    11.77\n",
      "| epoch  72 |  1200/ 1963 batches | lr 2.45 | ms/batch  5.53 | loss  2.45 | ppl    11.54\n",
      "| epoch  72 |  1500/ 1963 batches | lr 2.45 | ms/batch  5.53 | loss  2.44 | ppl    11.44\n",
      "| epoch  72 |  1800/ 1963 batches | lr 2.45 | ms/batch  5.54 | loss  2.42 | ppl    11.23\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 11.09s | valid loss  2.47 | valid ppl    11.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  73 |   300/ 1963 batches | lr 2.42 | ms/batch  5.57 | loss  2.42 | ppl    11.24\n",
      "| epoch  73 |   600/ 1963 batches | lr 2.42 | ms/batch  5.61 | loss  2.43 | ppl    11.40\n",
      "| epoch  73 |   900/ 1963 batches | lr 2.42 | ms/batch  5.54 | loss  2.47 | ppl    11.77\n",
      "| epoch  73 |  1200/ 1963 batches | lr 2.42 | ms/batch  5.55 | loss  2.45 | ppl    11.55\n",
      "| epoch  73 |  1500/ 1963 batches | lr 2.42 | ms/batch  5.56 | loss  2.44 | ppl    11.43\n",
      "| epoch  73 |  1800/ 1963 batches | lr 2.42 | ms/batch  5.57 | loss  2.42 | ppl    11.24\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 11.13s | valid loss  2.48 | valid ppl    11.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 |   300/ 1963 batches | lr 2.40 | ms/batch  5.58 | loss  2.42 | ppl    11.23\n",
      "| epoch  74 |   600/ 1963 batches | lr 2.40 | ms/batch  5.56 | loss  2.43 | ppl    11.39\n",
      "| epoch  74 |   900/ 1963 batches | lr 2.40 | ms/batch  5.56 | loss  2.46 | ppl    11.76\n",
      "| epoch  74 |  1200/ 1963 batches | lr 2.40 | ms/batch  5.56 | loss  2.45 | ppl    11.54\n",
      "| epoch  74 |  1500/ 1963 batches | lr 2.40 | ms/batch  5.57 | loss  2.44 | ppl    11.43\n",
      "| epoch  74 |  1800/ 1963 batches | lr 2.40 | ms/batch  5.55 | loss  2.42 | ppl    11.23\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 11.14s | valid loss  2.48 | valid ppl    11.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 |   300/ 1963 batches | lr 2.38 | ms/batch  5.62 | loss  2.42 | ppl    11.22\n",
      "| epoch  75 |   600/ 1963 batches | lr 2.38 | ms/batch  5.59 | loss  2.43 | ppl    11.39\n",
      "| epoch  75 |   900/ 1963 batches | lr 2.38 | ms/batch  5.55 | loss  2.46 | ppl    11.76\n",
      "| epoch  75 |  1200/ 1963 batches | lr 2.38 | ms/batch  5.58 | loss  2.44 | ppl    11.53\n",
      "| epoch  75 |  1500/ 1963 batches | lr 2.38 | ms/batch  5.55 | loss  2.44 | ppl    11.42\n",
      "| epoch  75 |  1800/ 1963 batches | lr 2.38 | ms/batch  5.54 | loss  2.42 | ppl    11.22\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 11.14s | valid loss  2.48 | valid ppl    11.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 |   300/ 1963 batches | lr 2.35 | ms/batch  5.60 | loss  2.42 | ppl    11.23\n",
      "| epoch  76 |   600/ 1963 batches | lr 2.35 | ms/batch  5.56 | loss  2.43 | ppl    11.39\n",
      "| epoch  76 |   900/ 1963 batches | lr 2.35 | ms/batch  5.54 | loss  2.46 | ppl    11.75\n",
      "| epoch  76 |  1200/ 1963 batches | lr 2.35 | ms/batch  5.55 | loss  2.45 | ppl    11.54\n",
      "| epoch  76 |  1500/ 1963 batches | lr 2.35 | ms/batch  5.54 | loss  2.44 | ppl    11.44\n",
      "| epoch  76 |  1800/ 1963 batches | lr 2.35 | ms/batch  5.55 | loss  2.42 | ppl    11.24\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 11.11s | valid loss  2.47 | valid ppl    11.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 |   300/ 1963 batches | lr 2.33 | ms/batch  5.58 | loss  2.42 | ppl    11.23\n",
      "| epoch  77 |   600/ 1963 batches | lr 2.33 | ms/batch  5.61 | loss  2.43 | ppl    11.38\n",
      "| epoch  77 |   900/ 1963 batches | lr 2.33 | ms/batch  5.60 | loss  2.46 | ppl    11.76\n",
      "| epoch  77 |  1200/ 1963 batches | lr 2.33 | ms/batch  5.57 | loss  2.44 | ppl    11.52\n",
      "| epoch  77 |  1500/ 1963 batches | lr 2.33 | ms/batch  5.56 | loss  2.44 | ppl    11.42\n",
      "| epoch  77 |  1800/ 1963 batches | lr 2.33 | ms/batch  5.55 | loss  2.42 | ppl    11.22\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 11.15s | valid loss  2.48 | valid ppl    11.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 |   300/ 1963 batches | lr 2.31 | ms/batch  5.54 | loss  2.42 | ppl    11.23\n",
      "| epoch  78 |   600/ 1963 batches | lr 2.31 | ms/batch  5.56 | loss  2.43 | ppl    11.38\n",
      "| epoch  78 |   900/ 1963 batches | lr 2.31 | ms/batch  5.55 | loss  2.46 | ppl    11.75\n",
      "| epoch  78 |  1200/ 1963 batches | lr 2.31 | ms/batch  5.55 | loss  2.45 | ppl    11.53\n",
      "| epoch  78 |  1500/ 1963 batches | lr 2.31 | ms/batch  5.53 | loss  2.44 | ppl    11.42\n",
      "| epoch  78 |  1800/ 1963 batches | lr 2.31 | ms/batch  5.47 | loss  2.42 | ppl    11.22\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 11.05s | valid loss  2.48 | valid ppl    11.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 |   300/ 1963 batches | lr 2.28 | ms/batch  5.56 | loss  2.42 | ppl    11.22\n",
      "| epoch  79 |   600/ 1963 batches | lr 2.28 | ms/batch  5.57 | loss  2.43 | ppl    11.37\n",
      "| epoch  79 |   900/ 1963 batches | lr 2.28 | ms/batch  5.57 | loss  2.46 | ppl    11.74\n",
      "| epoch  79 |  1200/ 1963 batches | lr 2.28 | ms/batch  5.58 | loss  2.44 | ppl    11.50\n",
      "| epoch  79 |  1500/ 1963 batches | lr 2.28 | ms/batch  5.57 | loss  2.44 | ppl    11.42\n",
      "| epoch  79 |  1800/ 1963 batches | lr 2.28 | ms/batch  5.57 | loss  2.42 | ppl    11.21\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 11.13s | valid loss  2.48 | valid ppl    11.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 |   300/ 1963 batches | lr 2.26 | ms/batch  5.56 | loss  2.42 | ppl    11.21\n",
      "| epoch  80 |   600/ 1963 batches | lr 2.26 | ms/batch  5.57 | loss  2.43 | ppl    11.37\n",
      "| epoch  80 |   900/ 1963 batches | lr 2.26 | ms/batch  5.56 | loss  2.46 | ppl    11.74\n",
      "| epoch  80 |  1200/ 1963 batches | lr 2.26 | ms/batch  5.55 | loss  2.44 | ppl    11.52\n",
      "| epoch  80 |  1500/ 1963 batches | lr 2.26 | ms/batch  5.56 | loss  2.44 | ppl    11.42\n",
      "| epoch  80 |  1800/ 1963 batches | lr 2.26 | ms/batch  5.56 | loss  2.42 | ppl    11.22\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 11.11s | valid loss  2.48 | valid ppl    11.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 |   300/ 1963 batches | lr 2.24 | ms/batch  5.56 | loss  2.42 | ppl    11.21\n",
      "| epoch  81 |   600/ 1963 batches | lr 2.24 | ms/batch  5.55 | loss  2.43 | ppl    11.38\n",
      "| epoch  81 |   900/ 1963 batches | lr 2.24 | ms/batch  5.55 | loss  2.46 | ppl    11.73\n",
      "| epoch  81 |  1200/ 1963 batches | lr 2.24 | ms/batch  5.55 | loss  2.44 | ppl    11.51\n",
      "| epoch  81 |  1500/ 1963 batches | lr 2.24 | ms/batch  5.56 | loss  2.43 | ppl    11.41\n",
      "| epoch  81 |  1800/ 1963 batches | lr 2.24 | ms/batch  5.54 | loss  2.42 | ppl    11.21\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 11.10s | valid loss  2.48 | valid ppl    11.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 |   300/ 1963 batches | lr 2.22 | ms/batch  5.56 | loss  2.42 | ppl    11.21\n",
      "| epoch  82 |   600/ 1963 batches | lr 2.22 | ms/batch  5.55 | loss  2.43 | ppl    11.37\n",
      "| epoch  82 |   900/ 1963 batches | lr 2.22 | ms/batch  5.56 | loss  2.46 | ppl    11.73\n",
      "| epoch  82 |  1200/ 1963 batches | lr 2.22 | ms/batch  5.55 | loss  2.44 | ppl    11.51\n",
      "| epoch  82 |  1500/ 1963 batches | lr 2.22 | ms/batch  5.55 | loss  2.43 | ppl    11.41\n",
      "| epoch  82 |  1800/ 1963 batches | lr 2.22 | ms/batch  5.72 | loss  2.42 | ppl    11.20\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 11.27s | valid loss  2.47 | valid ppl    11.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 |   300/ 1963 batches | lr 2.19 | ms/batch  5.61 | loss  2.42 | ppl    11.20\n",
      "| epoch  83 |   600/ 1963 batches | lr 2.19 | ms/batch  5.55 | loss  2.43 | ppl    11.38\n",
      "| epoch  83 |   900/ 1963 batches | lr 2.19 | ms/batch  5.56 | loss  2.46 | ppl    11.72\n",
      "| epoch  83 |  1200/ 1963 batches | lr 2.19 | ms/batch  5.54 | loss  2.44 | ppl    11.51\n",
      "| epoch  83 |  1500/ 1963 batches | lr 2.19 | ms/batch  5.56 | loss  2.43 | ppl    11.40\n",
      "| epoch  83 |  1800/ 1963 batches | lr 2.19 | ms/batch  5.57 | loss  2.42 | ppl    11.20\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 11.12s | valid loss  2.48 | valid ppl    11.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 |   300/ 1963 batches | lr 2.17 | ms/batch  5.56 | loss  2.42 | ppl    11.20\n",
      "| epoch  84 |   600/ 1963 batches | lr 2.17 | ms/batch  5.55 | loss  2.43 | ppl    11.37\n",
      "| epoch  84 |   900/ 1963 batches | lr 2.17 | ms/batch  5.55 | loss  2.46 | ppl    11.72\n",
      "| epoch  84 |  1200/ 1963 batches | lr 2.17 | ms/batch  5.55 | loss  2.44 | ppl    11.50\n",
      "| epoch  84 |  1500/ 1963 batches | lr 2.17 | ms/batch  5.55 | loss  2.43 | ppl    11.40\n",
      "| epoch  84 |  1800/ 1963 batches | lr 2.17 | ms/batch  5.56 | loss  2.42 | ppl    11.20\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 11.11s | valid loss  2.48 | valid ppl    11.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 |   300/ 1963 batches | lr 2.15 | ms/batch  5.57 | loss  2.42 | ppl    11.20\n",
      "| epoch  85 |   600/ 1963 batches | lr 2.15 | ms/batch  5.56 | loss  2.43 | ppl    11.37\n",
      "| epoch  85 |   900/ 1963 batches | lr 2.15 | ms/batch  5.56 | loss  2.46 | ppl    11.72\n",
      "| epoch  85 |  1200/ 1963 batches | lr 2.15 | ms/batch  5.56 | loss  2.44 | ppl    11.49\n",
      "| epoch  85 |  1500/ 1963 batches | lr 2.15 | ms/batch  5.56 | loss  2.43 | ppl    11.40\n",
      "| epoch  85 |  1800/ 1963 batches | lr 2.15 | ms/batch  5.56 | loss  2.42 | ppl    11.19\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 11.11s | valid loss  2.48 | valid ppl    11.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 |   300/ 1963 batches | lr 2.13 | ms/batch  5.58 | loss  2.42 | ppl    11.20\n",
      "| epoch  86 |   600/ 1963 batches | lr 2.13 | ms/batch  5.56 | loss  2.43 | ppl    11.36\n",
      "| epoch  86 |   900/ 1963 batches | lr 2.13 | ms/batch  5.56 | loss  2.46 | ppl    11.72\n",
      "| epoch  86 |  1200/ 1963 batches | lr 2.13 | ms/batch  5.54 | loss  2.44 | ppl    11.50\n",
      "| epoch  86 |  1500/ 1963 batches | lr 2.13 | ms/batch  5.55 | loss  2.43 | ppl    11.39\n",
      "| epoch  86 |  1800/ 1963 batches | lr 2.13 | ms/batch  5.55 | loss  2.42 | ppl    11.20\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 11.10s | valid loss  2.48 | valid ppl    11.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 |   300/ 1963 batches | lr 2.11 | ms/batch  5.58 | loss  2.42 | ppl    11.20\n",
      "| epoch  87 |   600/ 1963 batches | lr 2.11 | ms/batch  5.56 | loss  2.43 | ppl    11.36\n",
      "| epoch  87 |   900/ 1963 batches | lr 2.11 | ms/batch  5.56 | loss  2.46 | ppl    11.71\n",
      "| epoch  87 |  1200/ 1963 batches | lr 2.11 | ms/batch  5.57 | loss  2.44 | ppl    11.49\n",
      "| epoch  87 |  1500/ 1963 batches | lr 2.11 | ms/batch  5.57 | loss  2.43 | ppl    11.39\n",
      "| epoch  87 |  1800/ 1963 batches | lr 2.11 | ms/batch  5.57 | loss  2.41 | ppl    11.19\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 11.12s | valid loss  2.48 | valid ppl    11.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 |   300/ 1963 batches | lr 2.09 | ms/batch  5.58 | loss  2.42 | ppl    11.20\n",
      "| epoch  88 |   600/ 1963 batches | lr 2.09 | ms/batch  5.57 | loss  2.43 | ppl    11.35\n",
      "| epoch  88 |   900/ 1963 batches | lr 2.09 | ms/batch  5.57 | loss  2.46 | ppl    11.70\n",
      "| epoch  88 |  1200/ 1963 batches | lr 2.09 | ms/batch  5.57 | loss  2.44 | ppl    11.49\n",
      "| epoch  88 |  1500/ 1963 batches | lr 2.09 | ms/batch  5.57 | loss  2.43 | ppl    11.39\n",
      "| epoch  88 |  1800/ 1963 batches | lr 2.09 | ms/batch  5.57 | loss  2.41 | ppl    11.19\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 11.13s | valid loss  2.47 | valid ppl    11.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 |   300/ 1963 batches | lr 2.06 | ms/batch  5.58 | loss  2.42 | ppl    11.20\n",
      "| epoch  89 |   600/ 1963 batches | lr 2.06 | ms/batch  5.57 | loss  2.43 | ppl    11.35\n",
      "| epoch  89 |   900/ 1963 batches | lr 2.06 | ms/batch  5.58 | loss  2.46 | ppl    11.71\n",
      "| epoch  89 |  1200/ 1963 batches | lr 2.06 | ms/batch  5.58 | loss  2.44 | ppl    11.48\n",
      "| epoch  89 |  1500/ 1963 batches | lr 2.06 | ms/batch  5.62 | loss  2.43 | ppl    11.39\n",
      "| epoch  89 |  1800/ 1963 batches | lr 2.06 | ms/batch  5.62 | loss  2.41 | ppl    11.19\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 11.18s | valid loss  2.48 | valid ppl    11.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 |   300/ 1963 batches | lr 2.04 | ms/batch  5.63 | loss  2.41 | ppl    11.19\n",
      "| epoch  90 |   600/ 1963 batches | lr 2.04 | ms/batch  5.61 | loss  2.43 | ppl    11.36\n",
      "| epoch  90 |   900/ 1963 batches | lr 2.04 | ms/batch  5.59 | loss  2.46 | ppl    11.70\n",
      "| epoch  90 |  1200/ 1963 batches | lr 2.04 | ms/batch  5.56 | loss  2.44 | ppl    11.49\n",
      "| epoch  90 |  1500/ 1963 batches | lr 2.04 | ms/batch  5.56 | loss  2.43 | ppl    11.40\n",
      "| epoch  90 |  1800/ 1963 batches | lr 2.04 | ms/batch  5.56 | loss  2.41 | ppl    11.18\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 11.15s | valid loss  2.48 | valid ppl    11.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 |   300/ 1963 batches | lr 2.02 | ms/batch  5.57 | loss  2.42 | ppl    11.19\n",
      "| epoch  91 |   600/ 1963 batches | lr 2.02 | ms/batch  5.56 | loss  2.43 | ppl    11.35\n",
      "| epoch  91 |   900/ 1963 batches | lr 2.02 | ms/batch  5.56 | loss  2.46 | ppl    11.70\n",
      "| epoch  91 |  1200/ 1963 batches | lr 2.02 | ms/batch  5.56 | loss  2.44 | ppl    11.48\n",
      "| epoch  91 |  1500/ 1963 batches | lr 2.02 | ms/batch  5.94 | loss  2.43 | ppl    11.39\n",
      "| epoch  91 |  1800/ 1963 batches | lr 2.02 | ms/batch  5.56 | loss  2.41 | ppl    11.18\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 11.31s | valid loss  2.48 | valid ppl    11.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 |   300/ 1963 batches | lr 2.00 | ms/batch  5.58 | loss  2.41 | ppl    11.19\n",
      "| epoch  92 |   600/ 1963 batches | lr 2.00 | ms/batch  5.57 | loss  2.43 | ppl    11.35\n",
      "| epoch  92 |   900/ 1963 batches | lr 2.00 | ms/batch  5.57 | loss  2.46 | ppl    11.70\n",
      "| epoch  92 |  1200/ 1963 batches | lr 2.00 | ms/batch  5.58 | loss  2.44 | ppl    11.49\n",
      "| epoch  92 |  1500/ 1963 batches | lr 2.00 | ms/batch  5.57 | loss  2.43 | ppl    11.39\n",
      "| epoch  92 |  1800/ 1963 batches | lr 2.00 | ms/batch  5.56 | loss  2.41 | ppl    11.19\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 11.13s | valid loss  2.48 | valid ppl    11.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 |   300/ 1963 batches | lr 1.98 | ms/batch  5.58 | loss  2.41 | ppl    11.19\n",
      "| epoch  93 |   600/ 1963 batches | lr 1.98 | ms/batch  5.56 | loss  2.43 | ppl    11.34\n",
      "| epoch  93 |   900/ 1963 batches | lr 1.98 | ms/batch  5.56 | loss  2.46 | ppl    11.70\n",
      "| epoch  93 |  1200/ 1963 batches | lr 1.98 | ms/batch  5.57 | loss  2.44 | ppl    11.46\n",
      "| epoch  93 |  1500/ 1963 batches | lr 1.98 | ms/batch  5.82 | loss  2.43 | ppl    11.39\n",
      "| epoch  93 |  1800/ 1963 batches | lr 1.98 | ms/batch  5.56 | loss  2.41 | ppl    11.18\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 11.21s | valid loss  2.48 | valid ppl    11.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 |   300/ 1963 batches | lr 1.96 | ms/batch  5.57 | loss  2.41 | ppl    11.18\n",
      "| epoch  94 |   600/ 1963 batches | lr 1.96 | ms/batch  5.56 | loss  2.43 | ppl    11.34\n",
      "| epoch  94 |   900/ 1963 batches | lr 1.96 | ms/batch  5.55 | loss  2.46 | ppl    11.70\n",
      "| epoch  94 |  1200/ 1963 batches | lr 1.96 | ms/batch  5.56 | loss  2.44 | ppl    11.47\n",
      "| epoch  94 |  1500/ 1963 batches | lr 1.96 | ms/batch  5.56 | loss  2.43 | ppl    11.39\n",
      "| epoch  94 |  1800/ 1963 batches | lr 1.96 | ms/batch  5.55 | loss  2.41 | ppl    11.17\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 11.11s | valid loss  2.47 | valid ppl    11.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  95 |   300/ 1963 batches | lr 1.94 | ms/batch  5.57 | loss  2.41 | ppl    11.17\n",
      "| epoch  95 |   600/ 1963 batches | lr 1.94 | ms/batch  5.57 | loss  2.43 | ppl    11.33\n",
      "| epoch  95 |   900/ 1963 batches | lr 1.94 | ms/batch  5.57 | loss  2.46 | ppl    11.69\n",
      "| epoch  95 |  1200/ 1963 batches | lr 1.94 | ms/batch  5.54 | loss  2.44 | ppl    11.47\n",
      "| epoch  95 |  1500/ 1963 batches | lr 1.94 | ms/batch  5.55 | loss  2.43 | ppl    11.38\n",
      "| epoch  95 |  1800/ 1963 batches | lr 1.94 | ms/batch  5.57 | loss  2.41 | ppl    11.17\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 11.13s | valid loss  2.47 | valid ppl    11.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 |   300/ 1963 batches | lr 1.92 | ms/batch  5.64 | loss  2.41 | ppl    11.18\n",
      "| epoch  96 |   600/ 1963 batches | lr 1.92 | ms/batch  5.64 | loss  2.43 | ppl    11.33\n",
      "| epoch  96 |   900/ 1963 batches | lr 1.92 | ms/batch  5.77 | loss  2.46 | ppl    11.69\n",
      "| epoch  96 |  1200/ 1963 batches | lr 1.92 | ms/batch  5.98 | loss  2.44 | ppl    11.47\n",
      "| epoch  96 |  1500/ 1963 batches | lr 1.92 | ms/batch  5.87 | loss  2.43 | ppl    11.39\n",
      "| epoch  96 |  1800/ 1963 batches | lr 1.92 | ms/batch  5.70 | loss  2.41 | ppl    11.18\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 11.53s | valid loss  2.48 | valid ppl    11.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  97 |   300/ 1963 batches | lr 1.91 | ms/batch  5.83 | loss  2.41 | ppl    11.18\n",
      "| epoch  97 |   600/ 1963 batches | lr 1.91 | ms/batch  5.64 | loss  2.43 | ppl    11.34\n",
      "| epoch  97 |   900/ 1963 batches | lr 1.91 | ms/batch  5.78 | loss  2.46 | ppl    11.69\n",
      "| epoch  97 |  1200/ 1963 batches | lr 1.91 | ms/batch  6.05 | loss  2.44 | ppl    11.47\n",
      "| epoch  97 |  1500/ 1963 batches | lr 1.91 | ms/batch  5.91 | loss  2.43 | ppl    11.39\n",
      "| epoch  97 |  1800/ 1963 batches | lr 1.91 | ms/batch  5.95 | loss  2.41 | ppl    11.19\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 11.75s | valid loss  2.47 | valid ppl    11.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  98 |   300/ 1963 batches | lr 1.89 | ms/batch  5.76 | loss  2.41 | ppl    11.18\n",
      "| epoch  98 |   600/ 1963 batches | lr 1.89 | ms/batch  5.62 | loss  2.43 | ppl    11.34\n",
      "| epoch  98 |   900/ 1963 batches | lr 1.89 | ms/batch  5.63 | loss  2.46 | ppl    11.68\n",
      "| epoch  98 |  1200/ 1963 batches | lr 1.89 | ms/batch  5.66 | loss  2.44 | ppl    11.46\n",
      "| epoch  98 |  1500/ 1963 batches | lr 1.89 | ms/batch  5.73 | loss  2.43 | ppl    11.39\n",
      "| epoch  98 |  1800/ 1963 batches | lr 1.89 | ms/batch  5.65 | loss  2.41 | ppl    11.17\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 11.36s | valid loss  2.47 | valid ppl    11.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 |   300/ 1963 batches | lr 1.87 | ms/batch  5.97 | loss  2.41 | ppl    11.18\n",
      "| epoch  99 |   600/ 1963 batches | lr 1.87 | ms/batch  6.21 | loss  2.43 | ppl    11.34\n",
      "| epoch  99 |   900/ 1963 batches | lr 1.87 | ms/batch  5.84 | loss  2.46 | ppl    11.69\n",
      "| epoch  99 |  1200/ 1963 batches | lr 1.87 | ms/batch  5.74 | loss  2.44 | ppl    11.46\n",
      "| epoch  99 |  1500/ 1963 batches | lr 1.87 | ms/batch  5.77 | loss  2.43 | ppl    11.38\n",
      "| epoch  99 |  1800/ 1963 batches | lr 1.87 | ms/batch  5.73 | loss  2.41 | ppl    11.17\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 11.85s | valid loss  2.47 | valid ppl    11.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 100 |   300/ 1963 batches | lr 1.85 | ms/batch  6.12 | loss  2.41 | ppl    11.18\n",
      "| epoch 100 |   600/ 1963 batches | lr 1.85 | ms/batch  5.75 | loss  2.43 | ppl    11.34\n",
      "| epoch 100 |   900/ 1963 batches | lr 1.85 | ms/batch  5.72 | loss  2.46 | ppl    11.68\n",
      "| epoch 100 |  1200/ 1963 batches | lr 1.85 | ms/batch  5.67 | loss  2.44 | ppl    11.46\n",
      "| epoch 100 |  1500/ 1963 batches | lr 1.85 | ms/batch  5.63 | loss  2.43 | ppl    11.37\n",
      "| epoch 100 |  1800/ 1963 batches | lr 1.85 | ms/batch  5.60 | loss  2.41 | ppl    11.17\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 11.50s | valid loss  2.47 | valid ppl    11.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "Reloading the overall best model...\n",
      "\n",
      "Saving the overall best model to permanent location...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()  # loss function\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 10\n",
    "\n",
    "# Load the previously saved best model checkpoint if it exists\n",
    "final_model_params_path = os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\\n\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "    best_val_loss = evaluate(model)\n",
    "    print(f\"Previous validation loss: \\t {best_val_loss:5.2f}\\n\")\n",
    "\n",
    "# Train the model\n",
    "if epochs > 0:\n",
    "    print(f\"Training for {epochs} epoch(s)...\\n\")\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            train(model)\n",
    "            print(f\"TRAIN done.\\n\")\n",
    "            val_loss = evaluate(model)\n",
    "            print(f\"EVAL done.\\n\")\n",
    "            val_ppl = math.exp(val_loss)\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            print(\"-\" * 89)\n",
    "            print(\n",
    "                f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "                f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "            )\n",
    "            print(\"-\" * 89)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                print(f\"Saving the newest best model...\\n\")\n",
    "                torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        if os.path.exists(best_model_params_path):\n",
    "            print(f\"Reloading the overall best model...\\n\")\n",
    "            model.load_state_dict(torch.load(best_model_params_path))  # reload the final best model\n",
    "            print(f\"Saving the overall best model to permanent location...\\n\")\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                final_model_params_path,\n",
    "            )  # save the final best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 510]) torch.Size([1, 610])\n",
      "\n",
      "rance ta'en As shall with either part's agreement stand? BAPTISTA: Not in my house, Lucentio; for, you know, Pitchers have ears, and I have many servants: Besides, old Gremio is hearkening still; And happily we might be interrupted. TRANIO: Then at my lodging, an it like you: There doth my father lie; and there, this night, We'll pass the business privately and well. Send for your daughter by your servant here: My boy shall fetch the scrivener presently. The worst is this, that, at so slender warning, You\n",
      "\n",
      "n g Yoothind: s d Wiid, a</s></s> Hend llds ienosefeld--be Pe age tom Bu io y h whe; THAMI y d l s r SAn g\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Generate new text\n",
    "\n",
    "max_new_tokens = 100\n",
    "idx = test_dataset[0][:-1].unsqueeze(0).to(DEVICE)\n",
    "idx_gen = model.generate(idx, max_new_tokens, top_k=None)\n",
    "\n",
    "print(idx.shape, idx_gen.shape, end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx.tolist()[0]), end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx_gen.tolist()[0][-max_new_tokens:]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  torch.Size([1, 511, 302]) torch.float32 False cpu\n",
      "\n",
      "mask: torch.Size([1, 302]) torch.bool False cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create neural datasets\n",
    "# @markdown A synthetic dataset where the neural activity is the embeddings of tokens from the tiny Shakespeare.\n",
    "\n",
    "init_random_seeds()  # set random seeds\n",
    "\n",
    "ntokens = tokenizer.vocab_size\n",
    "emsize = 302\n",
    "d_hid = 512\n",
    "embedding = torch.nn.Embedding(ntokens, emsize, _freeze=True)  # fixed embedding map\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"train\"][\"input_ids\"]\n",
    "]\n",
    "validation_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"validation\"][\"input_ids\"]\n",
    "]\n",
    "test_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"test\"][\"input_ids\"]\n",
    "]\n",
    "\n",
    "# get a test sample for an example\n",
    "data = test_dataset[0].unsqueeze(0)\n",
    "print(\"data: \", data.shape, data.dtype, data.requires_grad, data.device, end=\"\\n\\n\")\n",
    "\n",
    "mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "print(\"mask:\", mask.shape, mask.dtype, mask.requires_grad, mask.device, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (i.e. num tokens) = 256\n",
      "Model internal tokens = 256\n",
      "Number of attn heads = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Instantiate a NeuralTransformer model\n",
    "\n",
    "model = NeuralTransformer(\n",
    "    input_size=emsize,\n",
    "    hidden_size=d_hid,\n",
    "    version_2=True,\n",
    "    num_tokens=ntokens,\n",
    "    multi_channel=False,\n",
    "    vq_vae=False,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Vocab size (i.e. num tokens) = {ntokens}\")\n",
    "print(f\"Model internal tokens = {model.num_tokens}\")\n",
    "print(f\"Number of attn heads = {model.num_heads}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> torch.Size([511, 302]) torch.Size([511]) torch.Size([511])\n",
      "tensor([117, 100, 113, 102, 104]) tensor([117, 100, 113, 102, 104])\n",
      "\n",
      "<class 'list'> torch.Size([511, 302]) torch.Size([511]) torch.Size([511])\n",
      "tensor([117, 100, 113, 102, 104]) tensor([168, 137, 114, 110,  39])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Test that our NeuralTransformer model's internal tokenizer works as expected\n",
    "\n",
    "# The code below should only work when we cheat and set the codebook to be exactky the embedding table that was used to generate the neural data.\n",
    "# This is because the codebook is initialized randomly. If the model has not been trained then there is no reason for its internal tokenizer to\n",
    "# correctly invert the ground-truth embedding, which should be unknown to us. Thereforem the goal of our optimization is ultimately to learn the\n",
    "# a codebook that is as close as possible to the ground-truth but unknown embedding map.\n",
    "\n",
    "assert not torch.allclose(embedding.weight, model.codebook.cpu())\n",
    "\n",
    "# Replace model codebook with the embedding map use to generate the dataset\n",
    "tmp = model.codebook  # save for later restoration\n",
    "model.codebook = torch.nn.Parameter(embedding.weight.to(DEVICE))  # let's cheat\n",
    "\n",
    "assert torch.allclose(embedding.weight, model.codebook.cpu())\n",
    "\n",
    "# Get some ground-truth test data\n",
    "token_list = text_dataset[\"test\"][\"input_ids\"][0]\n",
    "token_target = torch.LongTensor(token_list)\n",
    "neural_target = torch.vstack([embedding(t) for t in token_target])\n",
    "\n",
    "# Compare the tokenized and retokenized sequences when the codebook is the true embedding map\n",
    "with torch.no_grad():  # model tokenizer takes in a neural sequence and outputs a token sequence\n",
    "    retokenized_target = model.tokenize_neural_data(neural_target.unsqueeze(0)).squeeze(0)\n",
    "print(type(token_list), neural_target.shape, token_target.shape, retokenized_target.shape)\n",
    "print(token_target[:5], retokenized_target[:5], end=\"\\n\\n\")\n",
    "\n",
    "# The tokenized and retokenized sequences should be the same\n",
    "assert torch.allclose(\n",
    "    token_target, retokenized_target\n",
    "), \"The tokenized and retokenized sequences should be the same!\"\n",
    "\n",
    "# Restore the model codebook to its original random initialization\n",
    "model.codebook = tmp\n",
    "\n",
    "assert not torch.allclose(embedding.weight, model.codebook.cpu())\n",
    "\n",
    "# Compare the tokenized and retokenized sequences when the codebook is NOT the true embedding map\n",
    "with torch.no_grad():  # model tokenizer takes in a neural sequence and outputs a token sequence\n",
    "    retokenized_target = model.tokenize_neural_data(neural_target.unsqueeze(0)).squeeze(0)\n",
    "print(type(token_list), neural_target.shape, token_target.shape, retokenized_target.shape)\n",
    "print(token_target[:5], retokenized_target[:5], end=\"\\n\\n\")\n",
    "\n",
    "# The tokenized and retokenized sequences should NOT be the same\n",
    "assert not torch.allclose(\n",
    "    token_target, retokenized_target\n",
    "), \"The tokenized and retokenized sequences should NOT be the same!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "True None None\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "\n",
    "print(len(model.token_neural_map.unique(dim=0)) - 1)\n",
    "print(model.codebook.requires_grad, model.codebook.grad, model.codebook.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 510, 302]) torch.float32 False cuda:0\n",
      "\n",
      "target: torch.Size([1, 510, 302]) torch.float32 False cuda:0\n",
      "\n",
      "output: torch.Size([1, 510, 256]) torch.float16 False cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Test input-output functionality\n",
    "\n",
    "mask = mask.to(DEVICE)\n",
    "input = data[:, :-1, :].to(DEVICE)\n",
    "target = data[:, 1:, :].to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    output = model(input, mask)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"input:\",\n",
    "    input.shape,\n",
    "    input.dtype,\n",
    "    input.requires_grad,\n",
    "    input.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"target:\",\n",
    "    target.shape,\n",
    "    target.dtype,\n",
    "    target.requires_grad,\n",
    "    target.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"output:\",\n",
    "    output.shape,\n",
    "    output.dtype,\n",
    "    output.requires_grad,\n",
    "    output.device,\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the NeuralTransformer model\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch, optimizer, scheduler\n",
    "\n",
    "    num_batches = len(train_dataset)\n",
    "    for batch in range(num_batches):\n",
    "        data = train_dataset[batch].unsqueeze(0)\n",
    "        mask = mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "        # parse into input and target\n",
    "        mask = mask.to(DEVICE)\n",
    "        input = data[:, :-1, :].to(DEVICE)\n",
    "        target = data[:, 1:, :].to(DEVICE)\n",
    "        # forward pass\n",
    "        output = model(input, mask)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        # backpropagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss_fn()(\n",
    "            output, target, mask\n",
    "        )  # flattens output to ``[batch_size=1 * seq_len, ntokens]``\n",
    "\n",
    "        # check if the computed loss requires gradient\n",
    "        if loss.requires_grad:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            try:\n",
    "                ppl = math.exp(cur_loss)\n",
    "            except OverflowError:\n",
    "                ppl = float(\"inf\")\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(validation_dataset)\n",
    "        for batch in range(num_batches):\n",
    "            data = validation_dataset[batch].unsqueeze(0)\n",
    "            mask = mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "            mask = mask.to(DEVICE)\n",
    "            input = data[:, :-1, :].to(DEVICE)\n",
    "            target = data[:, 1:, :].to(DEVICE)\n",
    "            output = model(input, mask)\n",
    "            loss = model.loss_fn()(output, target, mask)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "True None None\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "\n",
    "print(len(model.token_neural_map.unique(dim=0)) - 1)\n",
    "print(model.codebook.requires_grad, model.codebook.grad, model.codebook.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE load torch.Size([1, 302])\n",
      "\n",
      "Training for 100 epoch(s)...\n",
      "\n",
      "| epoch   1 |   300/ 1963 batches | lr 5.00 | ms/batch  6.20 | loss 80.54 | ppl 95544085049837903354724616710914048.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   600/ 1963 batches | lr 5.00 | ms/batch  6.04 | loss 26.81 | ppl 439032176211.30\n",
      "| epoch   1 |   900/ 1963 batches | lr 5.00 | ms/batch  6.00 | loss 17.76 | ppl 51844012.05\n",
      "| epoch   1 |  1200/ 1963 batches | lr 5.00 | ms/batch  5.97 | loss 13.90 | ppl 1084455.46\n",
      "| epoch   1 |  1500/ 1963 batches | lr 5.00 | ms/batch  6.02 | loss 12.95 | ppl 421999.93\n",
      "| epoch   1 |  1800/ 1963 batches | lr 5.00 | ms/batch  5.96 | loss 10.40 | ppl 32888.73\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.08s | valid loss  9.32 | valid ppl 11119.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch   2 |   300/ 1963 batches | lr 4.95 | ms/batch  6.00 | loss  8.50 | ppl  4912.79\n",
      "| epoch   2 |   600/ 1963 batches | lr 4.95 | ms/batch  5.97 | loss  7.93 | ppl  2772.45\n",
      "| epoch   2 |   900/ 1963 batches | lr 4.95 | ms/batch  8.71 | loss  7.40 | ppl  1631.62\n",
      "| epoch   2 |  1200/ 1963 batches | lr 4.95 | ms/batch  5.99 | loss  7.18 | ppl  1313.40\n",
      "| epoch   2 |  1500/ 1963 batches | lr 4.95 | ms/batch  6.03 | loss  6.49 | ppl   661.01\n",
      "| epoch   2 |  1800/ 1963 batches | lr 4.95 | ms/batch  6.02 | loss  6.67 | ppl   788.81\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 12.86s | valid loss  6.41 | valid ppl   609.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch   3 |   300/ 1963 batches | lr 4.90 | ms/batch  6.24 | loss  6.17 | ppl   478.22\n",
      "| epoch   3 |   600/ 1963 batches | lr 4.90 | ms/batch  6.08 | loss  5.97 | ppl   392.35\n",
      "| epoch   3 |   900/ 1963 batches | lr 4.90 | ms/batch  6.04 | loss  6.14 | ppl   464.27\n",
      "| epoch   3 |  1200/ 1963 batches | lr 4.90 | ms/batch  6.04 | loss  5.99 | ppl   399.17\n",
      "| epoch   3 |  1500/ 1963 batches | lr 4.90 | ms/batch  6.06 | loss  5.92 | ppl   370.69\n",
      "| epoch   3 |  1800/ 1963 batches | lr 4.90 | ms/batch  6.10 | loss  5.75 | ppl   314.52\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 12.21s | valid loss  4.99 | valid ppl   146.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch   4 |   300/ 1963 batches | lr 4.85 | ms/batch  6.13 | loss  5.52 | ppl   249.75\n",
      "| epoch   4 |   600/ 1963 batches | lr 4.85 | ms/batch  6.10 | loss  5.36 | ppl   211.91\n",
      "| epoch   4 |   900/ 1963 batches | lr 4.85 | ms/batch  6.75 | loss  5.53 | ppl   252.86\n",
      "| epoch   4 |  1200/ 1963 batches | lr 4.85 | ms/batch  6.05 | loss  5.43 | ppl   228.56\n",
      "| epoch   4 |  1500/ 1963 batches | lr 4.85 | ms/batch  6.04 | loss  5.18 | ppl   177.45\n",
      "| epoch   4 |  1800/ 1963 batches | lr 4.85 | ms/batch  6.07 | loss  5.23 | ppl   187.56\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 12.38s | valid loss  4.29 | valid ppl    72.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch   5 |   300/ 1963 batches | lr 4.80 | ms/batch  6.05 | loss  5.05 | ppl   155.68\n",
      "| epoch   5 |   600/ 1963 batches | lr 4.80 | ms/batch  5.98 | loss  5.10 | ppl   164.35\n",
      "| epoch   5 |   900/ 1963 batches | lr 4.80 | ms/batch  6.00 | loss  5.01 | ppl   150.43\n",
      "| epoch   5 |  1200/ 1963 batches | lr 4.80 | ms/batch  6.05 | loss  4.91 | ppl   135.82\n",
      "| epoch   5 |  1500/ 1963 batches | lr 4.80 | ms/batch  6.17 | loss  4.91 | ppl   136.03\n",
      "| epoch   5 |  1800/ 1963 batches | lr 4.80 | ms/batch  6.16 | loss  4.78 | ppl   118.57\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 12.20s | valid loss  5.48 | valid ppl   240.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   300/ 1963 batches | lr 4.75 | ms/batch  6.18 | loss  4.58 | ppl    97.67\n",
      "| epoch   6 |   600/ 1963 batches | lr 4.75 | ms/batch  6.29 | loss  4.56 | ppl    95.88\n",
      "| epoch   6 |   900/ 1963 batches | lr 4.75 | ms/batch  6.31 | loss  4.60 | ppl    99.54\n",
      "| epoch   6 |  1200/ 1963 batches | lr 4.75 | ms/batch  6.21 | loss  4.41 | ppl    82.00\n",
      "| epoch   6 |  1500/ 1963 batches | lr 4.75 | ms/batch  6.09 | loss  4.34 | ppl    76.81\n",
      "| epoch   6 |  1800/ 1963 batches | lr 4.75 | ms/batch  6.11 | loss  4.30 | ppl    73.81\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 12.40s | valid loss  4.56 | valid ppl    95.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   300/ 1963 batches | lr 4.71 | ms/batch  6.18 | loss  4.20 | ppl    66.38\n",
      "| epoch   7 |   600/ 1963 batches | lr 4.71 | ms/batch  6.15 | loss  4.00 | ppl    54.63\n",
      "| epoch   7 |   900/ 1963 batches | lr 4.71 | ms/batch  6.20 | loss  4.05 | ppl    57.17\n",
      "| epoch   7 |  1200/ 1963 batches | lr 4.71 | ms/batch  6.02 | loss  4.09 | ppl    60.02\n",
      "| epoch   7 |  1500/ 1963 batches | lr 4.71 | ms/batch  6.04 | loss  3.98 | ppl    53.55\n",
      "| epoch   7 |  1800/ 1963 batches | lr 4.71 | ms/batch  6.16 | loss  3.88 | ppl    48.27\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 12.26s | valid loss  5.19 | valid ppl   178.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   300/ 1963 batches | lr 4.66 | ms/batch  6.16 | loss  3.91 | ppl    49.70\n",
      "| epoch   8 |   600/ 1963 batches | lr 4.66 | ms/batch  6.20 | loss  3.73 | ppl    41.82\n",
      "| epoch   8 |   900/ 1963 batches | lr 4.66 | ms/batch  6.11 | loss  3.85 | ppl    46.89\n",
      "| epoch   8 |  1200/ 1963 batches | lr 4.66 | ms/batch  6.02 | loss  3.88 | ppl    48.24\n",
      "| epoch   8 |  1500/ 1963 batches | lr 4.66 | ms/batch  6.03 | loss  3.82 | ppl    45.49\n",
      "| epoch   8 |  1800/ 1963 batches | lr 4.66 | ms/batch  6.04 | loss  3.89 | ppl    48.71\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.21s | valid loss  4.04 | valid ppl    56.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch   9 |   300/ 1963 batches | lr 4.61 | ms/batch  6.17 | loss  3.84 | ppl    46.66\n",
      "| epoch   9 |   600/ 1963 batches | lr 4.61 | ms/batch  6.13 | loss  3.71 | ppl    41.04\n",
      "| epoch   9 |   900/ 1963 batches | lr 4.61 | ms/batch  6.23 | loss  3.76 | ppl    43.02\n",
      "| epoch   9 |  1200/ 1963 batches | lr 4.61 | ms/batch  6.45 | loss  3.87 | ppl    48.01\n",
      "| epoch   9 |  1500/ 1963 batches | lr 4.61 | ms/batch  6.24 | loss  3.82 | ppl    45.45\n",
      "| epoch   9 |  1800/ 1963 batches | lr 4.61 | ms/batch  6.32 | loss  3.83 | ppl    46.16\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 12.55s | valid loss  3.76 | valid ppl    42.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  10 |   300/ 1963 batches | lr 4.57 | ms/batch  6.22 | loss  3.63 | ppl    37.71\n",
      "| epoch  10 |   600/ 1963 batches | lr 4.57 | ms/batch  6.17 | loss  3.55 | ppl    34.77\n",
      "| epoch  10 |   900/ 1963 batches | lr 4.57 | ms/batch  6.04 | loss  3.73 | ppl    41.81\n",
      "| epoch  10 |  1200/ 1963 batches | lr 4.57 | ms/batch  6.10 | loss  3.64 | ppl    37.92\n",
      "| epoch  10 |  1500/ 1963 batches | lr 4.57 | ms/batch  6.12 | loss  3.74 | ppl    42.12\n",
      "| epoch  10 |  1800/ 1963 batches | lr 4.57 | ms/batch  6.03 | loss  3.81 | ppl    45.22\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 12.25s | valid loss  5.06 | valid ppl   157.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   300/ 1963 batches | lr 4.52 | ms/batch  6.13 | loss  3.62 | ppl    37.47\n",
      "| epoch  11 |   600/ 1963 batches | lr 4.52 | ms/batch  6.20 | loss  3.58 | ppl    35.87\n",
      "| epoch  11 |   900/ 1963 batches | lr 4.52 | ms/batch  6.07 | loss  3.65 | ppl    38.44\n",
      "| epoch  11 |  1200/ 1963 batches | lr 4.52 | ms/batch  6.21 | loss  3.76 | ppl    42.76\n",
      "| epoch  11 |  1500/ 1963 batches | lr 4.52 | ms/batch  6.20 | loss  3.68 | ppl    39.47\n",
      "| epoch  11 |  1800/ 1963 batches | lr 4.52 | ms/batch  6.10 | loss  3.62 | ppl    37.35\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 12.31s | valid loss  4.61 | valid ppl   100.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   300/ 1963 batches | lr 4.48 | ms/batch  6.20 | loss  3.59 | ppl    36.36\n",
      "| epoch  12 |   600/ 1963 batches | lr 4.48 | ms/batch  6.04 | loss  3.56 | ppl    35.33\n",
      "| epoch  12 |   900/ 1963 batches | lr 4.48 | ms/batch  6.04 | loss  3.56 | ppl    35.14\n",
      "| epoch  12 |  1200/ 1963 batches | lr 4.48 | ms/batch  6.00 | loss  3.56 | ppl    35.18\n",
      "| epoch  12 |  1500/ 1963 batches | lr 4.48 | ms/batch  6.02 | loss  3.71 | ppl    40.91\n",
      "| epoch  12 |  1800/ 1963 batches | lr 4.48 | ms/batch  6.09 | loss  3.62 | ppl    37.29\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 12.15s | valid loss  5.95 | valid ppl   384.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   300/ 1963 batches | lr 4.43 | ms/batch  6.05 | loss  3.57 | ppl    35.52\n",
      "| epoch  13 |   600/ 1963 batches | lr 4.43 | ms/batch  6.05 | loss  3.47 | ppl    32.02\n",
      "| epoch  13 |   900/ 1963 batches | lr 4.43 | ms/batch  6.11 | loss  3.59 | ppl    36.12\n",
      "| epoch  13 |  1200/ 1963 batches | lr 4.43 | ms/batch  6.19 | loss  3.66 | ppl    38.67\n",
      "| epoch  13 |  1500/ 1963 batches | lr 4.43 | ms/batch  6.18 | loss  3.60 | ppl    36.55\n",
      "| epoch  13 |  1800/ 1963 batches | lr 4.43 | ms/batch  6.85 | loss  3.64 | ppl    38.13\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 12.56s | valid loss  3.44 | valid ppl    31.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  14 |   300/ 1963 batches | lr 4.39 | ms/batch  6.12 | loss  3.56 | ppl    35.30\n",
      "| epoch  14 |   600/ 1963 batches | lr 4.39 | ms/batch  6.05 | loss  3.51 | ppl    33.46\n",
      "| epoch  14 |   900/ 1963 batches | lr 4.39 | ms/batch  6.36 | loss  3.60 | ppl    36.77\n",
      "| epoch  14 |  1200/ 1963 batches | lr 4.39 | ms/batch  6.30 | loss  3.63 | ppl    37.56\n",
      "| epoch  14 |  1500/ 1963 batches | lr 4.39 | ms/batch  6.51 | loss  3.49 | ppl    32.94\n",
      "| epoch  14 |  1800/ 1963 batches | lr 4.39 | ms/batch  6.85 | loss  3.50 | ppl    33.04\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 12.78s | valid loss  3.64 | valid ppl    38.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   300/ 1963 batches | lr 4.34 | ms/batch  6.32 | loss  3.51 | ppl    33.48\n",
      "| epoch  15 |   600/ 1963 batches | lr 4.34 | ms/batch  6.20 | loss  3.44 | ppl    31.25\n",
      "| epoch  15 |   900/ 1963 batches | lr 4.34 | ms/batch  6.07 | loss  3.45 | ppl    31.64\n",
      "| epoch  15 |  1200/ 1963 batches | lr 4.34 | ms/batch  6.52 | loss  3.56 | ppl    35.05\n",
      "| epoch  15 |  1500/ 1963 batches | lr 4.34 | ms/batch  6.47 | loss  3.53 | ppl    34.11\n",
      "| epoch  15 |  1800/ 1963 batches | lr 4.34 | ms/batch  8.58 | loss  3.56 | ppl    35.06\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 13.35s | valid loss  3.60 | valid ppl    36.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   300/ 1963 batches | lr 4.30 | ms/batch  6.89 | loss  3.56 | ppl    34.99\n",
      "| epoch  16 |   600/ 1963 batches | lr 4.30 | ms/batch  6.24 | loss  3.40 | ppl    29.85\n",
      "| epoch  16 |   900/ 1963 batches | lr 4.30 | ms/batch 10.98 | loss  3.48 | ppl    32.34\n",
      "| epoch  16 |  1200/ 1963 batches | lr 4.30 | ms/batch 18.45 | loss  3.61 | ppl    36.91\n",
      "| epoch  16 |  1500/ 1963 batches | lr 4.30 | ms/batch 13.12 | loss  3.52 | ppl    33.79\n",
      "| epoch  16 |  1800/ 1963 batches | lr 4.30 | ms/batch 11.00 | loss  3.41 | ppl    30.25\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 21.92s | valid loss  4.24 | valid ppl    69.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   300/ 1963 batches | lr 4.26 | ms/batch 11.65 | loss  3.48 | ppl    32.61\n",
      "| epoch  17 |   600/ 1963 batches | lr 4.26 | ms/batch  6.30 | loss  3.36 | ppl    28.89\n",
      "| epoch  17 |   900/ 1963 batches | lr 4.26 | ms/batch  6.36 | loss  3.51 | ppl    33.55\n",
      "| epoch  17 |  1200/ 1963 batches | lr 4.26 | ms/batch  6.26 | loss  3.41 | ppl    30.24\n",
      "| epoch  17 |  1500/ 1963 batches | lr 4.26 | ms/batch  6.11 | loss  3.43 | ppl    30.91\n",
      "| epoch  17 |  1800/ 1963 batches | lr 4.26 | ms/batch  6.08 | loss  3.51 | ppl    33.28\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 14.10s | valid loss  3.65 | valid ppl    38.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   300/ 1963 batches | lr 4.21 | ms/batch  6.12 | loss  3.47 | ppl    32.09\n",
      "| epoch  18 |   600/ 1963 batches | lr 4.21 | ms/batch  5.97 | loss  3.32 | ppl    27.76\n",
      "| epoch  18 |   900/ 1963 batches | lr 4.21 | ms/batch  5.98 | loss  3.41 | ppl    30.34\n",
      "| epoch  18 |  1200/ 1963 batches | lr 4.21 | ms/batch  5.98 | loss  3.47 | ppl    32.22\n",
      "| epoch  18 |  1500/ 1963 batches | lr 4.21 | ms/batch  5.99 | loss  3.40 | ppl    30.01\n",
      "| epoch  18 |  1800/ 1963 batches | lr 4.21 | ms/batch  5.98 | loss  3.46 | ppl    31.83\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 12.04s | valid loss  3.49 | valid ppl    32.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   300/ 1963 batches | lr 4.17 | ms/batch  6.15 | loss  3.41 | ppl    30.19\n",
      "| epoch  19 |   600/ 1963 batches | lr 4.17 | ms/batch  6.14 | loss  3.30 | ppl    27.23\n",
      "| epoch  19 |   900/ 1963 batches | lr 4.17 | ms/batch  6.15 | loss  3.38 | ppl    29.48\n",
      "| epoch  19 |  1200/ 1963 batches | lr 4.17 | ms/batch  6.31 | loss  3.52 | ppl    33.81\n",
      "| epoch  19 |  1500/ 1963 batches | lr 4.17 | ms/batch  6.17 | loss  3.40 | ppl    29.86\n",
      "| epoch  19 |  1800/ 1963 batches | lr 4.17 | ms/batch  6.36 | loss  3.42 | ppl    30.57\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 12.48s | valid loss  3.73 | valid ppl    41.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   300/ 1963 batches | lr 4.13 | ms/batch  6.30 | loss  3.28 | ppl    26.57\n",
      "| epoch  20 |   600/ 1963 batches | lr 4.13 | ms/batch  6.18 | loss  3.24 | ppl    25.48\n",
      "| epoch  20 |   900/ 1963 batches | lr 4.13 | ms/batch  6.10 | loss  3.34 | ppl    28.15\n",
      "| epoch  20 |  1200/ 1963 batches | lr 4.13 | ms/batch  6.13 | loss  3.38 | ppl    29.30\n",
      "| epoch  20 |  1500/ 1963 batches | lr 4.13 | ms/batch  6.15 | loss  3.31 | ppl    27.36\n",
      "| epoch  20 |  1800/ 1963 batches | lr 4.13 | ms/batch  6.15 | loss  3.34 | ppl    28.16\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 12.35s | valid loss  3.01 | valid ppl    20.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  21 |   300/ 1963 batches | lr 4.09 | ms/batch  6.16 | loss  3.22 | ppl    25.11\n",
      "| epoch  21 |   600/ 1963 batches | lr 4.09 | ms/batch  6.16 | loss  3.29 | ppl    26.95\n",
      "| epoch  21 |   900/ 1963 batches | lr 4.09 | ms/batch  6.15 | loss  3.29 | ppl    26.91\n",
      "| epoch  21 |  1200/ 1963 batches | lr 4.09 | ms/batch  6.25 | loss  3.34 | ppl    28.10\n",
      "| epoch  21 |  1500/ 1963 batches | lr 4.09 | ms/batch  6.01 | loss  3.25 | ppl    25.71\n",
      "| epoch  21 |  1800/ 1963 batches | lr 4.09 | ms/batch  6.01 | loss  3.28 | ppl    26.68\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 12.28s | valid loss  3.32 | valid ppl    27.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   300/ 1963 batches | lr 4.05 | ms/batch  6.48 | loss  3.23 | ppl    25.23\n",
      "| epoch  22 |   600/ 1963 batches | lr 4.05 | ms/batch  6.14 | loss  3.25 | ppl    25.67\n",
      "| epoch  22 |   900/ 1963 batches | lr 4.05 | ms/batch  6.07 | loss  3.28 | ppl    26.47\n",
      "| epoch  22 |  1200/ 1963 batches | lr 4.05 | ms/batch  6.05 | loss  3.34 | ppl    28.25\n",
      "| epoch  22 |  1500/ 1963 batches | lr 4.05 | ms/batch  6.03 | loss  3.27 | ppl    26.36\n",
      "| epoch  22 |  1800/ 1963 batches | lr 4.05 | ms/batch  5.98 | loss  3.27 | ppl    26.33\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 12.25s | valid loss  2.98 | valid ppl    19.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  23 |   300/ 1963 batches | lr 4.01 | ms/batch  9.65 | loss  3.23 | ppl    25.19\n",
      "| epoch  23 |   600/ 1963 batches | lr 4.01 | ms/batch 13.43 | loss  3.15 | ppl    23.35\n",
      "| epoch  23 |   900/ 1963 batches | lr 4.01 | ms/batch  6.08 | loss  3.23 | ppl    25.37\n",
      "| epoch  23 |  1200/ 1963 batches | lr 4.01 | ms/batch  6.09 | loss  3.37 | ppl    29.01\n",
      "| epoch  23 |  1500/ 1963 batches | lr 4.01 | ms/batch  6.11 | loss  3.16 | ppl    23.49\n",
      "| epoch  23 |  1800/ 1963 batches | lr 4.01 | ms/batch  6.10 | loss  3.19 | ppl    24.31\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 15.48s | valid loss  2.82 | valid ppl    16.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  24 |   300/ 1963 batches | lr 3.97 | ms/batch  6.71 | loss  3.15 | ppl    23.45\n",
      "| epoch  24 |   600/ 1963 batches | lr 3.97 | ms/batch  6.78 | loss  3.11 | ppl    22.51\n",
      "| epoch  24 |   900/ 1963 batches | lr 3.97 | ms/batch  6.07 | loss  3.21 | ppl    24.73\n",
      "| epoch  24 |  1200/ 1963 batches | lr 3.97 | ms/batch  5.97 | loss  3.18 | ppl    23.99\n",
      "| epoch  24 |  1500/ 1963 batches | lr 3.97 | ms/batch  8.46 | loss  3.28 | ppl    26.69\n",
      "| epoch  24 |  1800/ 1963 batches | lr 3.97 | ms/batch 11.64 | loss  3.30 | ppl    27.11\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 16.26s | valid loss  2.79 | valid ppl    16.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  25 |   300/ 1963 batches | lr 3.93 | ms/batch 10.85 | loss  3.15 | ppl    23.44\n",
      "| epoch  25 |   600/ 1963 batches | lr 3.93 | ms/batch  9.19 | loss  3.19 | ppl    24.28\n",
      "| epoch  25 |   900/ 1963 batches | lr 3.93 | ms/batch  5.97 | loss  3.14 | ppl    23.18\n",
      "| epoch  25 |  1200/ 1963 batches | lr 3.93 | ms/batch  6.00 | loss  3.17 | ppl    23.85\n",
      "| epoch  25 |  1500/ 1963 batches | lr 3.93 | ms/batch  5.99 | loss  3.19 | ppl    24.24\n",
      "| epoch  25 |  1800/ 1963 batches | lr 3.93 | ms/batch  6.01 | loss  3.20 | ppl    24.50\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 14.43s | valid loss  3.20 | valid ppl    24.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   300/ 1963 batches | lr 3.89 | ms/batch  6.12 | loss  3.08 | ppl    21.66\n",
      "| epoch  26 |   600/ 1963 batches | lr 3.89 | ms/batch  6.21 | loss  2.98 | ppl    19.64\n",
      "| epoch  26 |   900/ 1963 batches | lr 3.89 | ms/batch  6.12 | loss  3.13 | ppl    22.89\n",
      "| epoch  26 |  1200/ 1963 batches | lr 3.89 | ms/batch  6.09 | loss  3.05 | ppl    21.20\n",
      "| epoch  26 |  1500/ 1963 batches | lr 3.89 | ms/batch  6.09 | loss  3.06 | ppl    21.39\n",
      "| epoch  26 |  1800/ 1963 batches | lr 3.89 | ms/batch  6.07 | loss  3.14 | ppl    23.04\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 12.26s | valid loss  2.79 | valid ppl    16.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   300/ 1963 batches | lr 3.85 | ms/batch  6.34 | loss  3.11 | ppl    22.53\n",
      "| epoch  27 |   600/ 1963 batches | lr 3.85 | ms/batch  7.02 | loss  3.02 | ppl    20.53\n",
      "| epoch  27 |   900/ 1963 batches | lr 3.85 | ms/batch  6.45 | loss  3.10 | ppl    22.14\n",
      "| epoch  27 |  1200/ 1963 batches | lr 3.85 | ms/batch  6.43 | loss  3.09 | ppl    21.89\n",
      "| epoch  27 |  1500/ 1963 batches | lr 3.85 | ms/batch  6.50 | loss  3.12 | ppl    22.56\n",
      "| epoch  27 |  1800/ 1963 batches | lr 3.85 | ms/batch  6.59 | loss  3.16 | ppl    23.49\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 13.25s | valid loss  3.15 | valid ppl    23.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   300/ 1963 batches | lr 3.81 | ms/batch  7.00 | loss  3.04 | ppl    20.80\n",
      "| epoch  28 |   600/ 1963 batches | lr 3.81 | ms/batch  6.64 | loss  2.99 | ppl    19.84\n",
      "| epoch  28 |   900/ 1963 batches | lr 3.81 | ms/batch  6.10 | loss  3.14 | ppl    23.08\n",
      "| epoch  28 |  1200/ 1963 batches | lr 3.81 | ms/batch  6.43 | loss  3.04 | ppl    21.00\n",
      "| epoch  28 |  1500/ 1963 batches | lr 3.81 | ms/batch  6.69 | loss  3.05 | ppl    21.16\n",
      "| epoch  28 |  1800/ 1963 batches | lr 3.81 | ms/batch  7.06 | loss  3.06 | ppl    21.39\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 13.44s | valid loss  2.87 | valid ppl    17.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   300/ 1963 batches | lr 3.77 | ms/batch  6.83 | loss  3.10 | ppl    22.13\n",
      "| epoch  29 |   600/ 1963 batches | lr 3.77 | ms/batch  7.04 | loss  2.98 | ppl    19.71\n",
      "| epoch  29 |   900/ 1963 batches | lr 3.77 | ms/batch  7.06 | loss  3.08 | ppl    21.67\n",
      "| epoch  29 |  1200/ 1963 batches | lr 3.77 | ms/batch  6.99 | loss  3.07 | ppl    21.65\n",
      "| epoch  29 |  1500/ 1963 batches | lr 3.77 | ms/batch  6.93 | loss  3.04 | ppl    20.90\n",
      "| epoch  29 |  1800/ 1963 batches | lr 3.77 | ms/batch  7.41 | loss  3.05 | ppl    21.22\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 14.25s | valid loss  4.51 | valid ppl    90.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   300/ 1963 batches | lr 3.74 | ms/batch  7.77 | loss  3.01 | ppl    20.21\n",
      "| epoch  30 |   600/ 1963 batches | lr 3.74 | ms/batch  7.29 | loss  2.97 | ppl    19.44\n",
      "| epoch  30 |   900/ 1963 batches | lr 3.74 | ms/batch  6.99 | loss  3.04 | ppl    20.98\n",
      "| epoch  30 |  1200/ 1963 batches | lr 3.74 | ms/batch  6.34 | loss  2.99 | ppl    19.95\n",
      "| epoch  30 |  1500/ 1963 batches | lr 3.74 | ms/batch  6.95 | loss  2.97 | ppl    19.44\n",
      "| epoch  30 |  1800/ 1963 batches | lr 3.74 | ms/batch  6.57 | loss  2.99 | ppl    19.92\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 13.90s | valid loss  3.76 | valid ppl    42.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |   300/ 1963 batches | lr 3.70 | ms/batch  6.47 | loss  3.02 | ppl    20.57\n",
      "| epoch  31 |   600/ 1963 batches | lr 3.70 | ms/batch  6.90 | loss  2.94 | ppl    19.00\n",
      "| epoch  31 |   900/ 1963 batches | lr 3.70 | ms/batch  7.45 | loss  2.96 | ppl    19.21\n",
      "| epoch  31 |  1200/ 1963 batches | lr 3.70 | ms/batch  7.67 | loss  2.96 | ppl    19.33\n",
      "| epoch  31 |  1500/ 1963 batches | lr 3.70 | ms/batch  6.95 | loss  2.89 | ppl    17.94\n",
      "| epoch  31 |  1800/ 1963 batches | lr 3.70 | ms/batch  6.70 | loss  3.03 | ppl    20.69\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 13.99s | valid loss  2.75 | valid ppl    15.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  32 |   300/ 1963 batches | lr 3.66 | ms/batch  6.50 | loss  2.95 | ppl    19.05\n",
      "| epoch  32 |   600/ 1963 batches | lr 3.66 | ms/batch  6.80 | loss  2.84 | ppl    17.13\n",
      "| epoch  32 |   900/ 1963 batches | lr 3.66 | ms/batch  7.24 | loss  3.02 | ppl    20.41\n",
      "| epoch  32 |  1200/ 1963 batches | lr 3.66 | ms/batch  7.21 | loss  2.96 | ppl    19.27\n",
      "| epoch  32 |  1500/ 1963 batches | lr 3.66 | ms/batch  6.33 | loss  2.92 | ppl    18.62\n",
      "| epoch  32 |  1800/ 1963 batches | lr 3.66 | ms/batch  6.38 | loss  3.00 | ppl    20.11\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 13.48s | valid loss  2.77 | valid ppl    15.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |   300/ 1963 batches | lr 3.62 | ms/batch  6.07 | loss  2.91 | ppl    18.30\n",
      "| epoch  33 |   600/ 1963 batches | lr 3.62 | ms/batch  6.20 | loss  2.84 | ppl    17.18\n",
      "| epoch  33 |   900/ 1963 batches | lr 3.62 | ms/batch  6.17 | loss  2.93 | ppl    18.79\n",
      "| epoch  33 |  1200/ 1963 batches | lr 3.62 | ms/batch  6.07 | loss  2.94 | ppl    18.98\n",
      "| epoch  33 |  1500/ 1963 batches | lr 3.62 | ms/batch  6.32 | loss  2.91 | ppl    18.44\n",
      "| epoch  33 |  1800/ 1963 batches | lr 3.62 | ms/batch  6.28 | loss  2.96 | ppl    19.21\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 12.47s | valid loss  2.80 | valid ppl    16.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |   300/ 1963 batches | lr 3.59 | ms/batch  6.92 | loss  2.86 | ppl    17.38\n",
      "| epoch  34 |   600/ 1963 batches | lr 3.59 | ms/batch  7.00 | loss  2.79 | ppl    16.28\n",
      "| epoch  34 |   900/ 1963 batches | lr 3.59 | ms/batch  7.10 | loss  2.90 | ppl    18.18\n",
      "| epoch  34 |  1200/ 1963 batches | lr 3.59 | ms/batch  7.40 | loss  2.98 | ppl    19.62\n",
      "| epoch  34 |  1500/ 1963 batches | lr 3.59 | ms/batch  7.11 | loss  2.86 | ppl    17.47\n",
      "| epoch  34 |  1800/ 1963 batches | lr 3.59 | ms/batch  6.55 | loss  2.89 | ppl    17.92\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 13.90s | valid loss  2.71 | valid ppl    15.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  35 |   300/ 1963 batches | lr 3.55 | ms/batch  6.29 | loss  2.85 | ppl    17.31\n",
      "| epoch  35 |   600/ 1963 batches | lr 3.55 | ms/batch  6.71 | loss  2.78 | ppl    16.14\n",
      "| epoch  35 |   900/ 1963 batches | lr 3.55 | ms/batch  6.71 | loss  2.81 | ppl    16.54\n",
      "| epoch  35 |  1200/ 1963 batches | lr 3.55 | ms/batch  6.42 | loss  2.88 | ppl    17.81\n",
      "| epoch  35 |  1500/ 1963 batches | lr 3.55 | ms/batch  6.39 | loss  2.75 | ppl    15.69\n",
      "| epoch  35 |  1800/ 1963 batches | lr 3.55 | ms/batch  6.68 | loss  2.91 | ppl    18.31\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 13.15s | valid loss  2.98 | valid ppl    19.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |   300/ 1963 batches | lr 3.52 | ms/batch  7.34 | loss  2.79 | ppl    16.27\n",
      "| epoch  36 |   600/ 1963 batches | lr 3.52 | ms/batch  6.84 | loss  2.74 | ppl    15.56\n",
      "| epoch  36 |   900/ 1963 batches | lr 3.52 | ms/batch  7.04 | loss  2.79 | ppl    16.27\n",
      "| epoch  36 |  1200/ 1963 batches | lr 3.52 | ms/batch  6.98 | loss  2.83 | ppl    16.96\n",
      "| epoch  36 |  1500/ 1963 batches | lr 3.52 | ms/batch  6.58 | loss  2.80 | ppl    16.43\n",
      "| epoch  36 |  1800/ 1963 batches | lr 3.52 | ms/batch  6.64 | loss  2.77 | ppl    16.01\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 13.77s | valid loss  2.70 | valid ppl    14.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  37 |   300/ 1963 batches | lr 3.48 | ms/batch  6.49 | loss  2.78 | ppl    16.20\n",
      "| epoch  37 |   600/ 1963 batches | lr 3.48 | ms/batch  6.27 | loss  2.73 | ppl    15.30\n",
      "| epoch  37 |   900/ 1963 batches | lr 3.48 | ms/batch  6.69 | loss  2.77 | ppl    15.94\n",
      "| epoch  37 |  1200/ 1963 batches | lr 3.48 | ms/batch  7.59 | loss  2.77 | ppl    16.00\n",
      "| epoch  37 |  1500/ 1963 batches | lr 3.48 | ms/batch  6.76 | loss  2.75 | ppl    15.69\n",
      "| epoch  37 |  1800/ 1963 batches | lr 3.48 | ms/batch  6.64 | loss  2.81 | ppl    16.57\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 13.43s | valid loss  2.70 | valid ppl    14.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  38 |   300/ 1963 batches | lr 3.45 | ms/batch  7.07 | loss  2.81 | ppl    16.58\n",
      "| epoch  38 |   600/ 1963 batches | lr 3.45 | ms/batch  6.91 | loss  2.70 | ppl    14.89\n",
      "| epoch  38 |   900/ 1963 batches | lr 3.45 | ms/batch  6.72 | loss  2.72 | ppl    15.15\n",
      "| epoch  38 |  1200/ 1963 batches | lr 3.45 | ms/batch  6.82 | loss  2.77 | ppl    16.03\n",
      "| epoch  38 |  1500/ 1963 batches | lr 3.45 | ms/batch  6.37 | loss  2.72 | ppl    15.21\n",
      "| epoch  38 |  1800/ 1963 batches | lr 3.45 | ms/batch  6.45 | loss  2.76 | ppl    15.87\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 13.53s | valid loss  2.85 | valid ppl    17.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |   300/ 1963 batches | lr 3.41 | ms/batch  7.34 | loss  2.71 | ppl    15.01\n",
      "| epoch  39 |   600/ 1963 batches | lr 3.41 | ms/batch  6.25 | loss  2.66 | ppl    14.23\n",
      "| epoch  39 |   900/ 1963 batches | lr 3.41 | ms/batch  5.97 | loss  2.70 | ppl    14.92\n",
      "| epoch  39 |  1200/ 1963 batches | lr 3.41 | ms/batch  5.97 | loss  2.73 | ppl    15.39\n",
      "| epoch  39 |  1500/ 1963 batches | lr 3.41 | ms/batch  5.99 | loss  2.73 | ppl    15.35\n",
      "| epoch  39 |  1800/ 1963 batches | lr 3.41 | ms/batch  5.97 | loss  2.70 | ppl    14.92\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 12.47s | valid loss  2.65 | valid ppl    14.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  40 |   300/ 1963 batches | lr 3.38 | ms/batch  6.05 | loss  2.69 | ppl    14.70\n",
      "| epoch  40 |   600/ 1963 batches | lr 3.38 | ms/batch  6.02 | loss  2.67 | ppl    14.49\n",
      "| epoch  40 |   900/ 1963 batches | lr 3.38 | ms/batch  6.06 | loss  2.70 | ppl    14.83\n",
      "| epoch  40 |  1200/ 1963 batches | lr 3.38 | ms/batch  6.08 | loss  2.70 | ppl    14.92\n",
      "| epoch  40 |  1500/ 1963 batches | lr 3.38 | ms/batch  6.06 | loss  2.69 | ppl    14.66\n",
      "| epoch  40 |  1800/ 1963 batches | lr 3.38 | ms/batch  6.06 | loss  2.71 | ppl    15.05\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 12.14s | valid loss  3.48 | valid ppl    32.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |   300/ 1963 batches | lr 3.34 | ms/batch  6.00 | loss  2.68 | ppl    14.61\n",
      "| epoch  41 |   600/ 1963 batches | lr 3.34 | ms/batch  5.97 | loss  2.62 | ppl    13.78\n",
      "| epoch  41 |   900/ 1963 batches | lr 3.34 | ms/batch  5.97 | loss  2.68 | ppl    14.58\n",
      "| epoch  41 |  1200/ 1963 batches | lr 3.34 | ms/batch  5.98 | loss  2.67 | ppl    14.42\n",
      "| epoch  41 |  1500/ 1963 batches | lr 3.34 | ms/batch  5.97 | loss  2.64 | ppl    14.03\n",
      "| epoch  41 |  1800/ 1963 batches | lr 3.34 | ms/batch  5.97 | loss  2.66 | ppl    14.27\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 12.00s | valid loss  2.62 | valid ppl    13.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  42 |   300/ 1963 batches | lr 3.31 | ms/batch  6.00 | loss  2.66 | ppl    14.35\n",
      "| epoch  42 |   600/ 1963 batches | lr 3.31 | ms/batch  5.97 | loss  2.61 | ppl    13.62\n",
      "| epoch  42 |   900/ 1963 batches | lr 3.31 | ms/batch  5.97 | loss  2.65 | ppl    14.10\n",
      "| epoch  42 |  1200/ 1963 batches | lr 3.31 | ms/batch  5.97 | loss  2.67 | ppl    14.38\n",
      "| epoch  42 |  1500/ 1963 batches | lr 3.31 | ms/batch  5.97 | loss  2.63 | ppl    13.85\n",
      "| epoch  42 |  1800/ 1963 batches | lr 3.31 | ms/batch  5.97 | loss  2.64 | ppl    13.98\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 11.98s | valid loss  2.57 | valid ppl    13.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  43 |   300/ 1963 batches | lr 3.28 | ms/batch  6.00 | loss  2.62 | ppl    13.78\n",
      "| epoch  43 |   600/ 1963 batches | lr 3.28 | ms/batch  5.97 | loss  2.60 | ppl    13.49\n",
      "| epoch  43 |   900/ 1963 batches | lr 3.28 | ms/batch  5.97 | loss  2.64 | ppl    13.96\n",
      "| epoch  43 |  1200/ 1963 batches | lr 3.28 | ms/batch  5.97 | loss  2.64 | ppl    14.04\n",
      "| epoch  43 |  1500/ 1963 batches | lr 3.28 | ms/batch  5.97 | loss  2.61 | ppl    13.65\n",
      "| epoch  43 |  1800/ 1963 batches | lr 3.28 | ms/batch  5.98 | loss  2.59 | ppl    13.36\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 11.98s | valid loss  2.57 | valid ppl    13.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |   300/ 1963 batches | lr 3.25 | ms/batch  5.99 | loss  2.62 | ppl    13.74\n",
      "| epoch  44 |   600/ 1963 batches | lr 3.25 | ms/batch  5.97 | loss  2.58 | ppl    13.22\n",
      "| epoch  44 |   900/ 1963 batches | lr 3.25 | ms/batch  5.97 | loss  2.61 | ppl    13.62\n",
      "| epoch  44 |  1200/ 1963 batches | lr 3.25 | ms/batch  5.97 | loss  2.60 | ppl    13.49\n",
      "| epoch  44 |  1500/ 1963 batches | lr 3.25 | ms/batch  5.97 | loss  2.59 | ppl    13.34\n",
      "| epoch  44 |  1800/ 1963 batches | lr 3.25 | ms/batch  5.97 | loss  2.57 | ppl    13.07\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 11.98s | valid loss  2.58 | valid ppl    13.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |   300/ 1963 batches | lr 3.21 | ms/batch  6.00 | loss  2.58 | ppl    13.26\n",
      "| epoch  45 |   600/ 1963 batches | lr 3.21 | ms/batch  5.97 | loss  2.57 | ppl    13.03\n",
      "| epoch  45 |   900/ 1963 batches | lr 3.21 | ms/batch  5.97 | loss  2.59 | ppl    13.34\n",
      "| epoch  45 |  1200/ 1963 batches | lr 3.21 | ms/batch  5.98 | loss  2.59 | ppl    13.35\n",
      "| epoch  45 |  1500/ 1963 batches | lr 3.21 | ms/batch  5.97 | loss  2.56 | ppl    12.96\n",
      "| epoch  45 |  1800/ 1963 batches | lr 3.21 | ms/batch  5.97 | loss  2.57 | ppl    13.07\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 11.99s | valid loss  2.49 | valid ppl    12.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  46 |   300/ 1963 batches | lr 3.18 | ms/batch  6.09 | loss  2.56 | ppl    12.90\n",
      "| epoch  46 |   600/ 1963 batches | lr 3.18 | ms/batch  6.07 | loss  2.53 | ppl    12.57\n",
      "| epoch  46 |   900/ 1963 batches | lr 3.18 | ms/batch  6.07 | loss  2.57 | ppl    13.09\n",
      "| epoch  46 |  1200/ 1963 batches | lr 3.18 | ms/batch  6.08 | loss  2.56 | ppl    12.99\n",
      "| epoch  46 |  1500/ 1963 batches | lr 3.18 | ms/batch  6.06 | loss  2.54 | ppl    12.67\n",
      "| epoch  46 |  1800/ 1963 batches | lr 3.18 | ms/batch  5.97 | loss  2.52 | ppl    12.45\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 12.14s | valid loss  2.46 | valid ppl    11.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  47 |   300/ 1963 batches | lr 3.15 | ms/batch  6.00 | loss  2.53 | ppl    12.52\n",
      "| epoch  47 |   600/ 1963 batches | lr 3.15 | ms/batch  5.98 | loss  2.50 | ppl    12.20\n",
      "| epoch  47 |   900/ 1963 batches | lr 3.15 | ms/batch  5.98 | loss  2.55 | ppl    12.76\n",
      "| epoch  47 |  1200/ 1963 batches | lr 3.15 | ms/batch  5.99 | loss  2.54 | ppl    12.63\n",
      "| epoch  47 |  1500/ 1963 batches | lr 3.15 | ms/batch  6.06 | loss  2.52 | ppl    12.40\n",
      "| epoch  47 |  1800/ 1963 batches | lr 3.15 | ms/batch  6.21 | loss  2.50 | ppl    12.14\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 12.13s | valid loss  2.46 | valid ppl    11.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |   300/ 1963 batches | lr 3.12 | ms/batch  6.07 | loss  2.50 | ppl    12.18\n",
      "| epoch  48 |   600/ 1963 batches | lr 3.12 | ms/batch  6.03 | loss  2.47 | ppl    11.87\n",
      "| epoch  48 |   900/ 1963 batches | lr 3.12 | ms/batch  6.13 | loss  2.52 | ppl    12.39\n",
      "| epoch  48 |  1200/ 1963 batches | lr 3.12 | ms/batch  6.18 | loss  2.51 | ppl    12.34\n",
      "| epoch  48 |  1500/ 1963 batches | lr 3.12 | ms/batch  6.09 | loss  2.49 | ppl    12.08\n",
      "| epoch  48 |  1800/ 1963 batches | lr 3.12 | ms/batch  6.28 | loss  2.47 | ppl    11.87\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 12.36s | valid loss  2.48 | valid ppl    11.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |   300/ 1963 batches | lr 3.09 | ms/batch  6.17 | loss  2.48 | ppl    11.99\n",
      "| epoch  49 |   600/ 1963 batches | lr 3.09 | ms/batch  6.05 | loss  2.46 | ppl    11.74\n",
      "| epoch  49 |   900/ 1963 batches | lr 3.09 | ms/batch  6.06 | loss  2.50 | ppl    12.24\n",
      "| epoch  49 |  1200/ 1963 batches | lr 3.09 | ms/batch  6.11 | loss  2.49 | ppl    12.06\n",
      "| epoch  49 |  1500/ 1963 batches | lr 3.09 | ms/batch  6.04 | loss  2.47 | ppl    11.85\n",
      "| epoch  49 |  1800/ 1963 batches | lr 3.09 | ms/batch  6.12 | loss  2.46 | ppl    11.67\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 12.22s | valid loss  2.45 | valid ppl    11.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  50 |   300/ 1963 batches | lr 3.06 | ms/batch  6.25 | loss  2.46 | ppl    11.66\n",
      "| epoch  50 |   600/ 1963 batches | lr 3.06 | ms/batch  6.15 | loss  2.44 | ppl    11.49\n",
      "| epoch  50 |   900/ 1963 batches | lr 3.06 | ms/batch  6.23 | loss  2.48 | ppl    11.96\n",
      "| epoch  50 |  1200/ 1963 batches | lr 3.06 | ms/batch  6.67 | loss  2.47 | ppl    11.83\n",
      "| epoch  50 |  1500/ 1963 batches | lr 3.06 | ms/batch  6.41 | loss  2.46 | ppl    11.65\n",
      "| epoch  50 |  1800/ 1963 batches | lr 3.06 | ms/batch  6.32 | loss  2.44 | ppl    11.44\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 12.77s | valid loss  2.42 | valid ppl    11.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  51 |   300/ 1963 batches | lr 3.03 | ms/batch  6.31 | loss  2.44 | ppl    11.47\n",
      "| epoch  51 |   600/ 1963 batches | lr 3.03 | ms/batch  6.23 | loss  2.42 | ppl    11.28\n",
      "| epoch  51 |   900/ 1963 batches | lr 3.03 | ms/batch  6.32 | loss  2.47 | ppl    11.83\n",
      "| epoch  51 |  1200/ 1963 batches | lr 3.03 | ms/batch  6.19 | loss  2.46 | ppl    11.67\n",
      "| epoch  51 |  1500/ 1963 batches | lr 3.03 | ms/batch  6.15 | loss  2.43 | ppl    11.41\n",
      "| epoch  51 |  1800/ 1963 batches | lr 3.03 | ms/batch  6.07 | loss  2.42 | ppl    11.21\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 12.45s | valid loss  2.41 | valid ppl    11.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  52 |   300/ 1963 batches | lr 2.99 | ms/batch  6.29 | loss  2.43 | ppl    11.33\n",
      "| epoch  52 |   600/ 1963 batches | lr 2.99 | ms/batch  6.47 | loss  2.42 | ppl    11.20\n",
      "| epoch  52 |   900/ 1963 batches | lr 2.99 | ms/batch  6.23 | loss  2.45 | ppl    11.64\n",
      "| epoch  52 |  1200/ 1963 batches | lr 2.99 | ms/batch  6.33 | loss  2.43 | ppl    11.42\n",
      "| epoch  52 |  1500/ 1963 batches | lr 2.99 | ms/batch  6.44 | loss  2.42 | ppl    11.23\n",
      "| epoch  52 |  1800/ 1963 batches | lr 2.99 | ms/batch  6.54 | loss  2.40 | ppl    11.06\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 12.84s | valid loss  2.43 | valid ppl    11.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |   300/ 1963 batches | lr 2.96 | ms/batch  6.61 | loss  2.41 | ppl    11.08\n",
      "| epoch  53 |   600/ 1963 batches | lr 2.96 | ms/batch  6.65 | loss  2.39 | ppl    10.90\n",
      "| epoch  53 |   900/ 1963 batches | lr 2.96 | ms/batch  6.40 | loss  2.43 | ppl    11.39\n",
      "| epoch  53 |  1200/ 1963 batches | lr 2.96 | ms/batch  6.70 | loss  2.42 | ppl    11.26\n",
      "| epoch  53 |  1500/ 1963 batches | lr 2.96 | ms/batch  6.89 | loss  2.40 | ppl    11.03\n",
      "| epoch  53 |  1800/ 1963 batches | lr 2.96 | ms/batch  6.59 | loss  2.39 | ppl    10.88\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 13.32s | valid loss  2.36 | valid ppl    10.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  54 |   300/ 1963 batches | lr 2.94 | ms/batch  6.35 | loss  2.39 | ppl    10.95\n",
      "| epoch  54 |   600/ 1963 batches | lr 2.94 | ms/batch  6.39 | loss  2.37 | ppl    10.75\n",
      "| epoch  54 |   900/ 1963 batches | lr 2.94 | ms/batch  6.24 | loss  2.42 | ppl    11.20\n",
      "| epoch  54 |  1200/ 1963 batches | lr 2.94 | ms/batch  6.20 | loss  2.41 | ppl    11.08\n",
      "| epoch  54 |  1500/ 1963 batches | lr 2.94 | ms/batch  6.53 | loss  2.38 | ppl    10.82\n",
      "| epoch  54 |  1800/ 1963 batches | lr 2.94 | ms/batch  6.66 | loss  2.37 | ppl    10.72\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 12.81s | valid loss  2.36 | valid ppl    10.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  55 |   300/ 1963 batches | lr 2.91 | ms/batch  6.72 | loss  2.37 | ppl    10.70\n",
      "| epoch  55 |   600/ 1963 batches | lr 2.91 | ms/batch  7.30 | loss  2.36 | ppl    10.59\n",
      "| epoch  55 |   900/ 1963 batches | lr 2.91 | ms/batch  6.49 | loss  2.41 | ppl    11.09\n",
      "| epoch  55 |  1200/ 1963 batches | lr 2.91 | ms/batch  6.28 | loss  2.39 | ppl    10.93\n",
      "| epoch  55 |  1500/ 1963 batches | lr 2.91 | ms/batch  6.30 | loss  2.37 | ppl    10.68\n",
      "| epoch  55 |  1800/ 1963 batches | lr 2.91 | ms/batch  6.41 | loss  2.36 | ppl    10.59\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 13.18s | valid loss  2.32 | valid ppl    10.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  56 |   300/ 1963 batches | lr 2.88 | ms/batch  6.42 | loss  2.37 | ppl    10.65\n",
      "| epoch  56 |   600/ 1963 batches | lr 2.88 | ms/batch  6.28 | loss  2.34 | ppl    10.42\n",
      "| epoch  56 |   900/ 1963 batches | lr 2.88 | ms/batch  6.39 | loss  2.40 | ppl    10.97\n",
      "| epoch  56 |  1200/ 1963 batches | lr 2.88 | ms/batch  6.24 | loss  2.38 | ppl    10.79\n",
      "| epoch  56 |  1500/ 1963 batches | lr 2.88 | ms/batch  6.98 | loss  2.36 | ppl    10.57\n",
      "| epoch  56 |  1800/ 1963 batches | lr 2.88 | ms/batch  6.14 | loss  2.34 | ppl    10.36\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 12.85s | valid loss  2.40 | valid ppl    11.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |   300/ 1963 batches | lr 2.85 | ms/batch  6.38 | loss  2.35 | ppl    10.49\n",
      "| epoch  57 |   600/ 1963 batches | lr 2.85 | ms/batch  6.30 | loss  2.33 | ppl    10.27\n",
      "| epoch  57 |   900/ 1963 batches | lr 2.85 | ms/batch  6.58 | loss  2.38 | ppl    10.75\n",
      "| epoch  57 |  1200/ 1963 batches | lr 2.85 | ms/batch  7.30 | loss  2.36 | ppl    10.63\n",
      "| epoch  57 |  1500/ 1963 batches | lr 2.85 | ms/batch  7.02 | loss  2.34 | ppl    10.39\n",
      "| epoch  57 |  1800/ 1963 batches | lr 2.85 | ms/batch  6.84 | loss  2.33 | ppl    10.31\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 13.53s | valid loss  2.31 | valid ppl    10.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  58 |   300/ 1963 batches | lr 2.82 | ms/batch  6.86 | loss  2.34 | ppl    10.40\n",
      "| epoch  58 |   600/ 1963 batches | lr 2.82 | ms/batch  7.38 | loss  2.32 | ppl    10.15\n",
      "| epoch  58 |   900/ 1963 batches | lr 2.82 | ms/batch  6.92 | loss  2.36 | ppl    10.60\n",
      "| epoch  58 |  1200/ 1963 batches | lr 2.82 | ms/batch  6.48 | loss  2.35 | ppl    10.50\n",
      "| epoch  58 |  1500/ 1963 batches | lr 2.82 | ms/batch  6.31 | loss  2.33 | ppl    10.26\n",
      "| epoch  58 |  1800/ 1963 batches | lr 2.82 | ms/batch  6.64 | loss  2.31 | ppl    10.10\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 13.51s | valid loss  2.35 | valid ppl    10.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |   300/ 1963 batches | lr 2.79 | ms/batch  6.13 | loss  2.33 | ppl    10.23\n",
      "| epoch  59 |   600/ 1963 batches | lr 2.79 | ms/batch  6.18 | loss  2.30 | ppl     9.97\n",
      "| epoch  59 |   900/ 1963 batches | lr 2.79 | ms/batch  6.08 | loss  2.34 | ppl    10.43\n",
      "| epoch  59 |  1200/ 1963 batches | lr 2.79 | ms/batch  6.11 | loss  2.34 | ppl    10.33\n",
      "| epoch  59 |  1500/ 1963 batches | lr 2.79 | ms/batch  6.59 | loss  2.32 | ppl    10.13\n",
      "| epoch  59 |  1800/ 1963 batches | lr 2.79 | ms/batch  6.80 | loss  2.30 | ppl    10.02\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 12.67s | valid loss  2.32 | valid ppl    10.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |   300/ 1963 batches | lr 2.76 | ms/batch  6.68 | loss  2.30 | ppl     9.99\n",
      "| epoch  60 |   600/ 1963 batches | lr 2.76 | ms/batch  6.38 | loss  2.28 | ppl     9.80\n",
      "| epoch  60 |   900/ 1963 batches | lr 2.76 | ms/batch  6.60 | loss  2.33 | ppl    10.30\n",
      "| epoch  60 |  1200/ 1963 batches | lr 2.76 | ms/batch  6.21 | loss  2.33 | ppl    10.24\n",
      "| epoch  60 |  1500/ 1963 batches | lr 2.76 | ms/batch  6.22 | loss  2.29 | ppl     9.90\n",
      "| epoch  60 |  1800/ 1963 batches | lr 2.76 | ms/batch  6.34 | loss  2.29 | ppl     9.87\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 12.81s | valid loss  2.34 | valid ppl    10.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |   300/ 1963 batches | lr 2.74 | ms/batch  6.18 | loss  2.30 | ppl     9.93\n",
      "| epoch  61 |   600/ 1963 batches | lr 2.74 | ms/batch  6.39 | loss  2.27 | ppl     9.68\n",
      "| epoch  61 |   900/ 1963 batches | lr 2.74 | ms/batch  6.50 | loss  2.32 | ppl    10.14\n",
      "| epoch  61 |  1200/ 1963 batches | lr 2.74 | ms/batch  5.99 | loss  2.31 | ppl    10.09\n",
      "| epoch  61 |  1500/ 1963 batches | lr 2.74 | ms/batch  6.08 | loss  2.28 | ppl     9.82\n",
      "| epoch  61 |  1800/ 1963 batches | lr 2.74 | ms/batch  6.11 | loss  2.28 | ppl     9.77\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 12.46s | valid loss  2.29 | valid ppl     9.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  62 |   300/ 1963 batches | lr 2.71 | ms/batch  6.11 | loss  2.29 | ppl     9.84\n",
      "| epoch  62 |   600/ 1963 batches | lr 2.71 | ms/batch  6.09 | loss  2.26 | ppl     9.60\n",
      "| epoch  62 |   900/ 1963 batches | lr 2.71 | ms/batch  6.04 | loss  2.31 | ppl    10.03\n",
      "| epoch  62 |  1200/ 1963 batches | lr 2.71 | ms/batch  6.08 | loss  2.30 | ppl     9.97\n",
      "| epoch  62 |  1500/ 1963 batches | lr 2.71 | ms/batch  6.11 | loss  2.27 | ppl     9.70\n",
      "| epoch  62 |  1800/ 1963 batches | lr 2.71 | ms/batch  6.16 | loss  2.27 | ppl     9.68\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 12.26s | valid loss  2.22 | valid ppl     9.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  63 |   300/ 1963 batches | lr 2.68 | ms/batch  6.29 | loss  2.27 | ppl     9.71\n",
      "| epoch  63 |   600/ 1963 batches | lr 2.68 | ms/batch  6.17 | loss  2.25 | ppl     9.49\n",
      "| epoch  63 |   900/ 1963 batches | lr 2.68 | ms/batch  6.38 | loss  2.30 | ppl     9.93\n",
      "| epoch  63 |  1200/ 1963 batches | lr 2.68 | ms/batch  6.29 | loss  2.28 | ppl     9.77\n",
      "| epoch  63 |  1500/ 1963 batches | lr 2.68 | ms/batch  6.35 | loss  2.26 | ppl     9.57\n",
      "| epoch  63 |  1800/ 1963 batches | lr 2.68 | ms/batch  6.23 | loss  2.26 | ppl     9.60\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 12.54s | valid loss  2.31 | valid ppl    10.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |   300/ 1963 batches | lr 2.65 | ms/batch  6.24 | loss  2.26 | ppl     9.62\n",
      "| epoch  64 |   600/ 1963 batches | lr 2.65 | ms/batch  6.33 | loss  2.24 | ppl     9.40\n",
      "| epoch  64 |   900/ 1963 batches | lr 2.65 | ms/batch  6.31 | loss  2.28 | ppl     9.80\n",
      "| epoch  64 |  1200/ 1963 batches | lr 2.65 | ms/batch  6.64 | loss  2.27 | ppl     9.69\n",
      "| epoch  64 |  1500/ 1963 batches | lr 2.65 | ms/batch  6.13 | loss  2.25 | ppl     9.45\n",
      "| epoch  64 |  1800/ 1963 batches | lr 2.65 | ms/batch  6.21 | loss  2.24 | ppl     9.44\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 12.63s | valid loss  2.35 | valid ppl    10.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |   300/ 1963 batches | lr 2.63 | ms/batch  6.12 | loss  2.25 | ppl     9.52\n",
      "| epoch  65 |   600/ 1963 batches | lr 2.63 | ms/batch  6.04 | loss  2.23 | ppl     9.27\n",
      "| epoch  65 |   900/ 1963 batches | lr 2.63 | ms/batch  6.04 | loss  2.27 | ppl     9.67\n",
      "| epoch  65 |  1200/ 1963 batches | lr 2.63 | ms/batch  6.11 | loss  2.26 | ppl     9.59\n",
      "| epoch  65 |  1500/ 1963 batches | lr 2.63 | ms/batch  6.13 | loss  2.23 | ppl     9.34\n",
      "| epoch  65 |  1800/ 1963 batches | lr 2.63 | ms/batch  6.24 | loss  2.23 | ppl     9.30\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 12.29s | valid loss  2.23 | valid ppl     9.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |   300/ 1963 batches | lr 2.60 | ms/batch  6.56 | loss  2.24 | ppl     9.36\n",
      "| epoch  66 |   600/ 1963 batches | lr 2.60 | ms/batch  6.42 | loss  2.21 | ppl     9.14\n",
      "| epoch  66 |   900/ 1963 batches | lr 2.60 | ms/batch  6.56 | loss  2.26 | ppl     9.59\n",
      "| epoch  66 |  1200/ 1963 batches | lr 2.60 | ms/batch  6.45 | loss  2.25 | ppl     9.50\n",
      "| epoch  66 |  1500/ 1963 batches | lr 2.60 | ms/batch  6.31 | loss  2.22 | ppl     9.24\n",
      "| epoch  66 |  1800/ 1963 batches | lr 2.60 | ms/batch  6.19 | loss  2.22 | ppl     9.24\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 12.84s | valid loss  2.27 | valid ppl     9.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |   300/ 1963 batches | lr 2.58 | ms/batch  6.40 | loss  2.23 | ppl     9.27\n",
      "| epoch  67 |   600/ 1963 batches | lr 2.58 | ms/batch  6.52 | loss  2.21 | ppl     9.08\n",
      "| epoch  67 |   900/ 1963 batches | lr 2.58 | ms/batch  6.33 | loss  2.25 | ppl     9.48\n",
      "| epoch  67 |  1200/ 1963 batches | lr 2.58 | ms/batch  9.75 | loss  2.24 | ppl     9.37\n",
      "| epoch  67 |  1500/ 1963 batches | lr 2.58 | ms/batch  6.38 | loss  2.21 | ppl     9.16\n",
      "| epoch  67 |  1800/ 1963 batches | lr 2.58 | ms/batch  6.21 | loss  2.21 | ppl     9.11\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 13.83s | valid loss  2.22 | valid ppl     9.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |   300/ 1963 batches | lr 2.55 | ms/batch  6.33 | loss  2.21 | ppl     9.13\n",
      "| epoch  68 |   600/ 1963 batches | lr 2.55 | ms/batch  6.44 | loss  2.19 | ppl     8.92\n",
      "| epoch  68 |   900/ 1963 batches | lr 2.55 | ms/batch  6.51 | loss  2.24 | ppl     9.42\n",
      "| epoch  68 |  1200/ 1963 batches | lr 2.55 | ms/batch  6.28 | loss  2.23 | ppl     9.29\n",
      "| epoch  68 |  1500/ 1963 batches | lr 2.55 | ms/batch  6.04 | loss  2.20 | ppl     8.99\n",
      "| epoch  68 |  1800/ 1963 batches | lr 2.55 | ms/batch  6.39 | loss  2.20 | ppl     9.06\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 12.72s | valid loss  2.28 | valid ppl     9.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |   300/ 1963 batches | lr 2.52 | ms/batch  7.03 | loss  2.21 | ppl     9.08\n",
      "| epoch  69 |   600/ 1963 batches | lr 2.52 | ms/batch  7.18 | loss  2.18 | ppl     8.89\n",
      "| epoch  69 |   900/ 1963 batches | lr 2.52 | ms/batch  6.90 | loss  2.23 | ppl     9.28\n",
      "| epoch  69 |  1200/ 1963 batches | lr 2.52 | ms/batch  7.09 | loss  2.22 | ppl     9.21\n",
      "| epoch  69 |  1500/ 1963 batches | lr 2.52 | ms/batch  7.15 | loss  2.20 | ppl     8.99\n",
      "| epoch  69 |  1800/ 1963 batches | lr 2.52 | ms/batch  6.90 | loss  2.19 | ppl     8.93\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 14.08s | valid loss  2.16 | valid ppl     8.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  70 |   300/ 1963 batches | lr 2.50 | ms/batch  6.50 | loss  2.19 | ppl     8.96\n",
      "| epoch  70 |   600/ 1963 batches | lr 2.50 | ms/batch  6.54 | loss  2.17 | ppl     8.73\n",
      "| epoch  70 |   900/ 1963 batches | lr 2.50 | ms/batch  6.48 | loss  2.22 | ppl     9.23\n",
      "| epoch  70 |  1200/ 1963 batches | lr 2.50 | ms/batch  7.04 | loss  2.20 | ppl     9.06\n",
      "| epoch  70 |  1500/ 1963 batches | lr 2.50 | ms/batch  7.45 | loss  2.19 | ppl     8.91\n",
      "| epoch  70 |  1800/ 1963 batches | lr 2.50 | ms/batch  6.58 | loss  2.18 | ppl     8.89\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 13.52s | valid loss  2.28 | valid ppl     9.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 |   300/ 1963 batches | lr 2.47 | ms/batch  7.13 | loss  2.19 | ppl     8.91\n",
      "| epoch  71 |   600/ 1963 batches | lr 2.47 | ms/batch  6.91 | loss  2.15 | ppl     8.62\n",
      "| epoch  71 |   900/ 1963 batches | lr 2.47 | ms/batch  6.81 | loss  2.21 | ppl     9.09\n",
      "| epoch  71 |  1200/ 1963 batches | lr 2.47 | ms/batch  7.10 | loss  2.20 | ppl     9.01\n",
      "| epoch  71 |  1500/ 1963 batches | lr 2.47 | ms/batch  6.67 | loss  2.18 | ppl     8.81\n",
      "| epoch  71 |  1800/ 1963 batches | lr 2.47 | ms/batch  6.59 | loss  2.17 | ppl     8.77\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 13.74s | valid loss  2.20 | valid ppl     9.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 |   300/ 1963 batches | lr 2.45 | ms/batch  8.32 | loss  2.17 | ppl     8.80\n",
      "| epoch  72 |   600/ 1963 batches | lr 2.45 | ms/batch 12.63 | loss  2.15 | ppl     8.58\n",
      "| epoch  72 |   900/ 1963 batches | lr 2.45 | ms/batch  8.50 | loss  2.20 | ppl     9.05\n",
      "| epoch  72 |  1200/ 1963 batches | lr 2.45 | ms/batch  8.01 | loss  2.19 | ppl     8.95\n",
      "| epoch  72 |  1500/ 1963 batches | lr 2.45 | ms/batch  8.03 | loss  2.16 | ppl     8.69\n",
      "| epoch  72 |  1800/ 1963 batches | lr 2.45 | ms/batch  7.26 | loss  2.16 | ppl     8.70\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 17.43s | valid loss  2.18 | valid ppl     8.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  73 |   300/ 1963 batches | lr 2.42 | ms/batch  8.04 | loss  2.17 | ppl     8.75\n",
      "| epoch  73 |   600/ 1963 batches | lr 2.42 | ms/batch  8.05 | loss  2.14 | ppl     8.50\n",
      "| epoch  73 |   900/ 1963 batches | lr 2.42 | ms/batch  8.27 | loss  2.19 | ppl     8.95\n",
      "| epoch  73 |  1200/ 1963 batches | lr 2.42 | ms/batch  8.16 | loss  2.18 | ppl     8.86\n",
      "| epoch  73 |  1500/ 1963 batches | lr 2.42 | ms/batch  8.04 | loss  2.15 | ppl     8.60\n",
      "| epoch  73 |  1800/ 1963 batches | lr 2.42 | ms/batch  8.06 | loss  2.15 | ppl     8.62\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 16.25s | valid loss  2.18 | valid ppl     8.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 |   300/ 1963 batches | lr 2.40 | ms/batch  8.10 | loss  2.16 | ppl     8.65\n",
      "| epoch  74 |   600/ 1963 batches | lr 2.40 | ms/batch  8.01 | loss  2.13 | ppl     8.44\n",
      "| epoch  74 |   900/ 1963 batches | lr 2.40 | ms/batch  8.02 | loss  2.18 | ppl     8.84\n",
      "| epoch  74 |  1200/ 1963 batches | lr 2.40 | ms/batch  8.00 | loss  2.17 | ppl     8.76\n",
      "| epoch  74 |  1500/ 1963 batches | lr 2.40 | ms/batch  8.03 | loss  2.14 | ppl     8.53\n",
      "| epoch  74 |  1800/ 1963 batches | lr 2.40 | ms/batch  6.96 | loss  2.15 | ppl     8.55\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 15.99s | valid loss  2.16 | valid ppl     8.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  75 |   300/ 1963 batches | lr 2.38 | ms/batch  8.11 | loss  2.15 | ppl     8.56\n",
      "| epoch  75 |   600/ 1963 batches | lr 2.38 | ms/batch  8.10 | loss  2.12 | ppl     8.33\n",
      "| epoch  75 |   900/ 1963 batches | lr 2.38 | ms/batch  8.12 | loss  2.17 | ppl     8.78\n",
      "| epoch  75 |  1200/ 1963 batches | lr 2.38 | ms/batch  8.10 | loss  2.16 | ppl     8.71\n",
      "| epoch  75 |  1500/ 1963 batches | lr 2.38 | ms/batch  8.09 | loss  2.14 | ppl     8.51\n",
      "| epoch  75 |  1800/ 1963 batches | lr 2.38 | ms/batch  6.36 | loss  2.14 | ppl     8.46\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 15.32s | valid loss  2.13 | valid ppl     8.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  76 |   300/ 1963 batches | lr 2.35 | ms/batch  6.00 | loss  2.14 | ppl     8.50\n",
      "| epoch  76 |   600/ 1963 batches | lr 2.35 | ms/batch  5.97 | loss  2.11 | ppl     8.24\n",
      "| epoch  76 |   900/ 1963 batches | lr 2.35 | ms/batch  5.98 | loss  2.16 | ppl     8.68\n",
      "| epoch  76 |  1200/ 1963 batches | lr 2.35 | ms/batch  5.99 | loss  2.16 | ppl     8.63\n",
      "| epoch  76 |  1500/ 1963 batches | lr 2.35 | ms/batch  5.98 | loss  2.13 | ppl     8.41\n",
      "| epoch  76 |  1800/ 1963 batches | lr 2.35 | ms/batch  5.98 | loss  2.13 | ppl     8.40\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 12.03s | valid loss  2.15 | valid ppl     8.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 |   300/ 1963 batches | lr 2.33 | ms/batch  6.28 | loss  2.13 | ppl     8.41\n",
      "| epoch  77 |   600/ 1963 batches | lr 2.33 | ms/batch  7.04 | loss  2.10 | ppl     8.20\n",
      "| epoch  77 |   900/ 1963 batches | lr 2.33 | ms/batch  7.04 | loss  2.16 | ppl     8.65\n",
      "| epoch  77 |  1200/ 1963 batches | lr 2.33 | ms/batch  6.70 | loss  2.14 | ppl     8.54\n",
      "| epoch  77 |  1500/ 1963 batches | lr 2.33 | ms/batch  6.50 | loss  2.12 | ppl     8.32\n",
      "| epoch  77 |  1800/ 1963 batches | lr 2.33 | ms/batch  6.53 | loss  2.12 | ppl     8.30\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 13.33s | valid loss  2.17 | valid ppl     8.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 |   300/ 1963 batches | lr 2.31 | ms/batch  6.69 | loss  2.13 | ppl     8.37\n",
      "| epoch  78 |   600/ 1963 batches | lr 2.31 | ms/batch  6.90 | loss  2.10 | ppl     8.14\n",
      "| epoch  78 |   900/ 1963 batches | lr 2.31 | ms/batch  6.77 | loss  2.14 | ppl     8.53\n",
      "| epoch  78 |  1200/ 1963 batches | lr 2.31 | ms/batch  6.75 | loss  2.13 | ppl     8.44\n",
      "| epoch  78 |  1500/ 1963 batches | lr 2.31 | ms/batch  7.31 | loss  2.11 | ppl     8.27\n",
      "| epoch  78 |  1800/ 1963 batches | lr 2.31 | ms/batch  6.84 | loss  2.11 | ppl     8.24\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 13.67s | valid loss  2.09 | valid ppl     8.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  79 |   300/ 1963 batches | lr 2.28 | ms/batch  6.36 | loss  2.12 | ppl     8.30\n",
      "| epoch  79 |   600/ 1963 batches | lr 2.28 | ms/batch  6.14 | loss  2.09 | ppl     8.10\n",
      "| epoch  79 |   900/ 1963 batches | lr 2.28 | ms/batch  6.19 | loss  2.14 | ppl     8.51\n",
      "| epoch  79 |  1200/ 1963 batches | lr 2.28 | ms/batch  6.22 | loss  2.13 | ppl     8.44\n",
      "| epoch  79 |  1500/ 1963 batches | lr 2.28 | ms/batch  6.33 | loss  2.10 | ppl     8.20\n",
      "| epoch  79 |  1800/ 1963 batches | lr 2.28 | ms/batch  6.70 | loss  2.10 | ppl     8.19\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 12.85s | valid loss  2.15 | valid ppl     8.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 |   300/ 1963 batches | lr 2.26 | ms/batch  6.90 | loss  2.11 | ppl     8.22\n",
      "| epoch  80 |   600/ 1963 batches | lr 2.26 | ms/batch  7.03 | loss  2.08 | ppl     8.00\n",
      "| epoch  80 |   900/ 1963 batches | lr 2.26 | ms/batch  6.67 | loss  2.13 | ppl     8.43\n",
      "| epoch  80 |  1200/ 1963 batches | lr 2.26 | ms/batch  8.78 | loss  2.12 | ppl     8.33\n",
      "| epoch  80 |  1500/ 1963 batches | lr 2.26 | ms/batch  6.60 | loss  2.10 | ppl     8.16\n",
      "| epoch  80 |  1800/ 1963 batches | lr 2.26 | ms/batch  6.44 | loss  2.09 | ppl     8.10\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 14.02s | valid loss  2.07 | valid ppl     7.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  81 |   300/ 1963 batches | lr 2.24 | ms/batch  6.28 | loss  2.10 | ppl     8.16\n",
      "| epoch  81 |   600/ 1963 batches | lr 2.24 | ms/batch  9.61 | loss  2.07 | ppl     7.93\n",
      "| epoch  81 |   900/ 1963 batches | lr 2.24 | ms/batch  6.41 | loss  2.13 | ppl     8.38\n",
      "| epoch  81 |  1200/ 1963 batches | lr 2.24 | ms/batch  6.25 | loss  2.12 | ppl     8.29\n",
      "| epoch  81 |  1500/ 1963 batches | lr 2.24 | ms/batch  6.23 | loss  2.09 | ppl     8.11\n",
      "| epoch  81 |  1800/ 1963 batches | lr 2.24 | ms/batch  9.95 | loss  2.09 | ppl     8.06\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 16.13s | valid loss  2.09 | valid ppl     8.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 |   300/ 1963 batches | lr 2.22 | ms/batch 11.13 | loss  2.09 | ppl     8.07\n",
      "| epoch  82 |   600/ 1963 batches | lr 2.22 | ms/batch 11.92 | loss  2.06 | ppl     7.88\n",
      "| epoch  82 |   900/ 1963 batches | lr 2.22 | ms/batch  7.84 | loss  2.12 | ppl     8.29\n",
      "| epoch  82 |  1200/ 1963 batches | lr 2.22 | ms/batch  6.15 | loss  2.11 | ppl     8.23\n",
      "| epoch  82 |  1500/ 1963 batches | lr 2.22 | ms/batch  6.66 | loss  2.08 | ppl     8.03\n",
      "| epoch  82 |  1800/ 1963 batches | lr 2.22 | ms/batch  6.98 | loss  2.08 | ppl     7.99\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 16.50s | valid loss  2.10 | valid ppl     8.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 |   300/ 1963 batches | lr 2.19 | ms/batch  6.09 | loss  2.08 | ppl     8.04\n",
      "| epoch  83 |   600/ 1963 batches | lr 2.19 | ms/batch  6.03 | loss  2.05 | ppl     7.79\n",
      "| epoch  83 |   900/ 1963 batches | lr 2.19 | ms/batch  6.03 | loss  2.11 | ppl     8.26\n",
      "| epoch  83 |  1200/ 1963 batches | lr 2.19 | ms/batch  6.78 | loss  2.10 | ppl     8.16\n",
      "| epoch  83 |  1500/ 1963 batches | lr 2.19 | ms/batch  6.45 | loss  2.07 | ppl     7.95\n",
      "| epoch  83 |  1800/ 1963 batches | lr 2.19 | ms/batch  6.25 | loss  2.07 | ppl     7.96\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 12.58s | valid loss  2.13 | valid ppl     8.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 |   300/ 1963 batches | lr 2.17 | ms/batch  6.37 | loss  2.08 | ppl     8.01\n",
      "| epoch  84 |   600/ 1963 batches | lr 2.17 | ms/batch  6.67 | loss  2.05 | ppl     7.74\n",
      "| epoch  84 |   900/ 1963 batches | lr 2.17 | ms/batch  6.83 | loss  2.10 | ppl     8.17\n",
      "| epoch  84 |  1200/ 1963 batches | lr 2.17 | ms/batch  6.38 | loss  2.09 | ppl     8.10\n",
      "| epoch  84 |  1500/ 1963 batches | lr 2.17 | ms/batch  6.42 | loss  2.07 | ppl     7.89\n",
      "| epoch  84 |  1800/ 1963 batches | lr 2.17 | ms/batch  6.44 | loss  2.07 | ppl     7.89\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 13.05s | valid loss  2.09 | valid ppl     8.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 |   300/ 1963 batches | lr 2.15 | ms/batch  6.49 | loss  2.07 | ppl     7.94\n",
      "| epoch  85 |   600/ 1963 batches | lr 2.15 | ms/batch  6.32 | loss  2.04 | ppl     7.69\n",
      "| epoch  85 |   900/ 1963 batches | lr 2.15 | ms/batch  6.16 | loss  2.09 | ppl     8.09\n",
      "| epoch  85 |  1200/ 1963 batches | lr 2.15 | ms/batch  6.16 | loss  2.08 | ppl     8.04\n",
      "| epoch  85 |  1500/ 1963 batches | lr 2.15 | ms/batch  6.96 | loss  2.06 | ppl     7.84\n",
      "| epoch  85 |  1800/ 1963 batches | lr 2.15 | ms/batch 10.75 | loss  2.06 | ppl     7.86\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 14.16s | valid loss  2.02 | valid ppl     7.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  86 |   300/ 1963 batches | lr 2.13 | ms/batch  6.38 | loss  2.06 | ppl     7.88\n",
      "| epoch  86 |   600/ 1963 batches | lr 2.13 | ms/batch  6.40 | loss  2.04 | ppl     7.66\n",
      "| epoch  86 |   900/ 1963 batches | lr 2.13 | ms/batch  6.25 | loss  2.09 | ppl     8.06\n",
      "| epoch  86 |  1200/ 1963 batches | lr 2.13 | ms/batch  6.31 | loss  2.08 | ppl     7.98\n",
      "| epoch  86 |  1500/ 1963 batches | lr 2.13 | ms/batch  6.42 | loss  2.05 | ppl     7.78\n",
      "| epoch  86 |  1800/ 1963 batches | lr 2.13 | ms/batch  6.29 | loss  2.05 | ppl     7.80\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 12.69s | valid loss  2.05 | valid ppl     7.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 |   300/ 1963 batches | lr 2.11 | ms/batch  6.05 | loss  2.05 | ppl     7.81\n",
      "| epoch  87 |   600/ 1963 batches | lr 2.11 | ms/batch  6.03 | loss  2.03 | ppl     7.60\n",
      "| epoch  87 |   900/ 1963 batches | lr 2.11 | ms/batch 10.76 | loss  2.08 | ppl     8.02\n",
      "| epoch  87 |  1200/ 1963 batches | lr 2.11 | ms/batch  7.89 | loss  2.07 | ppl     7.95\n",
      "| epoch  87 |  1500/ 1963 batches | lr 2.11 | ms/batch  5.97 | loss  2.05 | ppl     7.75\n",
      "| epoch  87 |  1800/ 1963 batches | lr 2.11 | ms/batch  5.97 | loss  2.05 | ppl     7.77\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 14.03s | valid loss  2.00 | valid ppl     7.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  88 |   300/ 1963 batches | lr 2.09 | ms/batch  6.00 | loss  2.05 | ppl     7.77\n",
      "| epoch  88 |   600/ 1963 batches | lr 2.09 | ms/batch  5.97 | loss  2.02 | ppl     7.56\n",
      "| epoch  88 |   900/ 1963 batches | lr 2.09 | ms/batch  5.97 | loss  2.07 | ppl     7.95\n",
      "| epoch  88 |  1200/ 1963 batches | lr 2.09 | ms/batch  5.97 | loss  2.07 | ppl     7.90\n",
      "| epoch  88 |  1500/ 1963 batches | lr 2.09 | ms/batch  5.97 | loss  2.04 | ppl     7.71\n",
      "| epoch  88 |  1800/ 1963 batches | lr 2.09 | ms/batch  5.98 | loss  2.04 | ppl     7.71\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 11.98s | valid loss  2.05 | valid ppl     7.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 |   300/ 1963 batches | lr 2.06 | ms/batch  6.00 | loss  2.04 | ppl     7.72\n",
      "| epoch  89 |   600/ 1963 batches | lr 2.06 | ms/batch  5.97 | loss  2.02 | ppl     7.51\n",
      "| epoch  89 |   900/ 1963 batches | lr 2.06 | ms/batch  5.98 | loss  2.07 | ppl     7.93\n",
      "| epoch  89 |  1200/ 1963 batches | lr 2.06 | ms/batch  5.97 | loss  2.06 | ppl     7.85\n",
      "| epoch  89 |  1500/ 1963 batches | lr 2.06 | ms/batch  5.98 | loss  2.04 | ppl     7.66\n",
      "| epoch  89 |  1800/ 1963 batches | lr 2.06 | ms/batch  5.97 | loss  2.04 | ppl     7.65\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 11.99s | valid loss  2.09 | valid ppl     8.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 |   300/ 1963 batches | lr 2.04 | ms/batch  6.00 | loss  2.04 | ppl     7.71\n",
      "| epoch  90 |   600/ 1963 batches | lr 2.04 | ms/batch  6.60 | loss  2.01 | ppl     7.45\n",
      "| epoch  90 |   900/ 1963 batches | lr 2.04 | ms/batch  9.84 | loss  2.06 | ppl     7.84\n",
      "| epoch  90 |  1200/ 1963 batches | lr 2.04 | ms/batch  7.94 | loss  2.05 | ppl     7.80\n",
      "| epoch  90 |  1500/ 1963 batches | lr 2.04 | ms/batch  6.01 | loss  2.03 | ppl     7.62\n",
      "| epoch  90 |  1800/ 1963 batches | lr 2.04 | ms/batch  6.09 | loss  2.03 | ppl     7.62\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 13.97s | valid loss  2.05 | valid ppl     7.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 |   300/ 1963 batches | lr 2.02 | ms/batch  6.18 | loss  2.04 | ppl     7.67\n",
      "| epoch  91 |   600/ 1963 batches | lr 2.02 | ms/batch  5.98 | loss  2.00 | ppl     7.40\n",
      "| epoch  91 |   900/ 1963 batches | lr 2.02 | ms/batch  5.98 | loss  2.06 | ppl     7.81\n",
      "| epoch  91 |  1200/ 1963 batches | lr 2.02 | ms/batch  5.98 | loss  2.05 | ppl     7.76\n",
      "| epoch  91 |  1500/ 1963 batches | lr 2.02 | ms/batch  5.98 | loss  2.02 | ppl     7.57\n",
      "| epoch  91 |  1800/ 1963 batches | lr 2.02 | ms/batch  6.02 | loss  2.03 | ppl     7.59\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 12.07s | valid loss  2.03 | valid ppl     7.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 |   300/ 1963 batches | lr 2.00 | ms/batch  6.08 | loss  2.03 | ppl     7.60\n",
      "| epoch  92 |   600/ 1963 batches | lr 2.00 | ms/batch  6.08 | loss  2.00 | ppl     7.38\n",
      "| epoch  92 |   900/ 1963 batches | lr 2.00 | ms/batch  6.12 | loss  2.05 | ppl     7.78\n",
      "| epoch  92 |  1200/ 1963 batches | lr 2.00 | ms/batch  6.32 | loss  2.05 | ppl     7.75\n",
      "| epoch  92 |  1500/ 1963 batches | lr 2.00 | ms/batch  6.07 | loss  2.02 | ppl     7.52\n",
      "| epoch  92 |  1800/ 1963 batches | lr 2.00 | ms/batch  6.02 | loss  2.02 | ppl     7.51\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 12.24s | valid loss  1.98 | valid ppl     7.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  93 |   300/ 1963 batches | lr 1.98 | ms/batch  6.04 | loss  2.02 | ppl     7.55\n",
      "| epoch  93 |   600/ 1963 batches | lr 1.98 | ms/batch  5.98 | loss  1.99 | ppl     7.33\n",
      "| epoch  93 |   900/ 1963 batches | lr 1.98 | ms/batch  6.12 | loss  2.05 | ppl     7.77\n",
      "| epoch  93 |  1200/ 1963 batches | lr 1.98 | ms/batch  6.09 | loss  2.04 | ppl     7.71\n",
      "| epoch  93 |  1500/ 1963 batches | lr 1.98 | ms/batch  7.63 | loss  2.01 | ppl     7.48\n",
      "| epoch  93 |  1800/ 1963 batches | lr 1.98 | ms/batch  6.05 | loss  2.01 | ppl     7.47\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 12.61s | valid loss  2.06 | valid ppl     7.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 |   300/ 1963 batches | lr 1.96 | ms/batch  6.00 | loss  2.02 | ppl     7.51\n",
      "| epoch  94 |   600/ 1963 batches | lr 1.96 | ms/batch  5.98 | loss  1.99 | ppl     7.29\n",
      "| epoch  94 |   900/ 1963 batches | lr 1.96 | ms/batch  5.98 | loss  2.04 | ppl     7.69\n",
      "| epoch  94 |  1200/ 1963 batches | lr 1.96 | ms/batch  5.97 | loss  2.03 | ppl     7.65\n",
      "| epoch  94 |  1500/ 1963 batches | lr 1.96 | ms/batch  5.97 | loss  2.01 | ppl     7.46\n",
      "| epoch  94 |  1800/ 1963 batches | lr 1.96 | ms/batch  5.98 | loss  2.01 | ppl     7.43\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 11.99s | valid loss  2.00 | valid ppl     7.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 |   300/ 1963 batches | lr 1.94 | ms/batch  6.00 | loss  2.01 | ppl     7.47\n",
      "| epoch  95 |   600/ 1963 batches | lr 1.94 | ms/batch  5.98 | loss  1.98 | ppl     7.26\n",
      "| epoch  95 |   900/ 1963 batches | lr 1.94 | ms/batch  5.97 | loss  2.04 | ppl     7.66\n",
      "| epoch  95 |  1200/ 1963 batches | lr 1.94 | ms/batch  5.97 | loss  2.03 | ppl     7.61\n",
      "| epoch  95 |  1500/ 1963 batches | lr 1.94 | ms/batch  5.97 | loss  2.00 | ppl     7.42\n",
      "| epoch  95 |  1800/ 1963 batches | lr 1.94 | ms/batch  5.97 | loss  2.00 | ppl     7.41\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 11.99s | valid loss  1.99 | valid ppl     7.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 |   300/ 1963 batches | lr 1.92 | ms/batch  6.00 | loss  2.01 | ppl     7.43\n",
      "| epoch  96 |   600/ 1963 batches | lr 1.92 | ms/batch  5.98 | loss  1.98 | ppl     7.21\n",
      "| epoch  96 |   900/ 1963 batches | lr 1.92 | ms/batch  5.99 | loss  2.03 | ppl     7.60\n",
      "| epoch  96 |  1200/ 1963 batches | lr 1.92 | ms/batch  6.06 | loss  2.02 | ppl     7.57\n",
      "| epoch  96 |  1500/ 1963 batches | lr 1.92 | ms/batch  6.02 | loss  2.00 | ppl     7.36\n",
      "| epoch  96 |  1800/ 1963 batches | lr 1.92 | ms/batch  6.16 | loss  2.00 | ppl     7.37\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 12.18s | valid loss  1.96 | valid ppl     7.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "| epoch  97 |   300/ 1963 batches | lr 1.91 | ms/batch  6.31 | loss  2.00 | ppl     7.40\n",
      "| epoch  97 |   600/ 1963 batches | lr 1.91 | ms/batch  6.27 | loss  1.97 | ppl     7.19\n",
      "| epoch  97 |   900/ 1963 batches | lr 1.91 | ms/batch  6.22 | loss  2.02 | ppl     7.55\n",
      "| epoch  97 |  1200/ 1963 batches | lr 1.91 | ms/batch  6.18 | loss  2.02 | ppl     7.52\n",
      "| epoch  97 |  1500/ 1963 batches | lr 1.91 | ms/batch  6.20 | loss  1.99 | ppl     7.33\n",
      "| epoch  97 |  1800/ 1963 batches | lr 1.91 | ms/batch  6.05 | loss  1.99 | ppl     7.34\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 12.45s | valid loss  2.00 | valid ppl     7.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  98 |   300/ 1963 batches | lr 1.89 | ms/batch 18.12 | loss  2.00 | ppl     7.37\n",
      "| epoch  98 |   600/ 1963 batches | lr 1.89 | ms/batch 20.39 | loss  1.97 | ppl     7.14\n",
      "| epoch  98 |   900/ 1963 batches | lr 1.89 | ms/batch 10.99 | loss  2.02 | ppl     7.52\n",
      "| epoch  98 |  1200/ 1963 batches | lr 1.89 | ms/batch  7.58 | loss  2.02 | ppl     7.53\n",
      "| epoch  98 |  1500/ 1963 batches | lr 1.89 | ms/batch  9.77 | loss  1.99 | ppl     7.32\n",
      "| epoch  98 |  1800/ 1963 batches | lr 1.89 | ms/batch  8.98 | loss  1.99 | ppl     7.29\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 24.60s | valid loss  2.00 | valid ppl     7.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 |   300/ 1963 batches | lr 1.87 | ms/batch  9.10 | loss  1.99 | ppl     7.34\n",
      "| epoch  99 |   600/ 1963 batches | lr 1.87 | ms/batch  9.37 | loss  1.96 | ppl     7.09\n",
      "| epoch  99 |   900/ 1963 batches | lr 1.87 | ms/batch 11.75 | loss  2.02 | ppl     7.52\n",
      "| epoch  99 |  1200/ 1963 batches | lr 1.87 | ms/batch 11.88 | loss  2.01 | ppl     7.47\n",
      "| epoch  99 |  1500/ 1963 batches | lr 1.87 | ms/batch  6.39 | loss  1.99 | ppl     7.28\n",
      "| epoch  99 |  1800/ 1963 batches | lr 1.87 | ms/batch  6.14 | loss  1.98 | ppl     7.25\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 18.71s | valid loss  1.98 | valid ppl     7.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 100 |   300/ 1963 batches | lr 1.85 | ms/batch 13.78 | loss  1.99 | ppl     7.29\n",
      "| epoch 100 |   600/ 1963 batches | lr 1.85 | ms/batch 10.51 | loss  1.95 | ppl     7.06\n",
      "| epoch 100 |   900/ 1963 batches | lr 1.85 | ms/batch 10.34 | loss  2.01 | ppl     7.46\n",
      "| epoch 100 |  1200/ 1963 batches | lr 1.85 | ms/batch 11.21 | loss  2.01 | ppl     7.46\n",
      "| epoch 100 |  1500/ 1963 batches | lr 1.85 | ms/batch 11.36 | loss  1.98 | ppl     7.21\n",
      "| epoch 100 |  1800/ 1963 batches | lr 1.85 | ms/batch  8.65 | loss  1.98 | ppl     7.22\n",
      "TRAIN done.\n",
      "\n",
      "EVAL done.\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 21.64s | valid loss  1.94 | valid ppl     6.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the newest best model...\n",
      "\n",
      "Reloading the overall best model...\n",
      "\n",
      "Saving the overall best model to permanent location...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 10\n",
    "\n",
    "### DEBUG ###\n",
    "print(\"BEFORE load\", model.token_neural_map.unique(dim=0).shape, end=\"\\n\\n\")\n",
    "### DEBUG ###\n",
    "\n",
    "# Load the previously saved best model checkpoint if it exists\n",
    "final_model_params_path = os.path.join(\"../models/\", \"neural_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\\n\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "    best_val_loss = evaluate(model)\n",
    "    print(f\"Previous validation loss: \\t {best_val_loss:5.2f}\\n\")\n",
    "\n",
    "    ### DEBUG ###\n",
    "    print(\"AFTER load\", model.token_neural_map.unique(dim=0).shape, end=\"\\n\\n\")\n",
    "    print(\n",
    "        \"NOTE: The neural data embedding used to make the datasets may be different now than when the model was saved.\"\n",
    "    )\n",
    "    ### DEBUG ###\n",
    "\n",
    "# Train the model\n",
    "if epochs > 0:\n",
    "    print(f\"Training for {epochs} epoch(s)...\\n\")\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            train(model)\n",
    "            print(f\"TRAIN done.\\n\")\n",
    "            val_loss = evaluate(model)\n",
    "            print(f\"EVAL done.\\n\")\n",
    "            try:\n",
    "                val_ppl = math.exp(val_loss)\n",
    "            except OverflowError:\n",
    "                val_ppl = float(\"inf\")\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            print(\"-\" * 89)\n",
    "            print(\n",
    "                f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "                f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "            )\n",
    "            print(\"-\" * 89)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                print(f\"Saving the newest best model...\\n\")\n",
    "                torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        if os.path.exists(best_model_params_path):\n",
    "            print(f\"Reloading the overall best model...\\n\")\n",
    "            model.load_state_dict(torch.load(best_model_params_path))  # reload the final best model\n",
    "            print(f\"Saving the overall best model to permanent location...\\n\")\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                final_model_params_path,\n",
    "            )  # save the final best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "\n",
      "True None None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "\n",
    "print(len(model.token_neural_map.unique(dim=0)) - 1, end=\"\\n\\n\")\n",
    "print(model.codebook.requires_grad, model.codebook.grad, model.codebook.grad_fn, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 302]) torch.Size([1, 510, 302]) torch.Size([1, 100, 302]) torch.Size([256, 302])\n",
      "\n",
      "tensor([ 0.5456, -0.4229, -0.4225, -0.3058, -0.6988], device='cuda:0')\n",
      "tensor([-0.3760, -0.2043,  0.4550, -0.3093, -0.7649], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# @title Generate new data\n",
    "\n",
    "data = test_dataset[0][:-1, :].unsqueeze(0).to(DEVICE)\n",
    "mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0).to(DEVICE)\n",
    "max_new_tokens = 100\n",
    "\n",
    "### DEBUG ###\n",
    "b1 = model.token_neural_map.detach().clone()\n",
    "b2 = model.codebook.detach().clone()\n",
    "### DEBUG ###\n",
    "\n",
    "data_gen = model.generate(data, mask, max_new_tokens, autoregressive=True, top_k=None)\n",
    "print(mask.shape, data.shape, data_gen.shape, embedding.weight.shape, end=\"\\n\\n\")\n",
    "\n",
    "### DEBUG ###\n",
    "a1 = model.token_neural_map.detach().clone()\n",
    "a2 = model.codebook.detach().clone()\n",
    "assert torch.allclose(b1, a1) and torch.allclose(b2, a2), \"Why the changes?\"\n",
    "print(data.squeeze()[-1][:5])\n",
    "print(data_gen.squeeze()[0][:5])\n",
    "### DEBUG ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117, 100, 113, 102, 104, 35, 119, 100, 42, 104, 113, 35, 68, 118, 35, 118, 107, 100, 111, 111, 35, 122, 108, 119, 107, 35, 104, 108, 119, 107, 104, 117, 35, 115, 100, 117, 119, 42, 118, 35, 100, 106, 117, 104, 104, 112, 104, 113, 119, 35, 118, 119, 100, 113, 103, 66, 35, 69, 68, 83, 87, 76, 86, 87, 68, 61, 35, 81, 114, 119, 35, 108, 113, 35, 112, 124, 35, 107, 114, 120, 118, 104, 47, 35, 79, 120, 102, 104, 113, 119, 108, 114, 62, 35, 105, 114, 117, 47, 35, 124, 114, 120, 35, 110, 113, 114, 122, 47, 35, 83, 108, 119, 102, 107, 104, 117, 118, 35, 107, 100, 121, 104, 35, 104, 100, 117, 118, 47, 35, 100, 113, 103, 35, 76, 35, 107, 100, 121, 104, 35, 112, 100, 113, 124, 35, 118, 104, 117, 121, 100, 113, 119, 118, 61, 35, 69, 104, 118, 108, 103, 104, 118, 47, 35, 114, 111, 103, 35, 74, 117, 104, 112, 108, 114, 35, 108, 118, 35, 107, 104, 100, 117, 110, 104, 113, 108, 113, 106, 35, 118, 119, 108, 111, 111, 62, 35, 68, 113, 103, 35, 107, 100, 115, 115, 108, 111, 124, 35, 122, 104, 35, 112, 108, 106, 107, 119, 35, 101, 104, 35, 108, 113, 119, 104, 117, 117, 120, 115, 119, 104, 103, 49, 35, 87, 85, 68, 81, 76, 82, 61, 35, 87, 107, 104, 113, 35, 100, 119, 35, 112, 124, 35, 111, 114, 103, 106, 108, 113, 106, 47, 35, 100, 113, 35, 108, 119, 35, 111, 108, 110, 104, 35, 124, 114, 120, 61, 35, 87, 107, 104, 117, 104, 35, 103, 114, 119, 107, 35, 112, 124, 35, 105, 100, 119, 107, 104, 117, 35, 111, 108, 104, 62, 35, 100, 113, 103, 35, 119, 107, 104, 117, 104, 47, 35, 119, 107, 108, 118, 35, 113, 108, 106, 107, 119, 47, 35, 90, 104, 42, 111, 111, 35, 115, 100, 118, 118, 35, 119, 107, 104, 35, 101, 120, 118, 108, 113, 104, 118, 118, 35, 115, 117, 108, 121, 100, 119, 104, 111, 124, 35, 100, 113, 103, 35, 122, 104, 111, 111, 49, 35, 86, 104, 113, 103, 35, 105, 114, 117, 35, 124, 114, 120, 117, 35, 103, 100, 120, 106, 107, 119, 104, 117, 35, 101, 124, 35, 124, 114, 120, 117, 35, 118, 104, 117, 121, 100, 113, 119, 35, 107, 104, 117, 104, 61, 35, 80, 124, 35, 101, 114, 124, 35, 118, 107, 100, 111, 111, 35, 105, 104, 119, 102, 107, 35, 119, 107, 104, 35, 118, 102, 117, 108, 121, 104, 113, 104, 117, 35, 115, 117, 104, 118, 104, 113, 119, 111, 124, 49, 35, 87, 107, 104, 35, 122, 114, 117, 118, 119, 35, 108, 118, 35, 119, 107, 108, 118, 47, 35, 119, 107, 100, 119, 47, 35, 100, 119, 35, 118, 114, 35, 118, 111, 104, 113, 103, 104, 117, 35, 122, 100, 117, 113, 108, 113, 106, 47, 35, 92, 114, 120, 1]\n",
      "\n",
      "rance ta'en As shall with either part's agreement stand? BAPTISTA: Not in my house, Lucentio; for, you know, Pitchers have ears, and I have many servants: Besides, old Gremio is hearkening still; And happily we might be interrupted. TRANIO: Then at my lodging, an it like you: There doth my father lie; and there, this night, We'll pass the business privately and well. Send for your daughter by your servant here: My boy shall fetch the scrivener presently. The worst is this, that, at so slender warning, You\n",
      "\n",
      "[117, 100, 113, 102, 104, 35, 119, 100, 42, 104, 113, 35, 68, 118, 35, 118, 107, 100, 111, 111, 35, 122, 108, 119, 107, 35, 104, 108, 119, 107, 104, 117, 35, 115, 100, 117, 119, 42, 118, 35, 100, 106, 117, 104, 104, 112, 104, 113, 119, 35, 118, 119, 100, 113, 103, 66, 35, 69, 68, 83, 87, 76, 86, 87, 68, 61, 35, 81, 114, 119, 35, 108, 113, 35, 112, 124, 35, 107, 114, 120, 118, 104, 47, 35, 79, 120, 102, 104, 113, 119, 108, 114, 62, 35, 105, 114, 117, 47, 35, 124, 114, 120, 35, 110, 113, 114, 122, 47, 35, 83, 108, 119, 102, 107, 104, 117, 118, 35, 107, 100, 121, 104, 35, 104, 100, 117, 118, 47, 35, 100, 113, 103, 35, 76, 35, 107, 100, 121, 104, 35, 112, 100, 113, 124, 35, 118, 104, 117, 121, 100, 113, 119, 118, 61, 35, 69, 104, 118, 108, 103, 104, 118, 47, 35, 114, 111, 103, 35, 74, 117, 104, 112, 108, 114, 35, 108, 118, 35, 107, 104, 100, 117, 110, 104, 113, 108, 113, 106, 35, 118, 119, 108, 111, 111, 62, 35, 68, 113, 103, 35, 107, 100, 115, 115, 108, 111, 124, 35, 122, 104, 35, 112, 108, 106, 107, 119, 35, 101, 104, 35, 108, 113, 119, 104, 117, 117, 120, 115, 119, 104, 103, 49, 35, 87, 85, 68, 81, 76, 82, 61, 35, 87, 107, 104, 113, 35, 100, 119, 35, 112, 124, 35, 111, 114, 103, 106, 108, 113, 106, 47, 35, 100, 113, 35, 108, 119, 35, 111, 108, 110, 104, 35, 124, 114, 120, 61, 35, 87, 107, 104, 117, 104, 35, 103, 114, 119, 107, 35, 112, 124, 35, 105, 100, 119, 107, 104, 117, 35, 111, 108, 104, 62, 35, 100, 113, 103, 35, 119, 107, 104, 117, 104, 47, 35, 119, 107, 108, 118, 35, 113, 108, 106, 107, 119, 47, 35, 90, 104, 42, 111, 111, 35, 115, 100, 118, 118, 35, 119, 107, 104, 35, 101, 120, 118, 108, 113, 104, 118, 118, 35, 115, 117, 108, 121, 100, 119, 104, 111, 124, 35, 100, 113, 103, 35, 122, 104, 111, 111, 49, 35, 86, 104, 113, 103, 35, 105, 114, 117, 35, 124, 114, 120, 117, 35, 103, 100, 120, 106, 107, 119, 104, 117, 35, 101, 124, 35, 124, 114, 120, 117, 35, 118, 104, 117, 121, 100, 113, 119, 35, 107, 104, 117, 104, 61, 35, 80, 124, 35, 101, 114, 124, 35, 118, 107, 100, 111, 111, 35, 105, 104, 119, 102, 107, 35, 119, 107, 104, 35, 118, 102, 117, 108, 121, 104, 113, 104, 117, 35, 115, 117, 104, 118, 104, 113, 119, 111, 124, 49, 35, 87, 107, 104, 35, 122, 114, 117, 118, 119, 35, 108, 118, 35, 119, 107, 108, 118, 47, 35, 119, 107, 100, 119, 47, 35, 100, 119, 35, 118, 114, 35, 118, 111, 104, 113, 103, 104, 117, 35, 122, 100, 117, 113, 108, 113, 106, 47, 35, 92, 114, 120]\n",
      "\n",
      "rance ta'en As shall with either part's agreement stand? BAPTISTA: Not in my house, Lucentio; for, you know, Pitchers have ears, and I have many servants: Besides, old Gremio is hearkening still; And happily we might be interrupted. TRANIO: Then at my lodging, an it like you: There doth my father lie; and there, this night, We'll pass the business privately and well. Send for your daughter by your servant here: My boy shall fetch the scrivener presently. The worst is this, that, at so slender warning, You\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @markdown We want to tokenize the neural data.\n",
    "# An oracle told us that the neural data itself is an embedding of tokens from some unknown vocabulary.\n",
    "# We can do this by using the tokenize_neural_data method of our model.\n",
    "\n",
    "# First run a test on data we know what the true token output should be.\n",
    "# This is just to confirm if our tokenize_neural_data method is working as expected.\n",
    "with torch.no_grad():\n",
    "    sequence = data\n",
    "    inp_tokens = model.tokenize_neural_data(\n",
    "        neural_sequence=data,\n",
    "        feature_mask=mask,\n",
    "        token_matrix=embedding.weight,\n",
    "    )\n",
    "    # iff correct these two should match\n",
    "    print(text_dataset[\"test\"][\"input_ids\"][0], end=\"\\n\\n\")  # ground-truth tokens\n",
    "    print(text_dataset[\"test\"][\"text\"][0], end=\"\\n\\n\")  # ground-truth text\n",
    "    print(inp_tokens.squeeze().tolist(), end=\"\\n\\n\")  # decoded tokens\n",
    "    print(tokenizer.decode(inp_tokens.squeeze().tolist()), end=\"\\n\\n\")  # decoded text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115, 118, 100, 90, 100, 113, 103, 100, 113, 100, 107, 100, 100, 119, 100, 100, 100, 107, 100, 113, 35, 107, 100, 100, 118, 104, 100, 113, 100, 118, 100, 100, 113, 105, 100, 118, 35, 76, 100, 118, 100, 107, 100, 115, 118, 100, 100, 107, 100, 119, 100, 100, 100, 100, 100, 103, 100, 100, 100, 100, 102, 100, 105, 100, 119, 100, 100, 118, 100, 100, 107, 100, 100, 100, 100, 118, 100, 100, 118, 119, 100, 100, 119, 100, 100, 107, 100, 100, 119, 100, 118, 100, 100, 118, 100, 100, 113, 66, 100, 118]\n",
      "\n",
      "psaWandanahaataaahan haaseanasaanfas Iasahapsaahataaaaadaaaacafataasaahaaaasaastaataahaatasaasaan?as\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the same thing on the newly generated data\n",
    "with torch.no_grad():\n",
    "    sequence = data_gen\n",
    "    gen_tokens = model.tokenize_neural_data(\n",
    "        neural_sequence=sequence,\n",
    "        feature_mask=mask,\n",
    "        token_matrix=embedding.weight,\n",
    "    )\n",
    "    print(gen_tokens.squeeze().tolist(), end=\"\\n\\n\")  # decoded tokens\n",
    "    print(tokenizer.decode(gen_tokens.squeeze().tolist()), end=\"\\n\\n\")  # decoded text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 \t {35, 36, 39, 41, 42, 47, 48, 49, 54, 61, 62, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125}\n",
      "\t !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      " ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "idx: 0 \n",
      "neural: tensor([-0.3760, -0.2043,  0.4550, -0.3093, -0.7649], device='cuda:0')\n",
      "\n",
      "token index, character, in train set: 151  False\n",
      "\n",
      "embedded: tensor([ 0.0656, -1.3435, -0.9762, -1.7471, -0.7190])\n",
      "\n",
      "mapped: tensor([-0.3760, -0.2043,  0.4550, -0.3093, -0.7649], device='cuda:0')\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "idx: 1 \n",
      "neural: tensor([ 0.1563,  0.1486, -1.6687,  1.7234,  0.0342], device='cuda:0')\n",
      "\n",
      "token index, character, in train set: 252  False\n",
      "\n",
      "embedded: tensor([-0.7748, -0.4404,  0.0709, -1.9464,  1.2193])\n",
      "\n",
      "mapped: tensor([ 0.1563,  0.1486, -1.6687,  1.7234,  0.0342], device='cuda:0')\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "idx: 2 \n",
      "neural: tensor([ 0.3993,  0.5163,  1.6017, -1.6191,  0.1428], device='cuda:0')\n",
      "\n",
      "token index, character, in train set: 137  False\n",
      "\n",
      "embedded: tensor([-0.2330,  0.1486, -0.1259,  0.7037, -0.4953])\n",
      "\n",
      "mapped: tensor([ 0.3993,  0.5163,  1.6017, -1.6191,  0.1428], device='cuda:0')\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "idx: 3 \n",
      "neural: tensor([-0.3391,  1.0223,  0.8561,  0.0971,  0.0793], device='cuda:0')\n",
      "\n",
      "token index, character, in train set: 15 \f False\n",
      "\n",
      "embedded: tensor([-0.0543,  0.0127, -0.0774,  0.6443, -1.2630])\n",
      "\n",
      "mapped: tensor([-0.3391,  1.0223,  0.8561,  0.0971,  0.0793], device='cuda:0')\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "idx: 4 \n",
      "neural: tensor([ 0.3993,  0.5163,  1.6017, -1.6191,  0.1428], device='cuda:0')\n",
      "\n",
      "token index, character, in train set: 137  False\n",
      "\n",
      "embedded: tensor([-0.2330,  0.1486, -0.1259,  0.7037, -0.4953])\n",
      "\n",
      "mapped: tensor([ 0.3993,  0.5163,  1.6017, -1.6191,  0.1428], device='cuda:0')\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "idx: 5 \n",
      "neural: tensor([-1.8486, -0.3663, -0.5902, -2.2135,  1.6358], device='cuda:0')\n",
      "\n",
      "token index, character, in train set: 114 o True\n",
      "\n",
      "embedded: tensor([ 0.4265,  1.4595,  0.6909,  0.5987, -1.0726])\n",
      "\n",
      "mapped: tensor([-1.8486, -0.3663, -0.5902, -2.2135,  1.6358], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The learned mapping did not converge to the true embedding!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(neural, mapped), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasic check failed; Inconsistency in mapping!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many(neural \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel did not at learn to map a vector it was trained on!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(\n\u001b[1;32m     33\u001b[0m         embedded, mapped\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     34\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe learned mapping did not converge to the true embedding!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m99\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: The learned mapping did not converge to the true embedding!"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "\n",
    "# What tokens and their corresponding characters are in the training set\n",
    "real_train_tokens = set()\n",
    "for sequence in text_dataset[\"train\"][\"input_ids\"]:\n",
    "    tokens = set(sequence[:-1])\n",
    "    real_train_tokens.update(tokens)\n",
    "print(len(real_train_tokens), \"\\t\", real_train_tokens)\n",
    "print(\"\\t\", tokenizer.decode(list(real_train_tokens)))\n",
    "print(\"\\n\", \"~\" * 99, \"\\n\")\n",
    "\n",
    "# Assert should not be raised if the model learned to correctly map the tokens in the training  set\n",
    "\n",
    "for idx in range(data_gen.shape[1]):\n",
    "    neural = data_gen[:, [idx], :]\n",
    "    print(\"idx:\", idx, \"\\nneural:\", neural.squeeze()[:5], end=\"\\n\\n\")\n",
    "    t = model.tokenize_neural_data(neural).item()\n",
    "    print(\n",
    "        \"token index, character, in train set:\",\n",
    "        t,\n",
    "        tokenizer.decode([t]),\n",
    "        t in real_train_tokens,\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "    embedded = embedding(torch.tensor(t, dtype=torch.long))\n",
    "    print(\"embedded:\", embedded[:5], end=\"\\n\\n\")\n",
    "    mapped = model.token_neural_map[torch.tensor(t, dtype=torch.long)]\n",
    "    print(\"mapped:\", mapped[:5], end=\"\\n\\n\")\n",
    "    if t in real_train_tokens:\n",
    "        assert torch.allclose(neural, mapped), \"Basic check failed; Inconsistency in mapping!\"\n",
    "        assert torch.any(neural != 0), \"Model did not at learn to map a vector it was trained on!\"\n",
    "        assert torch.allclose(\n",
    "            embedded, mapped.cpu()\n",
    "        ), \"The learned mapping did not converge to the true embedding!\"\n",
    "    print(\"~\" * 99, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([302]) tensor([ 0.4698,  0.8894,  0.4682,  0.1840, -0.5517], device='cuda:0')\n",
      "torch.Size([302]) tensor([ 0.4698,  0.8894,  0.4682,  0.1840, -0.5517], device='cuda:0')\n",
      "torch.Size([65, 302]) torch.Size([65, 302])\n",
      "65 65\n",
      "63 2\n"
     ]
    }
   ],
   "source": [
    "# DEBUG\n",
    "\n",
    "print(mapped.shape, mapped[:5])\n",
    "print(neural.squeeze().shape, neural.squeeze()[:5])\n",
    "print()\n",
    "\n",
    "all_unique_vectors = torch.vstack(train_dataset).unique(dim=0)\n",
    "learned_unique_vectors = model.token_neural_map.unique(dim=0)\n",
    "print(all_unique_vectors.shape, learned_unique_vectors.shape)\n",
    "print()\n",
    "\n",
    "all_unique_vectors = {tuple(row.round(decimals=2).cpu().numpy()) for row in all_unique_vectors}\n",
    "learned_unique_vectors = {\n",
    "    tuple(row.round(decimals=2).cpu().numpy()) for row in learned_unique_vectors\n",
    "}\n",
    "print(len(all_unique_vectors), len(learned_unique_vectors))\n",
    "print()\n",
    "\n",
    "inter = all_unique_vectors.intersection(learned_unique_vectors)\n",
    "diff = all_unique_vectors - learned_unique_vectors\n",
    "print(len(inter), len(diff))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Understanding `index_add_` and `scatter_add_`.\n",
    "\n",
    "x = torch.zeros(5, 3)\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
    "idx = torch.tensor([4, 4, 4])\n",
    "print(f\"self = x \\n {x}\")\n",
    "# INDEX_ADD_: the dim-th dimension of source must have the same size as the length of index (which must be a vector), and all other dimensions must match self\n",
    "print(x.index_add_(0, index=idx, source=t))\n",
    "# SCATTER_ADD_: self, source, and index must have the same number of dimensions\n",
    "print(x.scatter_add_(0, index=idx.unsqueeze(-1), src=t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
