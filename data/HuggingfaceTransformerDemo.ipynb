{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device found.\n"
     ]
    }
   ],
   "source": [
    "# @title Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import nbimporter\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tempfile import TemporaryDirectory\n",
    "from utils import DEVICE, BLOCK_SIZE\n",
    "from models._utils import NeuralTransformer\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "from CreateSyntheticDataset import tokenize_and_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling with tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `tokenizer.encode(text)`:\n",
      "\ttext: Welcome to the ðŸ¤— Tokenizers library.\n",
      "\ttokenized: [101, 6160, 2000, 1996, 100, 19204, 17629, 2015, 3075, 1012, 102]\n",
      "\tdecoded: [CLS] welcome to the [UNK] tokenizers library. [SEP]\n",
      "\n",
      "Calling `tokenizer(text)`:\n",
      "\tobject.keys(): dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\ttext: We are very happy to show you the ðŸ¤— Transformers library.\n",
      "\ttokenized: [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102]\n",
      "\tdecoded: [CLS] we are very happy to show you the [UNK] transformers library. [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Tokenizers\n",
    "# @markdown Note there are two ways to call the tokenizer's encoder.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "expl_text = \"Welcome to the ðŸ¤— Tokenizers library.\"\n",
    "impl_text = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
    "expl_encode = tokenizer.encode(expl_text)\n",
    "impl_encode = tokenizer(impl_text)\n",
    "print(\n",
    "    f\"Calling `tokenizer.encode(text)`:\\n\\ttext: {expl_text}\\n\\ttokenized: {expl_encode}\\n\\tdecoded: {tokenizer.decode(expl_encode)}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Calling `tokenizer(text)`:\\n\\tobject.keys(): {impl_encode.keys()}\\n\\ttext: {impl_text}\\n\\ttokenized: {impl_encode['input_ids']}\\n\\tdecoded: {tokenizer.decode(impl_encode['input_ids'])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "train: <class 'list'> 1 <class 'str'> 1003854\n",
      "\n",
      "validation: <class 'list'> 1 <class 'str'> 55770\n",
      "\n",
      "test: <class 'list'> 1 <class 'str'> 55770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Datasets\n",
    "\n",
    "text_dataset = load_hf_dataset(\"tiny_shakespeare\")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"train:\",\n",
    "    type(text_dataset[\"train\"][\"text\"]),\n",
    "    len(text_dataset[\"train\"][\"text\"]),\n",
    "    type(text_dataset[\"train\"][\"text\"][0]),\n",
    "    len(text_dataset[\"train\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"validation:\",\n",
    "    type(text_dataset[\"validation\"][\"text\"]),\n",
    "    len(text_dataset[\"validation\"][\"text\"]),\n",
    "    type(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    len(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"test:\",\n",
    "    type(text_dataset[\"test\"][\"text\"]),\n",
    "    len(text_dataset[\"test\"][\"text\"]),\n",
    "    type(text_dataset[\"test\"][\"text\"][0]),\n",
    "    len(text_dataset[\"test\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1963\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "})\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "text_dataset['train']['input_ids']:\n",
      " \ttype: <class 'list'> \n",
      "\tlength: 1963\n",
      "\n",
      "text_dataset['train']['input_ids'][0]:\n",
      " \ttype: <class 'list'> \n",
      "\tlength: 135\n",
      "\n",
      "text_dataset['train']['input_ids'][0][0]:\n",
      " \ttype: <class 'int'> \n",
      "\tvalue: 101\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Original sequence (text):\n",
      "\tFirst Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? All: Resolved. resolved. First Citizen: First, you know Caius Marcius is chief enemy to the people. All: We know't, we know't. First Citizen: Let us kill him, and we'll have corn at our own price. Is't a verdict? All: No more talking on't; let it be done: away, away! Second Citizen: One word, good citizens. First Citizen: We are accounted poor citizens, the\n",
      "\n",
      "Encoded sequence (tokens):\n",
      "\t [101, 2034, 6926, 1024, 2077, 2057, 10838, 2151, 2582, 1010, 2963, 2033, 3713, 1012, 2035, 1024, 3713, 1010, 3713, 1012, 2034, 6926, 1024, 2017, 2024, 2035, 10395, 2738, 2000, 3280, 2084, 2000, 6904, 15630, 2232, 1029, 2035, 1024, 10395, 1012, 10395, 1012, 2034, 6926, 1024, 2034, 1010, 2017, 2113, 29080, 2271, 7871, 4173, 2003, 2708, 4099, 2000, 1996, 2111, 1012, 2035, 1024, 2057, 2113, 1005, 1056, 1010, 2057, 2113, 1005, 1056, 1012, 2034, 6926, 1024, 2292, 2149, 3102, 2032, 1010, 1998, 2057, 1005, 2222, 2031, 9781, 2012, 2256, 2219, 3976, 1012, 2003, 1005, 1056, 1037, 14392, 1029, 2035, 1024, 2053, 2062, 3331, 2006, 1005, 1056, 1025, 2292, 2009, 2022, 2589, 1024, 2185, 1010, 2185, 999, 2117, 6926, 1024, 2028, 2773, 1010, 2204, 4480, 1012, 2034, 6926, 1024, 2057, 2024, 14729, 3532, 4480, 1010, 1996, 102]\n",
      "\n",
      "Decoded sequence (tokens):\n",
      "\t [CLS] first citizen : before we proceed any further, hear me speak. all : speak, speak. first citizen : you are all resolved rather to die than to famish? all : resolved. resolved. first citizen : first, you know caius marcius is chief enemy to the people. all : we know't, we know't. first citizen : let us kill him, and we'll have corn at our own price. is't a verdict? all : no more talking on't ; let it be done : away, away! second citizen : one word, good citizens. first citizen : we are accounted poor citizens, the [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Tokenization and Chunking\n",
    "# @markdown Apply the tokenization and chunking to each split.\n",
    "\n",
    "text_dataset = text_dataset.map(\n",
    "    tokenize_and_chunk, batched=True, fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids']:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0][0]),\n",
    "    \"\\n\\tvalue:\",\n",
    "    text_dataset[\"train\"][\"input_ids\"][0][0],\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(f\"Original sequence (text):\\n\\t{text_dataset['train']['text'][0]}\", end=\"\\n\\n\")\n",
    "print(\n",
    "    f\"Encoded sequence (tokens):\\n\\t {text_dataset['train']['input_ids'][0]}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Decoded sequence (tokens):\\n\\t {tokenizer.decode(text_dataset['train']['input_ids'][0])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define the Transformer model\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(1, max_len, d_model)  # batch_first=True\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ntoken: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        d_hid: int,\n",
    "        nlayers: int,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = torch.nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = torch.nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape [batch_size, seq_len]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [batch_size, seq_len, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            src_mask = torch.nn.Transformer.generate_square_subsequent_mask(\n",
    "                src.size(1)  # Use src.size(1) to get the seq_len\n",
    "            ).to(\n",
    "                src.device\n",
    "            )  # Use src.device to match device of src\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: torch.LongTensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Special generate method for the Transformer model.\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Since we trained the model to directly predict the next token we take the index as the argmin\n",
    "        over the distance between the output and the embedding table.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        # Loop through time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= BLOCK_SIZE else idx[:, -BLOCK_SIZE:]\n",
    "            # forward the model to get the output\n",
    "            outputs = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = outputs[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1).view(1, 1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an instance\n",
    "\n",
    "ntokens = tokenizer.vocab_size\n",
    "emsize = 302  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention (NOTE: nhead must be a divisor of d_hid)\n",
    "dropout = 0.1  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the Transformer model\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch\n",
    "\n",
    "    num_batches = len(text_dataset[\"train\"][\"input_ids\"])\n",
    "    for batch in range(num_batches):\n",
    "        tokens = text_dataset[\"train\"][\"input_ids\"][batch]\n",
    "        # parse into input and target\n",
    "        input = (\n",
    "            torch.LongTensor(tokens[:-1]).unsqueeze(0).to(DEVICE)\n",
    "        )  # ``[batch_size=1, seq_len]``\n",
    "        target = (\n",
    "            torch.LongTensor(tokens[1:]).unsqueeze(0).reshape(-1).to(DEVICE)\n",
    "        )  # ``[batch_size=1 * seq_len]``\n",
    "        # forward pass\n",
    "        output = model(input)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        output_flat = output.view(-1, ntokens)  # ``[batch_size=1 * seq_len, ntokens]``\n",
    "        # backpropagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output_flat, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(text_dataset[\"validation\"][\"input_ids\"])\n",
    "        for batch in range(num_batches):\n",
    "            tokens = text_dataset[\"validation\"][\"input_ids\"][batch]\n",
    "            input = torch.LongTensor(tokens[:-1]).unsqueeze(0).to(DEVICE)\n",
    "            target = torch.LongTensor(tokens[1:]).unsqueeze(0).reshape(-1).to(DEVICE)\n",
    "            seq_len = input.size(1)\n",
    "            output = model(input)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += criterion(output_flat, target).item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1\n",
    "\n",
    "final_model_params_path = os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loading and saving the new best model...\")\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\"),\n",
    "    )  # save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate new text using test input\n",
    "\n",
    "max_new_tokens = 100\n",
    "idx = torch.LongTensor(text_dataset[\"test\"][\"input_ids\"][0]).unsqueeze(0).to(DEVICE)\n",
    "idx_gen = model.generate(idx, max_new_tokens, top_k=5)\n",
    "\n",
    "print(idx.shape, idx_gen.shape, end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx.tolist()[0]), end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx_gen.tolist()[0][-max_new_tokens:]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  torch.Size([1, 132, 302]) torch.float32 False cpu\n",
      "\n",
      "mask: torch.Size([1, 302]) torch.bool False cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create neural datasets\n",
    "# @markdown A synthetic dataset where the neural activity is the embeddings of tokens from the tiny Shakespeare.\n",
    "\n",
    "ntokens = tokenizer.vocab_size\n",
    "emsize = 302\n",
    "d_hid = 512\n",
    "embedding = torch.nn.Embedding(ntokens, emsize, _freeze=True)\n",
    "\n",
    "train_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"train\"][\"input_ids\"]\n",
    "]\n",
    "validation_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"validation\"][\"input_ids\"]\n",
    "]\n",
    "test_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"test\"][\"input_ids\"]\n",
    "]\n",
    "\n",
    "# get a test sample for an example\n",
    "data = test_dataset[0].unsqueeze(0)\n",
    "print(\"data: \", data.shape, data.dtype, data.requires_grad, data.device, end=\"\\n\\n\")\n",
    "\n",
    "mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "print(\"mask:\", mask.shape, mask.dtype, mask.requires_grad, mask.device, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 131, 302]) torch.float32 False cuda:0\n",
      "\n",
      "target: torch.Size([1, 131, 302]) torch.float32 False cuda:0\n",
      "\n",
      "output: torch.Size([1, 131, 16384]) torch.float16 True cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create a NeuralTransformer model\n",
    "\n",
    "model = NeuralTransformer(input_size=emsize, hidden_size=d_hid).to(DEVICE)\n",
    "\n",
    "# test input-output functionality\n",
    "mask = mask.to(DEVICE)\n",
    "input = data[:, :-1, :].to(DEVICE)\n",
    "target = data[:, 1:, :].to(DEVICE)\n",
    "output = model(input, mask)\n",
    "\n",
    "print(\n",
    "    \"input:\",\n",
    "    input.shape,\n",
    "    input.dtype,\n",
    "    input.requires_grad,\n",
    "    input.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"target:\",\n",
    "    target.shape,\n",
    "    target.dtype,\n",
    "    target.requires_grad,\n",
    "    target.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"output:\",\n",
    "    output.shape,\n",
    "    output.dtype,\n",
    "    output.requires_grad,\n",
    "    output.device,\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the NeuralTransformer model\n",
    "\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch\n",
    "\n",
    "    num_batches = len(train_dataset)\n",
    "    for batch in range(num_batches):\n",
    "        data = train_dataset[batch].unsqueeze(0)\n",
    "        mask = mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "        # parse into input and target\n",
    "        mask = mask.to(DEVICE)\n",
    "        input = data[:, :-1, :].to(DEVICE)\n",
    "        target = data[:, 1:, :].to(DEVICE)\n",
    "        # forward pass\n",
    "        output = model(input, mask)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        # backpropopagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss_fn()(\n",
    "            output, target, mask\n",
    "        )  # flatens output to ``[batch_size=1 * seq_len, ntokens]``\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(validation_dataset)\n",
    "        for batch in range(num_batches):\n",
    "            data = validation_dataset[batch].unsqueeze(0)\n",
    "            mask = mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "            mask = mask.to(DEVICE)\n",
    "            input = data[:, :-1, :].to(DEVICE)\n",
    "            target = data[:, 1:, :].to(DEVICE)\n",
    "            output = model(input, mask)\n",
    "            loss = model.loss_fn()(output, target, mask)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a previously saved model checkpoint...\n",
      "| epoch   1 |   300/ 1963 batches | lr 4.52 | ms/batch 44.08 | loss 13.15 | ppl 515445.30\n",
      "| epoch   1 |   600/ 1963 batches | lr 4.52 | ms/batch 40.24 | loss 13.53 | ppl 750281.03\n",
      "| epoch   1 |   900/ 1963 batches | lr 4.52 | ms/batch 59.68 | loss 13.89 | ppl 1079552.64\n",
      "| epoch   1 |  1200/ 1963 batches | lr 4.52 | ms/batch 60.37 | loss 13.53 | ppl 749558.45\n",
      "| epoch   1 |  1500/ 1963 batches | lr 4.52 | ms/batch 53.37 | loss 13.51 | ppl 733206.23\n",
      "| epoch   1 |  1800/ 1963 batches | lr 4.52 | ms/batch 46.02 | loss 13.21 | ppl 544276.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 105.77s | valid loss 12.68 | valid ppl 320146.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   2 |   300/ 1963 batches | lr 4.48 | ms/batch 49.23 | loss 13.03 | ppl 457266.13\n",
      "| epoch   2 |   600/ 1963 batches | lr 4.48 | ms/batch 39.70 | loss 13.33 | ppl 614134.20\n",
      "| epoch   2 |   900/ 1963 batches | lr 4.48 | ms/batch 51.00 | loss 13.65 | ppl 848410.43\n",
      "| epoch   2 |  1200/ 1963 batches | lr 4.48 | ms/batch 48.63 | loss 13.26 | ppl 574033.31\n",
      "| epoch   2 |  1500/ 1963 batches | lr 4.48 | ms/batch 36.06 | loss 13.31 | ppl 603747.57\n",
      "| epoch   2 |  1800/ 1963 batches | lr 4.48 | ms/batch 47.69 | loss 13.17 | ppl 522146.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 96.84s | valid loss 11.85 | valid ppl 140404.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   3 |   300/ 1963 batches | lr 4.43 | ms/batch 52.57 | loss 12.82 | ppl 368468.08\n",
      "| epoch   3 |   600/ 1963 batches | lr 4.43 | ms/batch 47.74 | loss 13.09 | ppl 484190.83\n",
      "| epoch   3 |   900/ 1963 batches | lr 4.43 | ms/batch 38.78 | loss 13.48 | ppl 717771.28\n",
      "| epoch   3 |  1200/ 1963 batches | lr 4.43 | ms/batch 46.34 | loss 13.15 | ppl 515525.84\n",
      "| epoch   3 |  1500/ 1963 batches | lr 4.43 | ms/batch 42.71 | loss 13.10 | ppl 490447.20\n",
      "| epoch   3 |  1800/ 1963 batches | lr 4.43 | ms/batch 54.00 | loss 12.84 | ppl 378160.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 96.95s | valid loss 11.71 | valid ppl 122028.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   4 |   300/ 1963 batches | lr 4.39 | ms/batch 43.76 | loss 12.62 | ppl 302022.02\n",
      "| epoch   4 |   600/ 1963 batches | lr 4.39 | ms/batch 33.86 | loss 12.89 | ppl 395329.14\n",
      "| epoch   4 |   900/ 1963 batches | lr 4.39 | ms/batch 38.62 | loss 13.28 | ppl 587233.08\n",
      "| epoch   4 |  1200/ 1963 batches | lr 4.39 | ms/batch 47.40 | loss 12.89 | ppl 396236.14\n",
      "| epoch   4 |  1500/ 1963 batches | lr 4.39 | ms/batch 45.82 | loss 12.92 | ppl 407973.84\n",
      "| epoch   4 |  1800/ 1963 batches | lr 4.39 | ms/batch 45.80 | loss 12.72 | ppl 334891.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 88.42s | valid loss 11.42 | valid ppl 90706.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   5 |   300/ 1963 batches | lr 4.34 | ms/batch 44.42 | loss 12.38 | ppl 237832.74\n",
      "| epoch   5 |   600/ 1963 batches | lr 4.34 | ms/batch 33.20 | loss 12.76 | ppl 349294.92\n",
      "| epoch   5 |   900/ 1963 batches | lr 4.34 | ms/batch 34.74 | loss 13.12 | ppl 499001.60\n",
      "| epoch   5 |  1200/ 1963 batches | lr 4.34 | ms/batch 39.63 | loss 12.77 | ppl 350662.02\n",
      "| epoch   5 |  1500/ 1963 batches | lr 4.34 | ms/batch 35.76 | loss 12.74 | ppl 341479.07\n",
      "| epoch   5 |  1800/ 1963 batches | lr 4.34 | ms/batch 44.72 | loss 12.54 | ppl 279215.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 85.09s | valid loss 11.43 | valid ppl 92168.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   300/ 1963 batches | lr 4.30 | ms/batch 53.80 | loss 12.31 | ppl 223001.75\n",
      "| epoch   6 |   600/ 1963 batches | lr 4.30 | ms/batch 43.96 | loss 12.55 | ppl 281002.77\n",
      "| epoch   6 |   900/ 1963 batches | lr 4.30 | ms/batch 44.89 | loss 12.89 | ppl 395576.30\n",
      "| epoch   6 |  1200/ 1963 batches | lr 4.30 | ms/batch 46.20 | loss 12.61 | ppl 298542.23\n",
      "| epoch   6 |  1500/ 1963 batches | lr 4.30 | ms/batch 35.90 | loss 12.59 | ppl 293176.07\n",
      "| epoch   6 |  1800/ 1963 batches | lr 4.30 | ms/batch 45.89 | loss 12.33 | ppl 226242.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 100.95s | valid loss 11.72 | valid ppl 122985.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   300/ 1963 batches | lr 4.26 | ms/batch 47.23 | loss 12.10 | ppl 179035.35\n",
      "| epoch   7 |   600/ 1963 batches | lr 4.26 | ms/batch 33.21 | loss 12.33 | ppl 227278.70\n",
      "| epoch   7 |   900/ 1963 batches | lr 4.26 | ms/batch 36.85 | loss 12.78 | ppl 356351.14\n",
      "| epoch   7 |  1200/ 1963 batches | lr 4.26 | ms/batch 39.66 | loss 12.43 | ppl 249025.98\n",
      "| epoch   7 |  1500/ 1963 batches | lr 4.26 | ms/batch 48.37 | loss 12.43 | ppl 249733.85\n",
      "| epoch   7 |  1800/ 1963 batches | lr 4.26 | ms/batch 47.71 | loss 12.13 | ppl 184677.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 87.75s | valid loss 11.42 | valid ppl 91231.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   300/ 1963 batches | lr 4.21 | ms/batch 40.38 | loss 11.89 | ppl 146383.38\n",
      "| epoch   8 |   600/ 1963 batches | lr 4.21 | ms/batch 54.86 | loss 12.23 | ppl 204997.94\n",
      "| epoch   8 |   900/ 1963 batches | lr 4.21 | ms/batch 39.27 | loss 12.61 | ppl 298802.79\n",
      "| epoch   8 |  1200/ 1963 batches | lr 4.21 | ms/batch 55.68 | loss 12.18 | ppl 194665.20\n",
      "| epoch   8 |  1500/ 1963 batches | lr 4.21 | ms/batch 39.50 | loss 12.17 | ppl 193062.30\n",
      "| epoch   8 |  1800/ 1963 batches | lr 4.21 | ms/batch 38.68 | loss 12.02 | ppl 166597.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 92.99s | valid loss 10.64 | valid ppl 41948.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch   9 |   300/ 1963 batches | lr 4.17 | ms/batch 39.98 | loss 11.75 | ppl 126623.24\n",
      "| epoch   9 |   600/ 1963 batches | lr 4.17 | ms/batch 39.77 | loss 12.09 | ppl 178886.21\n",
      "| epoch   9 |   900/ 1963 batches | lr 4.17 | ms/batch 34.49 | loss 12.38 | ppl 237969.03\n",
      "| epoch   9 |  1200/ 1963 batches | lr 4.17 | ms/batch 48.35 | loss 12.11 | ppl 182015.83\n",
      "| epoch   9 |  1500/ 1963 batches | lr 4.17 | ms/batch 38.48 | loss 12.01 | ppl 164630.42\n",
      "| epoch   9 |  1800/ 1963 batches | lr 4.17 | ms/batch 40.24 | loss 11.88 | ppl 143956.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 84.80s | valid loss 10.92 | valid ppl 55446.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   300/ 1963 batches | lr 4.13 | ms/batch 47.08 | loss 11.67 | ppl 116862.11\n",
      "| epoch  10 |   600/ 1963 batches | lr 4.13 | ms/batch 33.06 | loss 11.87 | ppl 143150.76\n",
      "| epoch  10 |   900/ 1963 batches | lr 4.13 | ms/batch 34.46 | loss 12.22 | ppl 202045.86\n",
      "| epoch  10 |  1200/ 1963 batches | lr 4.13 | ms/batch 47.26 | loss 11.96 | ppl 156686.96\n",
      "| epoch  10 |  1500/ 1963 batches | lr 4.13 | ms/batch 42.83 | loss 11.87 | ppl 143606.29\n",
      "| epoch  10 |  1800/ 1963 batches | lr 4.13 | ms/batch 46.76 | loss 11.68 | ppl 118105.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 93.09s | valid loss 11.16 | valid ppl 70452.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   300/ 1963 batches | lr 4.09 | ms/batch 50.33 | loss 11.53 | ppl 101446.99\n",
      "| epoch  11 |   600/ 1963 batches | lr 4.09 | ms/batch 36.34 | loss 11.79 | ppl 131270.19\n",
      "| epoch  11 |   900/ 1963 batches | lr 4.09 | ms/batch 43.04 | loss 12.08 | ppl 176133.48\n",
      "| epoch  11 |  1200/ 1963 batches | lr 4.09 | ms/batch 43.33 | loss 11.77 | ppl 129769.57\n",
      "| epoch  11 |  1500/ 1963 batches | lr 4.09 | ms/batch 41.06 | loss 11.70 | ppl 120618.82\n",
      "| epoch  11 |  1800/ 1963 batches | lr 4.09 | ms/batch 46.63 | loss 11.58 | ppl 106620.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 91.16s | valid loss 11.41 | valid ppl 90078.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   300/ 1963 batches | lr 4.05 | ms/batch 52.03 | loss 11.35 | ppl 84879.20\n",
      "| epoch  12 |   600/ 1963 batches | lr 4.05 | ms/batch 34.95 | loss 11.61 | ppl 109967.78\n",
      "| epoch  12 |   900/ 1963 batches | lr 4.05 | ms/batch 36.67 | loss 11.86 | ppl 140837.86\n",
      "| epoch  12 |  1200/ 1963 batches | lr 4.05 | ms/batch 48.02 | loss 11.69 | ppl 119923.51\n",
      "| epoch  12 |  1500/ 1963 batches | lr 4.05 | ms/batch 40.68 | loss 11.62 | ppl 111297.37\n",
      "| epoch  12 |  1800/ 1963 batches | lr 4.05 | ms/batch 38.59 | loss 11.40 | ppl 89418.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 87.76s | valid loss 10.76 | valid ppl 46926.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   300/ 1963 batches | lr 4.01 | ms/batch 45.60 | loss 11.22 | ppl 74270.47\n",
      "| epoch  13 |   600/ 1963 batches | lr 4.01 | ms/batch 40.78 | loss 11.45 | ppl 93815.80\n",
      "| epoch  13 |   900/ 1963 batches | lr 4.01 | ms/batch 34.47 | loss 11.77 | ppl 129620.96\n",
      "| epoch  13 |  1200/ 1963 batches | lr 4.01 | ms/batch 39.19 | loss 11.48 | ppl 97041.17\n",
      "| epoch  13 |  1500/ 1963 batches | lr 4.01 | ms/batch 47.76 | loss 11.50 | ppl 98880.43\n",
      "| epoch  13 |  1800/ 1963 batches | lr 4.01 | ms/batch 47.19 | loss 11.25 | ppl 76527.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 90.94s | valid loss 10.86 | valid ppl 52307.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   300/ 1963 batches | lr 3.97 | ms/batch 58.81 | loss 11.04 | ppl 62559.93\n",
      "| epoch  14 |   600/ 1963 batches | lr 3.97 | ms/batch 41.53 | loss 11.34 | ppl 84122.22\n",
      "| epoch  14 |   900/ 1963 batches | lr 3.97 | ms/batch 38.24 | loss 11.61 | ppl 110511.80\n",
      "| epoch  14 |  1200/ 1963 batches | lr 3.97 | ms/batch 45.27 | loss 11.39 | ppl 88293.74\n",
      "| epoch  14 |  1500/ 1963 batches | lr 3.97 | ms/batch 46.96 | loss 11.27 | ppl 78071.29\n",
      "| epoch  14 |  1800/ 1963 batches | lr 3.97 | ms/batch 52.44 | loss 11.13 | ppl 68332.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 100.46s | valid loss 10.72 | valid ppl 45473.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   300/ 1963 batches | lr 3.93 | ms/batch 49.60 | loss 10.90 | ppl 54211.65\n",
      "| epoch  15 |   600/ 1963 batches | lr 3.93 | ms/batch 33.25 | loss 11.19 | ppl 72628.45\n",
      "| epoch  15 |   900/ 1963 batches | lr 3.93 | ms/batch 34.82 | loss 11.50 | ppl 98351.40\n",
      "| epoch  15 |  1200/ 1963 batches | lr 3.93 | ms/batch 45.30 | loss 11.20 | ppl 72949.74\n",
      "| epoch  15 |  1500/ 1963 batches | lr 3.93 | ms/batch 39.91 | loss 11.15 | ppl 69857.01\n",
      "| epoch  15 |  1800/ 1963 batches | lr 3.93 | ms/batch 45.03 | loss 10.98 | ppl 58892.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 95.97s | valid loss 10.37 | valid ppl 31829.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch  16 |   300/ 1963 batches | lr 3.89 | ms/batch 60.52 | loss 10.85 | ppl 51753.37\n",
      "| epoch  16 |   600/ 1963 batches | lr 3.89 | ms/batch 49.83 | loss 11.04 | ppl 62243.05\n",
      "| epoch  16 |   900/ 1963 batches | lr 3.89 | ms/batch 48.66 | loss 11.30 | ppl 80770.09\n",
      "| epoch  16 |  1200/ 1963 batches | lr 3.89 | ms/batch 42.06 | loss 11.06 | ppl 63697.53\n",
      "| epoch  16 |  1500/ 1963 batches | lr 3.89 | ms/batch 54.99 | loss 11.02 | ppl 61041.54\n",
      "| epoch  16 |  1800/ 1963 batches | lr 3.89 | ms/batch 41.05 | loss 10.84 | ppl 50982.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 102.19s | valid loss 10.03 | valid ppl 22780.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "| epoch  17 |   300/ 1963 batches | lr 3.85 | ms/batch 41.06 | loss 10.70 | ppl 44293.52\n",
      "| epoch  17 |   600/ 1963 batches | lr 3.85 | ms/batch 38.36 | loss 10.84 | ppl 51217.74\n",
      "| epoch  17 |   900/ 1963 batches | lr 3.85 | ms/batch 54.34 | loss 11.21 | ppl 73710.73\n",
      "| epoch  17 |  1200/ 1963 batches | lr 3.85 | ms/batch 65.73 | loss 10.96 | ppl 57525.69\n",
      "| epoch  17 |  1500/ 1963 batches | lr 3.85 | ms/batch 35.58 | loss 10.93 | ppl 55811.74\n",
      "| epoch  17 |  1800/ 1963 batches | lr 3.85 | ms/batch 38.49 | loss 10.72 | ppl 45039.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 93.82s | valid loss 10.17 | valid ppl 26030.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   300/ 1963 batches | lr 3.81 | ms/batch 39.60 | loss 10.53 | ppl 37516.12\n",
      "| epoch  18 |   600/ 1963 batches | lr 3.81 | ms/batch 32.86 | loss 10.78 | ppl 47839.12\n",
      "| epoch  18 |   900/ 1963 batches | lr 3.81 | ms/batch 34.40 | loss 11.02 | ppl 60845.54\n",
      "| epoch  18 |  1200/ 1963 batches | lr 3.81 | ms/batch 81.72 | loss 10.76 | ppl 46973.73\n",
      "| epoch  18 |  1500/ 1963 batches | lr 3.81 | ms/batch 65.53 | loss 10.79 | ppl 48319.91\n",
      "| epoch  18 |  1800/ 1963 batches | lr 3.81 | ms/batch 70.32 | loss 10.56 | ppl 38718.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 116.12s | valid loss 10.38 | valid ppl 32089.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   300/ 1963 batches | lr 3.77 | ms/batch 66.19 | loss 10.42 | ppl 33456.28\n",
      "| epoch  19 |   600/ 1963 batches | lr 3.77 | ms/batch 42.73 | loss 10.74 | ppl 45965.71\n",
      "| epoch  19 |   900/ 1963 batches | lr 3.77 | ms/batch 39.29 | loss 11.00 | ppl 59804.80\n",
      "| epoch  19 |  1200/ 1963 batches | lr 3.77 | ms/batch 39.56 | loss 10.78 | ppl 47960.74\n",
      "| epoch  19 |  1500/ 1963 batches | lr 3.77 | ms/batch 35.93 | loss 10.67 | ppl 43137.52\n",
      "| epoch  19 |  1800/ 1963 batches | lr 3.77 | ms/batch 38.99 | loss 10.44 | ppl 34347.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 90.80s | valid loss 10.50 | valid ppl 36396.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   300/ 1963 batches | lr 3.74 | ms/batch 41.02 | loss 10.31 | ppl 29892.16\n",
      "| epoch  20 |   600/ 1963 batches | lr 3.74 | ms/batch 33.01 | loss 10.49 | ppl 35908.78\n",
      "| epoch  20 |   900/ 1963 batches | lr 3.74 | ms/batch 37.95 | loss 10.81 | ppl 49385.98\n",
      "| epoch  20 |  1200/ 1963 batches | lr 3.74 | ms/batch 46.57 | loss 10.60 | ppl 39955.47\n",
      "| epoch  20 |  1500/ 1963 batches | lr 3.74 | ms/batch 40.97 | loss 10.54 | ppl 37864.56\n",
      "| epoch  20 |  1800/ 1963 batches | lr 3.74 | ms/batch 39.93 | loss 10.35 | ppl 31158.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 83.80s | valid loss  9.92 | valid ppl 20300.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving the new best model...\n",
      "Loading the best overall model...\n",
      "Saving the best overall model...\n"
     ]
    }
   ],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 20\n",
    "\n",
    "final_model_params_path = os.path.join(\"../models/\", \"neural_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"Saving the new best model...\")\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loading the best overall model...\")\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "    print(f\"Saving the best overall model...\")\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        os.path.join(\"../models/\", \"neural_transformer_model.pt\"),\n",
    "    )  # save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 302]) torch.Size([1, 135, 302]) torch.Size([1, 235, 302]) torch.Size([30522, 302])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Generate new data using test input\n",
    "\n",
    "data = train_dataset[0].unsqueeze(0).to(DEVICE)\n",
    "# data = test_dataset[0].unsqueeze(0).to(DEVICE)\n",
    "mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0).to(DEVICE)\n",
    "embedding = embedding.to(DEVICE)\n",
    "\n",
    "max_new_tokens = 100\n",
    "data_gen = model.generate(data, mask, max_new_tokens, top_k=1)\n",
    "\n",
    "print(mask.shape, data.shape, data_gen.shape, embedding.weight.shape, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2034, 6926, 1024, 2077, 2057, 10838, 2151, 2582, 1010, 2963, 2033, 3713, 1012, 2035, 1024, 3713, 1010, 3713, 1012, 2034, 6926, 1024, 2017, 2024, 2035, 10395, 2738, 2000, 3280, 2084, 2000, 6904, 15630, 2232, 1029, 2035, 1024, 10395, 1012, 10395, 1012, 2034, 6926, 1024, 2034, 1010, 2017, 2113, 29080, 2271, 7871, 4173, 2003, 2708, 4099, 2000, 1996, 2111, 1012, 2035, 1024, 2057, 2113, 1005, 1056, 1010, 2057, 2113, 1005, 1056, 1012, 2034, 6926, 1024, 2292, 2149, 3102, 2032, 1010, 1998, 2057, 1005, 2222, 2031, 9781, 2012, 2256, 2219, 3976, 1012, 2003, 1005, 1056, 1037, 14392, 1029, 2035, 1024, 2053, 2062, 3331, 2006, 1005, 1056, 1025, 2292, 2009, 2022, 2589, 1024, 2185, 1010, 2185, 999, 2117, 6926, 1024, 2028, 2773, 1010, 2204, 4480, 1012, 2034, 6926, 1024, 2057, 2024, 14729, 3532, 4480, 1010, 1996, 102]\n",
      "\n",
      "[CLS] first citizen : before we proceed any further, hear me speak. all : speak, speak. first citizen : you are all resolved rather to die than to famish? all : resolved. resolved. first citizen : first, you know caius marcius is chief enemy to the people. all : we know't, we know't. first citizen : let us kill him, and we'll have corn at our own price. is't a verdict? all : no more talking on't ; let it be done : away, away! second citizen : one word, good citizens. first citizen : we are accounted poor citizens, the [SEP]\n",
      "\n",
      "[101, 2743, 3401, 11937, 1005, 4372, 2004, 4618, 2007, 2593, 2112, 1005, 1055, 3820, 3233, 1029, 7550, 2050, 1024, 2025, 1999, 2026, 2160, 1010, 19913, 16778, 2080, 1025, 2005, 1010, 2017, 2113, 1010, 23232, 2031, 5551, 1010, 1998, 1045, 2031, 2116, 8858, 1024, 4661, 1010, 2214, 24665, 23238, 2080, 2003, 2963, 7520, 2075, 2145, 1025, 1998, 11361, 2057, 2453, 2022, 7153, 1012, 25283, 3695, 1024, 2059, 2012, 2026, 26859, 1010, 2019, 2009, 2066, 2017, 1024, 2045, 11089, 2232, 2026, 2269, 4682, 1025, 1998, 2045, 1010, 2023, 2305, 1010, 2057, 1005, 2222, 3413, 1996, 2449, 9139, 1998, 2092, 1012, 4604, 2005, 2115, 2684, 2011, 2115, 7947, 2182, 1024, 2026, 2879, 4618, 18584, 1996, 8040, 3089, 8159, 2121, 12825, 1012, 1996, 5409, 2003, 2023, 1010, 2008, 1010, 2012, 2061, 10944, 5432, 1010, 2017, 102]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @markdown We want to find the nearest token to each generated embedding.\n",
    "# We can do this by finding the nearest embedding to each generated embedding\n",
    "# and then finding the index corresponding to that embedding.\n",
    "\n",
    "# First run a test on data we know what the true token output should be\n",
    "with torch.no_grad():\n",
    "    sequence = data\n",
    "    tokens = model.tokenize_neural_data(\n",
    "        neural_sequence=data, feature_mask=mask, token_matrix=embedding.weight\n",
    "    )\n",
    "    print(tokens.squeeze().tolist(), end=\"\\n\\n\")\n",
    "    # decode into text\n",
    "    print(tokenizer.decode(tokens.squeeze().tolist()), end=\"\\n\\n\")\n",
    "\n",
    "# If correct this should match\n",
    "print(text_dataset[\"test\"][\"input_ids\"][0], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1012, 102, 1012, 14933, 3695, 1024, 1997, 2033, 102, 1012, 1998, 4011, 1997, 2135, 1025, 1998, 2025, 2092, 2014, 1997, 2004, 2004, 102, 1998, 1012, 102, 1012, 102, 1012, 2032, 5886, 2213, 2033, 3401, 2005, 2054, 2059, 18051, 2182, 2096, 999, 2005, 11221, 1045, 1996, 4757, 2000, 2014, 999, 7446, 2271, 2032, 2031, 19919, 1045, 1996, 1012, 2032, 2005, 2014, 2023, 2032, 2000, 3710, 1012, 102, 1012, 102, 2014, 1998, 2033, 2017, 2024, 7226, 1025, 1998, 16778, 1045, 2022, 2094, 1998, 2050, 1024, 1045, 6414, 2094, 2335, 4377, 13876, 5397, 2043, 2002, 1012, 7550, 2050, 1024, 2213, 1045, 1025, 1998]\n",
      "\n",
      ". [SEP]. pardonio : of me [SEP]. and supposed ofly ; and not well her of as as [SEP] and. [SEP]. [SEP]. himherm mece for what then bianca here while! for blowing i thess to her!gonus him have testify i the. him for her this him to serve. [SEP]. [SEP] her and me you are bid ; andnti i bed anda : i merelyd times dresspture when he. baptista :m i ; and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now do the same thing on the newly generated data\n",
    "with torch.no_grad():\n",
    "    sequence = data_gen[:, -max_new_tokens:, :]\n",
    "    tokens = model.tokenize_neural_data(\n",
    "        neural_sequence=sequence,\n",
    "        feature_mask=mask,\n",
    "        token_matrix=embedding.weight,\n",
    "    )\n",
    "    print(tokens.squeeze().tolist(), end=\"\\n\\n\")\n",
    "    # decode into text\n",
    "    print(tokenizer.decode(tokens.squeeze().tolist()), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
