{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device found.\n"
     ]
    }
   ],
   "source": [
    "# @title Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import nbimporter\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tempfile import TemporaryDirectory\n",
    "from utils import DEVICE, BLOCK_SIZE\n",
    "from models._utils import NeuralTransformer\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "from CreateSyntheticDataset import tokenize_and_chunk  # works because of nbimporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling with tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `tokenizer.encode(text)`:\n",
      "\ttext: Welcome to the ðŸ¤— Tokenizers library.\n",
      "\ttokenized: [101, 6160, 2000, 1996, 100, 19204, 17629, 2015, 3075, 1012, 102]\n",
      "\tdecoded: [CLS] welcome to the [UNK] tokenizers library. [SEP]\n",
      "\n",
      "Calling `tokenizer(text)`:\n",
      "\tobject.keys(): dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\ttext: We are very happy to show you the ðŸ¤— Transformers library.\n",
      "\ttokenized: [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102]\n",
      "\tdecoded: [CLS] we are very happy to show you the [UNK] transformers library. [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Tokenizers\n",
    "# @markdown Note there are two ways to call the tokenizer's encoder.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "expl_text = \"Welcome to the ðŸ¤— Tokenizers library.\"\n",
    "impl_text = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
    "expl_encode = tokenizer.encode(expl_text)\n",
    "impl_encode = tokenizer(impl_text)\n",
    "print(\n",
    "    f\"Calling `tokenizer.encode(text)`:\\n\\ttext: {expl_text}\\n\\ttokenized: {expl_encode}\\n\\tdecoded: {tokenizer.decode(expl_encode)}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Calling `tokenizer(text)`:\\n\\tobject.keys(): {impl_encode.keys()}\\n\\ttext: {impl_text}\\n\\ttokenized: {impl_encode['input_ids']}\\n\\tdecoded: {tokenizer.decode(impl_encode['input_ids'])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "train: <class 'list'> 1 <class 'str'> 1003854\n",
      "\n",
      "validation: <class 'list'> 1 <class 'str'> 55770\n",
      "\n",
      "test: <class 'list'> 1 <class 'str'> 55770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Datasets\n",
    "\n",
    "text_dataset = load_hf_dataset(\"tiny_shakespeare\")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"train:\",\n",
    "    type(text_dataset[\"train\"][\"text\"]),\n",
    "    len(text_dataset[\"train\"][\"text\"]),\n",
    "    type(text_dataset[\"train\"][\"text\"][0]),\n",
    "    len(text_dataset[\"train\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"validation:\",\n",
    "    type(text_dataset[\"validation\"][\"text\"]),\n",
    "    len(text_dataset[\"validation\"][\"text\"]),\n",
    "    type(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    len(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"test:\",\n",
    "    type(text_dataset[\"test\"][\"text\"]),\n",
    "    len(text_dataset[\"test\"][\"text\"]),\n",
    "    type(text_dataset[\"test\"][\"text\"][0]),\n",
    "    len(text_dataset[\"test\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1963\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "})\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "text_dataset['train']['input_ids']:\n",
      " \ttype: <class 'list'> \n",
      "\tlength: 1963\n",
      "\n",
      "text_dataset['train']['input_ids'][0]:\n",
      " \ttype: <class 'list'> \n",
      "\tlength: 135\n",
      "\n",
      "text_dataset['train']['input_ids'][0][0]:\n",
      " \ttype: <class 'int'> \n",
      "\tvalue: 101\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Original sequence (text):\n",
      "\tFirst Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? All: Resolved. resolved. First Citizen: First, you know Caius Marcius is chief enemy to the people. All: We know't, we know't. First Citizen: Let us kill him, and we'll have corn at our own price. Is't a verdict? All: No more talking on't; let it be done: away, away! Second Citizen: One word, good citizens. First Citizen: We are accounted poor citizens, the\n",
      "\n",
      "Encoded sequence (tokens):\n",
      "\t [101, 2034, 6926, 1024, 2077, 2057, 10838, 2151, 2582, 1010, 2963, 2033, 3713, 1012, 2035, 1024, 3713, 1010, 3713, 1012, 2034, 6926, 1024, 2017, 2024, 2035, 10395, 2738, 2000, 3280, 2084, 2000, 6904, 15630, 2232, 1029, 2035, 1024, 10395, 1012, 10395, 1012, 2034, 6926, 1024, 2034, 1010, 2017, 2113, 29080, 2271, 7871, 4173, 2003, 2708, 4099, 2000, 1996, 2111, 1012, 2035, 1024, 2057, 2113, 1005, 1056, 1010, 2057, 2113, 1005, 1056, 1012, 2034, 6926, 1024, 2292, 2149, 3102, 2032, 1010, 1998, 2057, 1005, 2222, 2031, 9781, 2012, 2256, 2219, 3976, 1012, 2003, 1005, 1056, 1037, 14392, 1029, 2035, 1024, 2053, 2062, 3331, 2006, 1005, 1056, 1025, 2292, 2009, 2022, 2589, 1024, 2185, 1010, 2185, 999, 2117, 6926, 1024, 2028, 2773, 1010, 2204, 4480, 1012, 2034, 6926, 1024, 2057, 2024, 14729, 3532, 4480, 1010, 1996, 102]\n",
      "\n",
      "Decoded sequence (tokens):\n",
      "\t [CLS] first citizen : before we proceed any further, hear me speak. all : speak, speak. first citizen : you are all resolved rather to die than to famish? all : resolved. resolved. first citizen : first, you know caius marcius is chief enemy to the people. all : we know't, we know't. first citizen : let us kill him, and we'll have corn at our own price. is't a verdict? all : no more talking on't ; let it be done : away, away! second citizen : one word, good citizens. first citizen : we are accounted poor citizens, the [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Tokenization and Chunking\n",
    "# @markdown Apply the tokenization and chunking to each split.\n",
    "\n",
    "text_dataset = text_dataset.map(\n",
    "    tokenize_and_chunk, batched=True, fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids']:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0][0]),\n",
    "    \"\\n\\tvalue:\",\n",
    "    text_dataset[\"train\"][\"input_ids\"][0][0],\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(f\"Original sequence (text):\\n\\t{text_dataset['train']['text'][0]}\", end=\"\\n\\n\")\n",
    "print(\n",
    "    f\"Encoded sequence (tokens):\\n\\t {text_dataset['train']['input_ids'][0]}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Decoded sequence (tokens):\\n\\t {tokenizer.decode(text_dataset['train']['input_ids'][0])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define the Transformer model\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)  # batch_first=True\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ntoken: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        d_hid: int,\n",
    "        nlayers: int,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = torch.nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = torch.nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape [batch_size, seq_len]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [batch_size, seq_len, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            src_mask = torch.nn.Transformer.generate_square_subsequent_mask(\n",
    "                src.size(1)  # Use src.size(1) to get the seq_len\n",
    "            ).to(\n",
    "                src.device\n",
    "            )  # Use src.device to match device of src\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: torch.LongTensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Special generate method for the Transformer model.\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Since we trained the model to directly predict the next token we take the index as the argmin\n",
    "        over the distance between the output and the embedding table.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        # Loop through time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= BLOCK_SIZE else idx[:, -BLOCK_SIZE:]\n",
    "            # forward the model to get the output\n",
    "            outputs = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = outputs[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1).view(1, 1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an instance\n",
    "\n",
    "ntokens = tokenizer.vocab_size\n",
    "emsize = 302  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention (NOTE: nhead must be a divisor of d_hid)\n",
    "dropout = 0.1  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the Transformer model\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch\n",
    "\n",
    "    num_batches = len(text_dataset[\"train\"][\"input_ids\"])\n",
    "    for batch in range(num_batches):\n",
    "        tokens = text_dataset[\"train\"][\"input_ids\"][batch]\n",
    "        # parse into input and target\n",
    "        input = torch.LongTensor(tokens[:-1]).unsqueeze(0).to(DEVICE)  # ``[batch_size=1, seq_len]``\n",
    "        target = (\n",
    "            torch.LongTensor(tokens[1:]).unsqueeze(0).reshape(-1).to(DEVICE)\n",
    "        )  # ``[batch_size=1 * seq_len]``\n",
    "        # forward pass\n",
    "        output = model(input)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        output_flat = output.view(-1, ntokens)  # ``[batch_size=1 * seq_len, ntokens]``\n",
    "        # backpropagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output_flat, target)\n",
    "        # check if the computed loss requires gradient\n",
    "        if loss.requires_grad:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(text_dataset[\"validation\"][\"input_ids\"])\n",
    "        for batch in range(num_batches):\n",
    "            tokens = text_dataset[\"validation\"][\"input_ids\"][batch]\n",
    "            input = torch.LongTensor(tokens[:-1]).unsqueeze(0).to(DEVICE)\n",
    "            target = torch.LongTensor(tokens[1:]).unsqueeze(0).reshape(-1).to(DEVICE)\n",
    "            seq_len = input.size(1)\n",
    "            output = model(input)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += criterion(output_flat, target).item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1\n",
    "\n",
    "final_model_params_path = os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loading and saving the new best model...\")\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\"),\n",
    "    )  # save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate new text using test input\n",
    "\n",
    "max_new_tokens = 100\n",
    "idx = torch.LongTensor(text_dataset[\"test\"][\"input_ids\"][0]).unsqueeze(0).to(DEVICE)\n",
    "idx_gen = model.generate(idx, max_new_tokens, top_k=5)\n",
    "\n",
    "print(idx.shape, idx_gen.shape, end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx.tolist()[0]), end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx_gen.tolist()[0][-max_new_tokens:]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  torch.Size([1, 132, 302]) torch.float32 False cpu\n",
      "\n",
      "mask: torch.Size([1, 302]) torch.bool False cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create neural datasets\n",
    "# @markdown A synthetic dataset where the neural activity is the embeddings of tokens from the tiny Shakespeare.\n",
    "\n",
    "ntokens = tokenizer.vocab_size\n",
    "emsize = 302\n",
    "d_hid = 512\n",
    "embedding = torch.nn.Embedding(ntokens, emsize, _freeze=True)\n",
    "\n",
    "train_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"train\"][\"input_ids\"]\n",
    "]\n",
    "validation_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"validation\"][\"input_ids\"]\n",
    "]\n",
    "test_dataset = [\n",
    "    torch.vstack([embedding(token) for token in torch.LongTensor(sequence)])\n",
    "    for sequence in text_dataset[\"test\"][\"input_ids\"]\n",
    "]\n",
    "\n",
    "# get a test sample for an example\n",
    "data = test_dataset[0].unsqueeze(0)\n",
    "print(\"data: \", data.shape, data.dtype, data.requires_grad, data.device, end=\"\\n\\n\")\n",
    "\n",
    "mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "print(\"mask:\", mask.shape, mask.dtype, mask.requires_grad, mask.device, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 131, 302]) torch.float32 False cuda:0\n",
      "\n",
      "target: torch.Size([1, 131, 302]) torch.float32 False cuda:0\n",
      "\n",
      "output: torch.Size([1, 131, 4096]) torch.float16 True cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Create a NeuralTransformer model\n",
    "\n",
    "model = NeuralTransformer(\n",
    "    input_size=emsize,\n",
    "    hidden_size=d_hid,\n",
    "    version_2=True,\n",
    "    multi_channel=False,\n",
    "    vq_vae=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "# test input-output functionality\n",
    "mask = mask.to(DEVICE)\n",
    "input = data[:, :-1, :].to(DEVICE)\n",
    "target = data[:, 1:, :].to(DEVICE)\n",
    "output = model(input, mask)\n",
    "\n",
    "print(\n",
    "    \"input:\",\n",
    "    input.shape,\n",
    "    input.dtype,\n",
    "    input.requires_grad,\n",
    "    input.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"target:\",\n",
    "    target.shape,\n",
    "    target.dtype,\n",
    "    target.requires_grad,\n",
    "    target.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"output:\",\n",
    "    output.shape,\n",
    "    output.dtype,\n",
    "    output.requires_grad,\n",
    "    output.device,\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the NeuralTransformer model\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch, optimizer, scheduler\n",
    "\n",
    "    num_batches = len(train_dataset)\n",
    "    for batch in range(num_batches):\n",
    "        data = train_dataset[batch].unsqueeze(0)\n",
    "        mask = mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "        # parse into input and target\n",
    "        mask = mask.to(DEVICE)\n",
    "        input = data[:, :-1, :].to(DEVICE)\n",
    "        target = data[:, 1:, :].to(DEVICE)\n",
    "        # forward pass\n",
    "        output = model(input, mask)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        # backpropagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss_fn()(\n",
    "            output, target, mask\n",
    "        )  # flatens output to ``[batch_size=1 * seq_len, ntokens]``\n",
    "        # check if the computed loss requires gradient\n",
    "        if loss.requires_grad:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            try:\n",
    "                ppl = math.exp(cur_loss)\n",
    "            except OverflowError:\n",
    "                ppl = float(\"inf\")\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(validation_dataset)\n",
    "        for batch in range(num_batches):\n",
    "            data = validation_dataset[batch].unsqueeze(0)\n",
    "            mask = mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0)\n",
    "            mask = mask.to(DEVICE)\n",
    "            input = data[:, :-1, :].to(DEVICE)\n",
    "            target = data[:, 1:, :].to(DEVICE)\n",
    "            output = model(input, mask)\n",
    "            loss = model.loss_fn()(output, target, mask)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a previously saved model checkpoint...\n",
      "| epoch   1 |   300/ 1963 batches | lr 10.00 | ms/batch 21.33 | loss 423.80 | ppl 11336179678655726878166593315737876153269239546782361879403636884504751381869034209983526098682801840643315967797020733870281826274419111035591834850378609397829460115691476967355318272.00\n",
      "| epoch   1 |   600/ 1963 batches | lr 10.00 | ms/batch 15.57 | loss 425.39 | ppl 55803502308661568218178588464784618797987212967831510221002371704293190261611825924601313729898810758874181276930748860410023904306566474281233352079662552895213533128995695360814874624.00\n",
      "| epoch   1 |   900/ 1963 batches | lr 10.00 | ms/batch 17.75 | loss 426.09 | ppl 111803849885616514389530916737936368962938741887871585741293966380476403346788289635838534901118644307487559532723732578447575040936541204357641295957313707377590026990872005434557857792.00\n",
      "| epoch   1 |  1200/ 1963 batches | lr 10.00 | ms/batch 23.97 | loss 424.69 | ppl 27659387806702845764042254607858144866057081918398401107801343598995684185872070285748744762311154118776251674008655303035753240189360002220746839319565228275828140297736060603578974208.00\n",
      "| epoch   1 |  1500/ 1963 batches | lr 10.00 | ms/batch 13.82 | loss 426.76 | ppl 219489547039602794158775531839372282380375914001081933338968188931928988838350549081010262067746719603587061806740125741035240330922824193247689266727809407737304003973838926127879421952.00\n",
      "| epoch   1 |  1800/ 1963 batches | lr 10.00 | ms/batch  8.92 | loss 424.90 | ppl 34068094154885699821319938620554368868386970492755643501813852002266015519526315465102501989266383877857297568425863647733924488215611699520043623036579466679220493159073037074663735296.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 32.27s | valid loss 415.60 | valid ppl 3104352360627078738893570276088745985949095059719820576491964375889168274737573491414917141768376813481451731420259102157553908025219740144482385609046184220173612540252587900796928.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   300/ 1963 batches | lr 9.90 | ms/batch  9.28 | loss 425.68 | ppl 74333111745376000784354580874385649394371735845515745377152808448922412829479755085772520502539860939476881056826028309845436502437748536422644523502876290718051724721426579509158608896.00\n",
      "| epoch   2 |   600/ 1963 batches | lr 9.90 | ms/batch  9.62 | loss 425.75 | ppl 79523059907845193135315971438890335701857582918323538001701877053282258719618736293834095950558458671948839651610825344236856605030360103658074612357738931244799368020656585212019343360.00\n",
      "| epoch   2 |   900/ 1963 batches | lr 9.90 | ms/batch  9.19 | loss 424.85 | ppl 32224506595056149517112997948067315166390608867349712194666067585699482579601338144662323083857011634025159883803746069775194167968773308786173109631141261258218961675622133927421935616.00\n",
      "| epoch   2 |  1200/ 1963 batches | lr 9.90 | ms/batch  9.29 | loss 422.39 | ppl 2770348057962842577294462813347011357138557150302115531859648538768324922576766633433296238253034015667761405207963508590417802779835007935967883553255309642576266781888039411175653376.00\n",
      "| epoch   2 |  1500/ 1963 batches | lr 9.90 | ms/batch  9.31 | loss 423.73 | ppl 10568966742656210683957853689300531790620910146247264195510266772564314421392757510702347866396079181477629401448880637474738566972847632812462581614380113606622169111670375339717033984.00\n",
      "| epoch   2 |  1800/ 1963 batches | lr 9.90 | ms/batch  9.02 | loss 422.47 | ppl 2983324894729384624416867919040619293611922174192965914339532425486168214971388553583600989044978033935245021568441135933744657475937810782751915114409089618211907238848427155093716992.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 18.56s | valid loss 425.44 | valid ppl 58430276678770038587327032881252539731815423786013920606323165734353881470778933291504670857582428407932375609189777142092103755673684582391657715703522734373747671516001057342599725056.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   300/ 1963 batches | lr 9.80 | ms/batch  8.95 | loss 423.20 | ppl 6238492772551669976167566534804090123674548259677370385239920606897389887614744807868812211748984373719179943074234300748732851196950871254641943288028596445229517665273358735254749184.00\n",
      "| epoch   3 |   600/ 1963 batches | lr 9.80 | ms/batch  9.34 | loss 422.45 | ppl 2948053748082863076071107059832746555665987250657592443444945177087449456293313134523300499168509517490383176644750756266666834922079486859415188537153136869341609484109899889178050560.00\n",
      "| epoch   3 |   900/ 1963 batches | lr 9.80 | ms/batch  9.54 | loss 421.84 | ppl 1595492876755481554269511820191350920720161055789863552965413034957832754659417309378578895071721518588724918936684071463201338433316597209275371964035657222726433212995999788376784896.00\n",
      "| epoch   3 |  1200/ 1963 batches | lr 9.80 | ms/batch 10.09 | loss 419.95 | ppl 241285669121367739451338771015795660544849900358491182125014734828003239257394960211057610559795782139079664885557129345714727238427651061046693827620828618581509366706369518224015360.00\n",
      "| epoch   3 |  1500/ 1963 batches | lr 9.80 | ms/batch  9.37 | loss 421.15 | ppl 802158522178754192463828157044840674575517918870787044686453258615344119242382364110765932709175517967318229383696776498632302570050026196083313062232889730645646923015582997083586560.00\n",
      "| epoch   3 |  1800/ 1963 batches | lr 9.80 | ms/batch  9.34 | loss 419.38 | ppl 136706339310193419464410660436723380371335499003773386897568133405953280942582016053774850802525497793996814488481988660809250422241165642898205586165912599372897655837996218193018880.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 18.99s | valid loss 411.76 | valid ppl 66695604349379255141787868218646082478453471388656566175843028246660407684751586493101781105158623303787069567016501849357259180814640632688608947626005837812254789353911789551616.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   300/ 1963 batches | lr 9.70 | ms/batch  8.56 | loss 420.72 | ppl 518283396058346761929166078722053992108399165645300659120701045695857193558395242358213515657090398666307734656538148380957119052108804612374601900488899713846049331695023822710243328.00\n",
      "| epoch   4 |   600/ 1963 batches | lr 9.70 | ms/batch  9.31 | loss 419.66 | ppl 181135954104025862980519213122196600484245619187180762093202496396369403000915379328071214469932544662000962604042219451294640459099593476956685582956405199421092156734409414258196480.00\n",
      "| epoch   4 |   900/ 1963 batches | lr 9.70 | ms/batch  9.59 | loss 419.64 | ppl 176713424661109691753976931105046231471409695565268850600990729790242420143969770501223902027962689913691378775137436161072113683465547336113004841424618799802704997750322161873059840.00\n",
      "| epoch   4 |  1200/ 1963 batches | lr 9.70 | ms/batch  8.91 | loss 417.86 | ppl 29797505306489678048752149167966126648035617508165247080156435253602806269632684336630054201489760576429919748541315237816657642562875764467826144722523714817174533939850945344045056.00\n",
      "| epoch   4 |  1500/ 1963 batches | lr 9.70 | ms/batch  9.42 | loss 419.13 | ppl 106090762289263719934279216379058593180092563446271754613256556610710239281834144485780080242200276518358151661678553733873665623162504331959881986400959577191079888602607052421857280.00\n",
      "| epoch   4 |  1800/ 1963 batches | lr 9.70 | ms/batch  9.96 | loss 417.31 | ppl 17116624497750322567101669338441581554652577834777066839279799516409410588861832887432024484219154992972832107040778309379914369678144840909152040550783666762263313251758295933255680.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 18.84s | valid loss 415.86 | valid ppl 4029166862212473563457875425237131513285203831613036265993169068608313005543897659180497593975071536894904027177152670649482068149244076210359297926715460533115561066232095130517504.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   300/ 1963 batches | lr 9.61 | ms/batch  8.66 | loss 418.46 | ppl 54221142515472709722851610417291383932082763075134228228320054361521919455720263283093127893158037433886322483267446632676718220532752855647038911792462942455604065546345622322282496.00\n",
      "| epoch   5 |   600/ 1963 batches | lr 9.61 | ms/batch  9.34 | loss 417.53 | ppl 21372988871860541914523702511117086714801527171485161404636944882878064841486494948928327731724016790804320390737365227854815549375084544285330207076049593632691215234475859889881088.00\n",
      "| epoch   5 |   900/ 1963 batches | lr 9.61 | ms/batch  9.32 | loss 417.64 | ppl 23992767095453534373878313756594977704930889089871920064902983550849864821357348430647404767485355193413582166147741514465189835262362550422986381102669097125613146850490573643579392.00\n",
      "| epoch   5 |  1200/ 1963 batches | lr 9.61 | ms/batch  9.33 | loss 416.02 | ppl 4711355795115777979242134259323211774687839348707251107822092581311792287103935530553643488878358380848376373758154324552999616805942264167697600284963554635317994613215124833435648.00\n",
      "| epoch   5 |  1500/ 1963 batches | lr 9.61 | ms/batch  9.18 | loss 416.84 | ppl 10784045379611257836028047495845951427125247219398431547809305477618455188293099747100960112331390401462113235874452810111830973422328206773676936960225913847535015132595207899447296.00\n",
      "| epoch   5 |  1800/ 1963 batches | lr 9.61 | ms/batch  8.92 | loss 415.14 | ppl 1964354133451911647094749404777581393465834708791849694061911583866106805968410461401610734017363554589251140858550599951266650359462568871738108956611872045153592487458449515347968.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 18.46s | valid loss 411.51 | valid ppl 52009339437552709644115346901125728041596116983698823426830138078786697541487679830970339371575220136714088258511693383136511578185675481462962302904942183871923380846575273115648.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   300/ 1963 batches | lr 9.51 | ms/batch  8.97 | loss 416.91 | ppl 11488621953330321056202855303327675548137747439593394133786405125691277363242449471173664825643064617770728720630461053418376948785326045624652840388374323394139953081529139283886080.00\n",
      "| epoch   6 |   600/ 1963 batches | lr 9.51 | ms/batch  8.49 | loss 415.49 | ppl 2788840586472740635961344262963192878379577956262937676119973115475985702126926132659315953584138046000910039500616788040910133018947072400388175524874983582543132945815746834857984.00\n",
      "| epoch   6 |   900/ 1963 batches | lr 9.51 | ms/batch  9.16 | loss 415.42 | ppl 2605530349794547578626153483781954948025451826270093395412780605347280924855467975358119713157176082512641202008901980232538553478999973491476162989317520907309619313764680949825536.00\n",
      "| epoch   6 |  1200/ 1963 batches | lr 9.51 | ms/batch  9.23 | loss 413.93 | ppl 586967447631733566799248714905716074844575583612641697184214795863332429735198975106725097469332074903442668867578221003203192055357942203264898844446841550583063614007263023333376.00\n",
      "| epoch   6 |  1500/ 1963 batches | lr 9.51 | ms/batch  9.14 | loss 415.21 | ppl 2114469703059699424727779804007548507271800341845770107845298394301937230836899295270237178135232827319490368145049076274053408211203154922888820321212783890301945167605128784510976.00\n",
      "| epoch   6 |  1800/ 1963 batches | lr 9.51 | ms/batch  9.13 | loss 413.82 | ppl 523962207233749236317127383436484574176674380327879669991511742063459516113568890418512724072838272925442457993890771581607484966934324702193521180740042322821346793643441110646784.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 18.32s | valid loss 414.24 | valid ppl 798449334794124940826656181294606665014037636310218201746561241508963749562966725294024189667672493245486397722672286261500597167706721683365352355756358873583649765924495048048640.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   300/ 1963 batches | lr 9.41 | ms/batch 12.17 | loss 414.83 | ppl 1435631541841926112032744025060109187879913338932760628535177880056656751899745007334111409664304144516740566598603808905531496374442449082075701577360523486157086185971923363561472.00\n",
      "| epoch   7 |   600/ 1963 batches | lr 9.41 | ms/batch  8.48 | loss 413.84 | ppl 537510790231858053323548552110420950531623018635351101310709712428976399430772510337962711430522916757690364983436396885567461813415144500038131295049218568152548897580040088190976.00\n",
      "| epoch   7 |   900/ 1963 batches | lr 9.41 | ms/batch  9.34 | loss 413.83 | ppl 530185319915058560076477947407135830865440646758114949120812773315152366937543079505591551668589744942924345804873174970023910865925809162873570595776669883921739302762935970955264.00\n",
      "| epoch   7 |  1200/ 1963 batches | lr 9.41 | ms/batch  9.68 | loss 412.85 | ppl 198870243631887682961226585033846655105763365785594278509038234374698898568334291788467700432980011714202785659949934451766748564304789487359275746548844513293846801721631619153920.00\n",
      "| epoch   7 |  1500/ 1963 batches | lr 9.41 | ms/batch  9.27 | loss 413.67 | ppl 451287393444752983796892273662523119786588348176632548269793849834537302162481660343042209716971773750021235910209645077332673730896748790684704846858405142927766308116827012595712.00\n",
      "| epoch   7 |  1800/ 1963 batches | lr 9.41 | ms/batch  9.49 | loss 411.91 | ppl 77353797531114951674348164379375493943187408741747101822740746877838787585813461041616469621355080288117587589201138066889567385156764047609521662605178270328326535889001872621568.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 19.61s | valid loss 407.85 | valid ppl 1334794696448879728482803900425776109136050031201690302524039371301090589676093661821044410318924454653244709080314064341365831219738215843606981083849412108754103969071325249536.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   300/ 1963 batches | lr 9.32 | ms/batch  9.40 | loss 413.69 | ppl 459552540493859871495902411288323618191576108721085763682364575295523624705262570836684244478271057095357362371985859964168375680281161581384687775005678925026407188264504839372800.00\n",
      "| epoch   8 |   600/ 1963 batches | lr 9.32 | ms/batch  8.90 | loss 412.34 | ppl 119632911206239975301589297358101028947457667798976681579290506092633156362535631471386852070376599313905729684427822108365568859507696755455981198565337707445636872030075916124160.00\n",
      "| epoch   8 |   900/ 1963 batches | lr 9.32 | ms/batch  8.79 | loss 412.43 | ppl 131258446808479393343979840828377138081924317875906262862402974936928735552769422349859270885155566709480222010254352864221911267187074279634666103922260439413719707232857806077952.00\n",
      "| epoch   8 |  1200/ 1963 batches | lr 9.32 | ms/batch  9.35 | loss 411.27 | ppl 40819708701029602573589927759879518330748471325708357059172775529667551490806180367707217432678258730724447540755769027216209530351013327682637943870144903449814972443840858816512.00\n",
      "| epoch   8 |  1500/ 1963 batches | lr 9.32 | ms/batch  9.24 | loss 412.55 | ppl 147931413057612806378335674674168745582024023494677361645149350904833374747995995125369721691820389959941097135191547532900482327482921062415540576999663033533008340613497710706688.00\n",
      "| epoch   8 |  1800/ 1963 batches | lr 9.32 | ms/batch 13.78 | loss 410.55 | ppl 19957766176677341697364409285706604743051911612986599634736097011175707033567512347861709216650459238942381256247986708775566370109409848785693130103558716837571019420489312370688.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 19.94s | valid loss 408.73 | valid ppl 3231219738052530351783604623969042336896472628172601889285565021652662580009607827714963993367631186359808909186298104502362198932071916707179520294301000447758917433517380993024.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   300/ 1963 batches | lr 9.23 | ms/batch  9.48 | loss 412.49 | ppl 139004288273529865501625185260858706187247993687372823596561545173220112272924566908930662175276989248150291405851675118985745743320791518456735251599979868369055502038891661099008.00\n",
      "| epoch   9 |   600/ 1963 batches | lr 9.23 | ms/batch  8.78 | loss 410.91 | ppl 28523413696686877944582417632045879564365330739031931870915538298150543156861255487939252316462200668246838323686856561537790057970845834538495491578793555957447927411413281144832.00\n",
      "| epoch   9 |   900/ 1963 batches | lr 9.23 | ms/batch  8.70 | loss 411.24 | ppl 39697286664309991693737574388023259311092347350110564838435150101974606193888070029208504137648306221681802913996131962355120246763986310724294103489646900901832015997626786775040.00\n",
      "| epoch   9 |  1200/ 1963 batches | lr 9.23 | ms/batch  9.41 | loss 410.03 | ppl 11909959248801221539893816863902443829507402561120881908079895504490507552732342188900718302962991300217472435793884682411834035633946253160413093809519186532226088534170971144192.00\n",
      "| epoch   9 |  1500/ 1963 batches | lr 9.23 | ms/batch 12.28 | loss 410.87 | ppl 27484223369271734713867615065933709921197443935998316035585424503248102596101160409403026795154239855988381838157203436742594700475290626964684459289442889644419423183007748456448.00\n"
     ]
    }
   ],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "lr = 10.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "\n",
    "best_val_loss = 400  # float(\"inf\")\n",
    "epochs = 40\n",
    "\n",
    "final_model_params_path = os.path.join(\"../models/\", \"neural_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model)\n",
    "        try:\n",
    "            val_ppl = math.exp(val_loss)\n",
    "        except OverflowError:\n",
    "            val_ppl = float(\"inf\")\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"Saving the new best model...\")\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loading the best overall model...\")\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "    print(f\"Saving the best overall model...\")\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        os.path.join(\"../models/\", \"neural_transformer_model.pt\"),\n",
    "    )  # save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 302]) torch.Size([1, 135, 302]) torch.Size([1, 100, 302]) torch.Size([30522, 302])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Generate new data using test input\n",
    "\n",
    "data = train_dataset[0].unsqueeze(0).to(DEVICE)\n",
    "# data = test_dataset[0].unsqueeze(0).to(DEVICE)\n",
    "mask = torch.ones(emsize, dtype=torch.bool).unsqueeze(0).to(DEVICE)\n",
    "embedding = embedding.to(DEVICE)\n",
    "\n",
    "max_new_tokens = 100\n",
    "data_gen = model.generate(data, mask, max_new_tokens, autoregressive=True)\n",
    "\n",
    "print(mask.shape, data.shape, data_gen.shape, embedding.weight.shape, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown We want to find the nearest token to each generated embedding.\n",
    "# We can do this by finding the nearest embedding to each generated embedding\n",
    "# and then finding the index corresponding to that embedding.\n",
    "\n",
    "# First run a test on data we know what the true token output should be\n",
    "with torch.no_grad():\n",
    "    sequence = data\n",
    "    tokens = model.tokenize_neural_data(\n",
    "        neural_sequence=data, feature_mask=mask, token_matrix=embedding.weight\n",
    "    )\n",
    "    print(tokens.squeeze().tolist(), end=\"\\n\\n\")\n",
    "    # decode into text\n",
    "    print(tokenizer.decode(tokens.squeeze().tolist()), end=\"\\n\\n\")\n",
    "\n",
    "# If correct this should match\n",
    "print(text_dataset[\"test\"][\"input_ids\"][0], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m data_gen[:, \u001b[38;5;241m-\u001b[39mmax_new_tokens:, :]\n\u001b[0;32m----> 4\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_neural_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneural_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokens\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist(), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# decode into text\u001b[39;00m\n",
      "File \u001b[0;32m/om2/user/qsimeon/miniconda3/envs/worm-graph/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/vast-storage/scratch/vast/yanglab/qsimeon/worm-graph/models/_utils.py:707\u001b[0m, in \u001b[0;36mModel.tokenize_neural_data\u001b[0;34m(self, neural_sequence, feature_mask, token_matrix)\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mDEVICE\u001b[38;5;241m.\u001b[39mtype, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mhalf)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_neural_data\u001b[39m(\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m     token_matrix: Union[\u001b[38;5;28;01mNone\u001b[39;00m, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    691\u001b[0m ):\n\u001b[1;32m    692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m    Convert the high-dimensional sequence of neural states to a 1-D sequence of tokens.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m    The approach used is similar to that of VQ-VAEs where the neural data is treated as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m        token_sequence: tensor of shape (batch_size, seq_len)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;66;03m# Route to the appropriate tokenization method\u001b[39;00m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_channel:\n",
      "File \u001b[0;32m/om2/user/qsimeon/miniconda3/envs/worm-graph/lib/python3.10/site-packages/torch/cuda/memory.py:159\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Now do the same thing on the newly generated data\n",
    "with torch.no_grad():\n",
    "    sequence = data_gen[:, -max_new_tokens:, :]\n",
    "    tokens = model.tokenize_neural_data(\n",
    "        neural_sequence=sequence,\n",
    "        feature_mask=mask,\n",
    "        token_matrix=embedding.weight,\n",
    "    )\n",
    "    print(tokens.squeeze().tolist(), end=\"\\n\\n\")\n",
    "    # decode into text\n",
    "    print(tokenizer.decode(tokens.squeeze().tolist()), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
