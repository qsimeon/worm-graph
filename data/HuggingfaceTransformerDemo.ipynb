{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device found.\n"
     ]
    }
   ],
   "source": [
    "# @title Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import nbimporter\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tempfile import TemporaryDirectory\n",
    "from utils import DEVICE, MAX_TOKEN_LEN\n",
    "from datasets import load_dataset as load_hf_dataset\n",
    "from preprocess._utils import smooth_data_preprocess, reshape_calcium_data\n",
    "from CreateSyntheticDataset import (\n",
    "    save_synthetic_dataset,\n",
    "    plot_neural_signals,\n",
    "    plot_3d_trajectory,\n",
    "    tokenize_and_chunk,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling with tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling `tokenizer.encode(text)`:\n",
      "\ttext: Welcome to the ðŸ¤— Tokenizers library.\n",
      "\ttokenized: [101, 6160, 2000, 1996, 100, 19204, 17629, 2015, 3075, 1012, 102]\n",
      "\tdecoded: [CLS] welcome to the [UNK] tokenizers library. [SEP]\n",
      "\n",
      "Calling `tokenizer(text)`:\n",
      "\tobject.keys(): dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\ttext: We are very happy to show you the ðŸ¤— Transformers library.\n",
      "\ttokenized: [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102]\n",
      "\tdecoded: [CLS] we are very happy to show you the [UNK] transformers library. [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Tokenizers\n",
    "# @markdown Note there are two ways to call the tokenizer's encoder.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "expl_text = \"Welcome to the ðŸ¤— Tokenizers library.\"\n",
    "impl_text = \"We are very happy to show you the ðŸ¤— Transformers library.\"\n",
    "expl_encode = tokenizer.encode(expl_text)\n",
    "impl_encode = tokenizer(impl_text)\n",
    "print(\n",
    "    f\"Calling `tokenizer.encode(text)`:\\n\\ttext: {expl_text}\\n\\ttokenized: {expl_encode}\\n\\tdecoded: {tokenizer.decode(expl_encode)}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Calling `tokenizer(text)`:\\n\\tobject.keys(): {impl_encode.keys()}\\n\\ttext: {impl_text}\\n\\ttokenized: {impl_encode['input_ids']}\\n\\tdecoded: {tokenizer.decode(impl_encode['input_ids'])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "train: <class 'list'> 1 <class 'str'> 1003854\n",
      "\n",
      "validation: <class 'list'> 1 <class 'str'> 55770\n",
      "\n",
      "test: <class 'list'> 1 <class 'str'> 55770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Datasets\n",
    "\n",
    "text_dataset = load_hf_dataset(\"tiny_shakespeare\")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"train:\",\n",
    "    type(text_dataset[\"train\"][\"text\"]),\n",
    "    len(text_dataset[\"train\"][\"text\"]),\n",
    "    type(text_dataset[\"train\"][\"text\"][0]),\n",
    "    len(text_dataset[\"train\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"validation:\",\n",
    "    type(text_dataset[\"validation\"][\"text\"]),\n",
    "    len(text_dataset[\"validation\"][\"text\"]),\n",
    "    type(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    len(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"test:\",\n",
    "    type(text_dataset[\"test\"][\"text\"]),\n",
    "    len(text_dataset[\"test\"][\"text\"]),\n",
    "    type(text_dataset[\"test\"][\"text\"][0]),\n",
    "    len(text_dataset[\"test\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Tokenization and Chunking\n",
    "# @markdown Apply the tokenization and chunking to each split.\n",
    "\n",
    "text_dataset = text_dataset.map(\n",
    "    tokenize_and_chunk, batched=True, fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids']:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    \"\\n\\tlength:\",\n",
    "    len(text_dataset[\"train\"][\"input_ids\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"text_dataset['train']['input_ids'][0][0]:\\n\",\n",
    "    \"\\ttype:\",\n",
    "    type(text_dataset[\"train\"][\"input_ids\"][0][0]),\n",
    "    \"\\n\\tvalue:\",\n",
    "    text_dataset[\"train\"][\"input_ids\"][0][0],\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\"~~~\" * 50, end=\"\\n\\n\")\n",
    "\n",
    "print(f\"Original sequence (text):\\n\\t{text_dataset['train']['text'][0]}\", end=\"\\n\\n\")\n",
    "print(\n",
    "    f\"Encoded sequence (tokens):\\n\\t {text_dataset['train']['input_ids'][0]}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"Decoded sequence (tokens):\\n\\t {tokenizer.decode(text_dataset['train']['input_ids'][0])}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define the Transformer model\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(1, max_len, d_model)  # batch_first=True\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ntoken: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        d_hid: int,\n",
    "        nlayers: int,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = torch.nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = torch.nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape [batch_size, seq_len]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [batch_size, seq_len, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            src_mask = torch.nn.Transformer.generate_square_subsequent_mask(\n",
    "                src.size(1)  # Use src.size(1) to get the seq_len\n",
    "            ).to(\n",
    "                src.device\n",
    "            )  # Use src.device to match device of src\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: torch.LongTensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Special generate method for the Transformer model.\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Since we trained the model to directly predict the next token we take the index as the argmin\n",
    "        over the distance between the output and the embedding table.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        # Loop through time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= MAX_TOKEN_LEN else idx[:, -MAX_TOKEN_LEN:]\n",
    "            # forward the model to get the output\n",
    "            outputs = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = outputs[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1).view(1, 1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initiate an instance\n",
    "\n",
    "ntokens = tokenizer.vocab_size\n",
    "emsize = 302  # embedding dimension\n",
    "d_hid = 302  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 1  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention (NOTE: nhead must be a divisor of d_hid)\n",
    "dropout = 0.1  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the Transformer model\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 1.0  # 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 300\n",
    "    start_time = time.time()\n",
    "    global epoch\n",
    "\n",
    "    num_batches = len(text_dataset[\"train\"][\"input_ids\"])\n",
    "    for batch in range(num_batches):\n",
    "        tokens = text_dataset[\"train\"][\"input_ids\"][batch]\n",
    "        # parse into input and target\n",
    "        input = (\n",
    "            torch.LongTensor(tokens[:-1]).unsqueeze(0).to(DEVICE)\n",
    "        )  # ``[batch_size=1, seq_len]``\n",
    "        target = (\n",
    "            torch.LongTensor(tokens[1:]).unsqueeze(0).reshape(-1).to(DEVICE)\n",
    "        )  # ``[batch_size=1 * seq_len]``\n",
    "        # forward pass\n",
    "        output = model(input)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        output_flat = output.view(-1, ntokens)  # ``[batch_size=1 * seq_len, ntokens]``\n",
    "        # backpropagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output_flat, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(text_dataset[\"validation\"][\"input_ids\"])\n",
    "        for batch in range(num_batches):\n",
    "            tokens = text_dataset[\"validation\"][\"input_ids\"][batch]\n",
    "            input = torch.LongTensor(tokens[:-1]).unsqueeze(0).to(DEVICE)\n",
    "            target = torch.LongTensor(tokens[1:]).unsqueeze(0).reshape(-1).to(DEVICE)\n",
    "            seq_len = input.size(1)\n",
    "            output = model(input)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += criterion(output_flat, target).item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 1\n",
    "\n",
    "final_model_params_path = os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loading and saving the new best model...\")\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        os.path.join(\"../models/\", \"shakespeare_transformer_model.pt\"),\n",
    "    )  # save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generate new text using test input\n",
    "\n",
    "max_new_tokens = 100\n",
    "idx = torch.LongTensor(text_dataset[\"test\"][\"input_ids\"][0]).unsqueeze(0).to(DEVICE)\n",
    "idx_gen = model.generate(idx, max_new_tokens, top_k=5)\n",
    "\n",
    "print(idx.shape, idx_gen.shape, end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx.tolist()[0]), end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx_gen.tolist()[0][-max_new_tokens:]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  torch.Size([1, 3000, 302]) torch.float16 False cpu\n",
      "\n",
      "mask: torch.Size([1, 302]) torch.bool False cpu\n",
      "\n",
      "Dictionary 1:\n",
      "100\n",
      "\n",
      "\n",
      "Dictionary 2:\n",
      "100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Get a neural dataset\n",
    "\n",
    "import random\n",
    "from data._utils import pick_worm\n",
    "from data._utils import load_dataset as load_neural_dataset\n",
    "\n",
    "# a synthetic dataset where the neural activity is the embeddings of tokens from the tiny_shakespeare\n",
    "Shakespeare0000 = load_neural_dataset(\"Shakespeare0000\")\n",
    "neural_dataset = pick_worm(Shakespeare0000, 0)\n",
    "\n",
    "data = neural_dataset[\"calcium_data\"].unsqueeze(0)\n",
    "print(\"data: \", data.shape, data.dtype, data.requires_grad, data.device, end=\"\\n\\n\")\n",
    "\n",
    "mask = neural_dataset[\"named_neurons_mask\"].unsqueeze(0)\n",
    "print(\"mask:\", mask.shape, mask.dtype, mask.requires_grad, mask.device, end=\"\\n\\n\")\n",
    "\n",
    "# Get the keys from the dictionary\n",
    "keys = list(Shakespeare0000.keys())\n",
    "\n",
    "# Randomly sample half the keys without replacement\n",
    "half_keys = random.sample(keys, len(keys) // 2)\n",
    "\n",
    "# Create two new dictionaries using the sampled keys\n",
    "train_Shakespeare0000 = {k: Shakespeare0000[k] for k in half_keys}\n",
    "validation_Shakespeare0000 = {k: Shakespeare0000[k] for k in keys if k not in half_keys}\n",
    "\n",
    "# Print the two dictionaries\n",
    "print(\"Dictionary 1:\")\n",
    "print(len(train_Shakespeare0000.keys()), end=\"\\n\\n\")\n",
    "print(\"\\nDictionary 2:\")\n",
    "print(len(validation_Shakespeare0000.keys()), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 2999, 302]) torch.float16 False cuda:0\n",
      "\n",
      "target: torch.Size([1, 2999, 302]) torch.float16 False cuda:0\n",
      "\n",
      "output: torch.Size([1, 2999, 1024]) torch.float16 True cuda:0\n",
      "\n",
      "Loss: 30.922\n"
     ]
    }
   ],
   "source": [
    "# @title Create a NeuralTransformer model\n",
    "\n",
    "from models._utils import NeuralTransformer\n",
    "\n",
    "model = NeuralTransformer(input_size=302, hidden_size=512).to(DEVICE)\n",
    "\n",
    "# test input-output functionality\n",
    "mask = mask.to(DEVICE)\n",
    "input = data[:, :-1, :].to(DEVICE)\n",
    "target = data[:, 1:, :].to(DEVICE)\n",
    "output = model(input, mask)\n",
    "\n",
    "print(\"input:\", input.shape, input.dtype, input.requires_grad, input.device, end=\"\\n\\n\")\n",
    "print(\n",
    "    \"target:\",\n",
    "    target.shape,\n",
    "    target.dtype,\n",
    "    target.requires_grad,\n",
    "    target.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"output:\",\n",
    "    output.shape,\n",
    "    output.dtype,\n",
    "    output.requires_grad,\n",
    "    output.device,\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# test optimization and computation graph\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "optimizer.zero_grad()\n",
    "loss = model.loss_fn()(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(f\"Loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train the NeuralTransformer model\n",
    "\n",
    "model = NeuralTransformer(input_size=302, hidden_size=512).to(DEVICE)\n",
    "lr = 5.0  # 1.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 10\n",
    "    start_time = time.time()\n",
    "    global epoch\n",
    "\n",
    "    num_batches = len(train_Shakespeare0000)\n",
    "    gen = iter(train_Shakespeare0000)\n",
    "    for batch in range(num_batches):\n",
    "        neural_dataset = pick_worm(train_Shakespeare0000, next(gen))\n",
    "        data = neural_dataset[\"calcium_data\"].unsqueeze(0)\n",
    "        mask = neural_dataset[\"named_neurons_mask\"].unsqueeze(0)\n",
    "        # parse into input and target\n",
    "        mask = mask.to(DEVICE)\n",
    "        input = data[:, :-1, :].to(DEVICE)\n",
    "        target = data[:, 1:, :].to(DEVICE)\n",
    "        # forward pass\n",
    "        output = model(input, mask)  # ``[batch_size=1, seq_len, ntokens]``\n",
    "        # backpropopagation step\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss_fn()(\n",
    "            output, target\n",
    "        )  # flatens output to ``[batch_size=1 * seq_len, ntokens]``\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(\n",
    "                f\"| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | \"\n",
    "                f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\"\n",
    "            )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(validation_Shakespeare0000)\n",
    "        gen = iter(validation_Shakespeare0000)\n",
    "        for batch in range(num_batches):\n",
    "            neural_dataset = pick_worm(validation_Shakespeare0000, next(gen))\n",
    "            data = neural_dataset[\"calcium_data\"].unsqueeze(0)\n",
    "            mask = neural_dataset[\"named_neurons_mask\"].unsqueeze(0)\n",
    "            mask = mask.to(DEVICE)\n",
    "            input = data[:, :-1, :].to(DEVICE)\n",
    "            target = data[:, 1:, :].to(DEVICE)\n",
    "            output = model(input, mask)\n",
    "            loss = model.loss_fn()(output, target)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    10/  100 batches | lr 5.00 | ms/batch 31.58 | loss 117.90 | ppl 1602051919510640008154071466837369019025317540397056.00\n",
      "| epoch   1 |    20/  100 batches | lr 5.00 | ms/batch 26.22 | loss 84.67 | ppl 5904333968710534549943895877155291136.00\n",
      "| epoch   1 |    30/  100 batches | lr 5.00 | ms/batch 25.65 | loss 57.25 | ppl 7323459079081677318782976.00\n",
      "| epoch   1 |    40/  100 batches | lr 5.00 | ms/batch 26.66 | loss 44.97 | ppl 33753810886753910784.00\n",
      "| epoch   1 |    50/  100 batches | lr 5.00 | ms/batch 24.82 | loss 38.43 | ppl 49185487235151568.00\n",
      "| epoch   1 |    60/  100 batches | lr 5.00 | ms/batch 24.55 | loss 36.13 | ppl 4915893881348630.00\n",
      "| epoch   1 |    70/  100 batches | lr 5.00 | ms/batch 24.43 | loss 33.96 | ppl 558485627501420.00\n",
      "| epoch   1 |    80/  100 batches | lr 5.00 | ms/batch 24.37 | loss 38.41 | ppl 47971093228386088.00\n",
      "| epoch   1 |    90/  100 batches | lr 5.00 | ms/batch 24.44 | loss 31.65 | ppl 55818417858292.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  4.58s | valid loss 21.50 | valid ppl 2183893216.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    10/  100 batches | lr 4.75 | ms/batch 26.88 | loss 30.10 | ppl 11773531091718.98\n",
      "| epoch   2 |    20/  100 batches | lr 4.75 | ms/batch 24.31 | loss 25.25 | ppl 92745498169.51\n",
      "| epoch   2 |    30/  100 batches | lr 4.75 | ms/batch 25.17 | loss 27.73 | ppl 1098536549052.99\n",
      "| epoch   2 |    40/  100 batches | lr 4.75 | ms/batch 25.35 | loss 28.40 | ppl 2157562007648.18\n",
      "| epoch   2 |    50/  100 batches | lr 4.75 | ms/batch 25.82 | loss 23.41 | ppl 14674468735.57\n",
      "| epoch   2 |    60/  100 batches | lr 4.75 | ms/batch 25.25 | loss 22.28 | ppl 4745529006.79\n",
      "| epoch   2 |    70/  100 batches | lr 4.75 | ms/batch 24.44 | loss 26.12 | ppl 221271490871.56\n",
      "| epoch   2 |    80/  100 batches | lr 4.75 | ms/batch 24.78 | loss 21.56 | ppl 2310979967.79\n",
      "| epoch   2 |    90/  100 batches | lr 4.75 | ms/batch 24.50 | loss 20.74 | ppl 1019101812.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  4.50s | valid loss 21.18 | valid ppl 1572262104.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    10/  100 batches | lr 4.51 | ms/batch 26.92 | loss 23.29 | ppl 13031365249.60\n",
      "| epoch   3 |    20/  100 batches | lr 4.51 | ms/batch 24.35 | loss 18.28 | ppl 86646202.72\n",
      "| epoch   3 |    30/  100 batches | lr 4.51 | ms/batch 24.68 | loss 16.20 | ppl 10853519.90\n",
      "| epoch   3 |    40/  100 batches | lr 4.51 | ms/batch 24.59 | loss 16.79 | ppl 19546000.59\n",
      "| epoch   3 |    50/  100 batches | lr 4.51 | ms/batch 25.65 | loss 19.19 | ppl 215964586.32\n",
      "| epoch   3 |    60/  100 batches | lr 4.51 | ms/batch 25.86 | loss 16.39 | ppl 13091844.01\n",
      "| epoch   3 |    70/  100 batches | lr 4.51 | ms/batch 25.22 | loss 16.44 | ppl 13741589.19\n",
      "| epoch   3 |    80/  100 batches | lr 4.51 | ms/batch 24.41 | loss 17.09 | ppl 26425598.80\n",
      "| epoch   3 |    90/  100 batches | lr 4.51 | ms/batch 24.57 | loss 13.85 | ppl 1030250.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  4.51s | valid loss 17.46 | valid ppl 38167693.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    10/  100 batches | lr 4.29 | ms/batch 27.30 | loss 24.00 | ppl 26385850829.60\n",
      "| epoch   4 |    20/  100 batches | lr 4.29 | ms/batch 24.75 | loss 14.59 | ppl 2160688.55\n",
      "| epoch   4 |    30/  100 batches | lr 4.29 | ms/batch 24.92 | loss 12.57 | ppl 289236.51\n",
      "| epoch   4 |    40/  100 batches | lr 4.29 | ms/batch 24.38 | loss 13.02 | ppl 450434.86\n",
      "| epoch   4 |    50/  100 batches | lr 4.29 | ms/batch 24.37 | loss 13.96 | ppl 1153825.79\n",
      "| epoch   4 |    60/  100 batches | lr 4.29 | ms/batch 25.13 | loss 11.53 | ppl 101214.77\n",
      "| epoch   4 |    70/  100 batches | lr 4.29 | ms/batch 25.33 | loss 14.38 | ppl 1751830.63\n",
      "| epoch   4 |    80/  100 batches | lr 4.29 | ms/batch 25.77 | loss 11.38 | ppl 87150.40\n",
      "| epoch   4 |    90/  100 batches | lr 4.29 | ms/batch 25.34 | loss 11.85 | ppl 140029.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  6.06s | valid loss  8.89 | valid ppl  7257.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    10/  100 batches | lr 4.07 | ms/batch 39.99 | loss 10.49 | ppl 36061.05\n",
      "| epoch   5 |    20/  100 batches | lr 4.07 | ms/batch 32.87 | loss 10.91 | ppl 54986.54\n",
      "| epoch   5 |    30/  100 batches | lr 4.07 | ms/batch 70.80 | loss 11.71 | ppl 121612.34\n",
      "| epoch   5 |    40/  100 batches | lr 4.07 | ms/batch 49.25 | loss 11.32 | ppl 82190.65\n",
      "| epoch   5 |    50/  100 batches | lr 4.07 | ms/batch 31.21 | loss 12.05 | ppl 171500.89\n",
      "| epoch   5 |    60/  100 batches | lr 4.07 | ms/batch 24.44 | loss  9.82 | ppl 18439.78\n",
      "| epoch   5 |    70/  100 batches | lr 4.07 | ms/batch 24.84 | loss  8.79 | ppl  6559.51\n",
      "| epoch   5 |    80/  100 batches | lr 4.07 | ms/batch 25.41 | loss  9.93 | ppl 20546.97\n",
      "| epoch   5 |    90/  100 batches | lr 4.07 | ms/batch 25.90 | loss  8.86 | ppl  7078.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  5.50s | valid loss  6.78 | valid ppl   876.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    10/  100 batches | lr 3.87 | ms/batch 27.79 | loss  8.77 | ppl  6455.30\n",
      "| epoch   6 |    20/  100 batches | lr 3.87 | ms/batch 24.78 | loss  7.91 | ppl  2736.55\n",
      "| epoch   6 |    30/  100 batches | lr 3.87 | ms/batch 24.31 | loss  7.24 | ppl  1388.44\n",
      "| epoch   6 |    40/  100 batches | lr 3.87 | ms/batch 24.44 | loss  7.61 | ppl  2024.91\n",
      "| epoch   6 |    50/  100 batches | lr 3.87 | ms/batch 24.39 | loss  7.56 | ppl  1911.17\n",
      "| epoch   6 |    60/  100 batches | lr 3.87 | ms/batch 24.47 | loss  8.06 | ppl  3169.50\n",
      "| epoch   6 |    70/  100 batches | lr 3.87 | ms/batch 24.41 | loss  8.52 | ppl  5011.70\n",
      "| epoch   6 |    80/  100 batches | lr 3.87 | ms/batch 24.36 | loss  7.94 | ppl  2815.71\n",
      "| epoch   6 |    90/  100 batches | lr 3.87 | ms/batch 25.15 | loss  7.05 | ppl  1157.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  4.50s | valid loss  5.56 | valid ppl   259.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    10/  100 batches | lr 3.68 | ms/batch 27.87 | loss  6.68 | ppl   793.28\n",
      "| epoch   7 |    20/  100 batches | lr 3.68 | ms/batch 25.55 | loss  6.28 | ppl   532.37\n",
      "| epoch   7 |    30/  100 batches | lr 3.68 | ms/batch 25.96 | loss  6.39 | ppl   595.76\n",
      "| epoch   7 |    40/  100 batches | lr 3.68 | ms/batch 24.49 | loss  6.61 | ppl   742.89\n",
      "| epoch   7 |    50/  100 batches | lr 3.68 | ms/batch 24.69 | loss  7.63 | ppl  2064.04\n",
      "| epoch   7 |    60/  100 batches | lr 3.68 | ms/batch 24.73 | loss  6.49 | ppl   661.51\n",
      "| epoch   7 |    70/  100 batches | lr 3.68 | ms/batch 24.44 | loss  6.19 | ppl   486.25\n",
      "| epoch   7 |    80/  100 batches | lr 3.68 | ms/batch 24.39 | loss  5.65 | ppl   283.29\n",
      "| epoch   7 |    90/  100 batches | lr 3.68 | ms/batch 24.42 | loss  6.10 | ppl   446.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  4.52s | valid loss  6.19 | valid ppl   486.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    10/  100 batches | lr 3.49 | ms/batch 26.98 | loss  6.39 | ppl   596.00\n",
      "| epoch   8 |    20/  100 batches | lr 3.49 | ms/batch 25.18 | loss  5.24 | ppl   187.76\n",
      "| epoch   8 |    30/  100 batches | lr 3.49 | ms/batch 26.00 | loss  5.78 | ppl   323.41\n",
      "| epoch   8 |    40/  100 batches | lr 3.49 | ms/batch 25.71 | loss  5.59 | ppl   268.85\n",
      "| epoch   8 |    50/  100 batches | lr 3.49 | ms/batch 25.36 | loss  5.48 | ppl   240.90\n",
      "| epoch   8 |    60/  100 batches | lr 3.49 | ms/batch 24.36 | loss  5.53 | ppl   252.36\n",
      "| epoch   8 |    70/  100 batches | lr 3.49 | ms/batch 24.52 | loss  5.57 | ppl   261.80\n",
      "| epoch   8 |    80/  100 batches | lr 3.49 | ms/batch 24.38 | loss  4.95 | ppl   140.90\n",
      "| epoch   8 |    90/  100 batches | lr 3.49 | ms/batch 24.39 | loss  5.47 | ppl   237.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  4.52s | valid loss  4.52 | valid ppl    92.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    10/  100 batches | lr 3.32 | ms/batch 26.91 | loss  5.40 | ppl   221.45\n",
      "| epoch   9 |    20/  100 batches | lr 3.32 | ms/batch 24.68 | loss  4.83 | ppl   125.03\n",
      "| epoch   9 |    30/  100 batches | lr 3.32 | ms/batch 24.38 | loss  5.10 | ppl   164.09\n",
      "| epoch   9 |    40/  100 batches | lr 3.32 | ms/batch 25.20 | loss  4.73 | ppl   113.30\n",
      "| epoch   9 |    50/  100 batches | lr 3.32 | ms/batch 25.45 | loss  4.74 | ppl   114.86\n",
      "| epoch   9 |    60/  100 batches | lr 3.32 | ms/batch 26.20 | loss  4.88 | ppl   131.98\n",
      "| epoch   9 |    70/  100 batches | lr 3.32 | ms/batch 24.70 | loss  4.81 | ppl   123.23\n",
      "| epoch   9 |    80/  100 batches | lr 3.32 | ms/batch 24.57 | loss  4.69 | ppl   109.11\n",
      "| epoch   9 |    90/  100 batches | lr 3.32 | ms/batch 24.54 | loss  4.99 | ppl   146.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  4.52s | valid loss  4.57 | valid ppl    96.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    10/  100 batches | lr 3.15 | ms/batch 27.00 | loss  4.86 | ppl   128.84\n",
      "| epoch  10 |    20/  100 batches | lr 3.15 | ms/batch 24.44 | loss  4.21 | ppl    67.55\n",
      "| epoch  10 |    30/  100 batches | lr 3.15 | ms/batch 24.39 | loss  4.73 | ppl   113.66\n",
      "| epoch  10 |    40/  100 batches | lr 3.15 | ms/batch 24.43 | loss  4.24 | ppl    69.57\n",
      "| epoch  10 |    50/  100 batches | lr 3.15 | ms/batch 24.83 | loss  4.07 | ppl    58.56\n",
      "| epoch  10 |    60/  100 batches | lr 3.15 | ms/batch 25.19 | loss  4.33 | ppl    75.80\n",
      "| epoch  10 |    70/  100 batches | lr 3.15 | ms/batch 25.54 | loss  4.24 | ppl    69.55\n",
      "| epoch  10 |    80/  100 batches | lr 3.15 | ms/batch 25.73 | loss  4.15 | ppl    63.32\n",
      "| epoch  10 |    90/  100 batches | lr 3.15 | ms/batch 24.46 | loss  4.52 | ppl    92.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  4.91s | valid loss  4.12 | valid ppl    61.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    10/  100 batches | lr 2.99 | ms/batch 27.01 | loss  4.66 | ppl   105.51\n",
      "| epoch  11 |    20/  100 batches | lr 2.99 | ms/batch 24.36 | loss  3.94 | ppl    51.27\n",
      "| epoch  11 |    30/  100 batches | lr 2.99 | ms/batch 24.33 | loss  4.25 | ppl    70.04\n",
      "| epoch  11 |    40/  100 batches | lr 2.99 | ms/batch 24.38 | loss  3.94 | ppl    51.26\n",
      "| epoch  11 |    50/  100 batches | lr 2.99 | ms/batch 24.71 | loss  3.94 | ppl    51.19\n",
      "| epoch  11 |    60/  100 batches | lr 2.99 | ms/batch 25.32 | loss  3.94 | ppl    51.29\n",
      "| epoch  11 |    70/  100 batches | lr 2.99 | ms/batch 25.69 | loss  3.88 | ppl    48.18\n",
      "| epoch  11 |    80/  100 batches | lr 2.99 | ms/batch 25.64 | loss  3.71 | ppl    41.04\n",
      "| epoch  11 |    90/  100 batches | lr 2.99 | ms/batch 24.39 | loss  4.16 | ppl    64.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  4.51s | valid loss  3.76 | valid ppl    42.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    10/  100 batches | lr 2.84 | ms/batch 26.99 | loss  4.09 | ppl    60.03\n",
      "| epoch  12 |    20/  100 batches | lr 2.84 | ms/batch 24.51 | loss  3.60 | ppl    36.51\n",
      "| epoch  12 |    30/  100 batches | lr 2.84 | ms/batch 24.35 | loss  3.83 | ppl    45.95\n",
      "| epoch  12 |    40/  100 batches | lr 2.84 | ms/batch 24.36 | loss  3.55 | ppl    34.98\n",
      "| epoch  12 |    50/  100 batches | lr 2.84 | ms/batch 24.35 | loss  3.57 | ppl    35.55\n",
      "| epoch  12 |    60/  100 batches | lr 2.84 | ms/batch 24.37 | loss  3.69 | ppl    39.98\n",
      "| epoch  12 |    70/  100 batches | lr 2.84 | ms/batch 25.27 | loss  3.57 | ppl    35.68\n",
      "| epoch  12 |    80/  100 batches | lr 2.84 | ms/batch 25.56 | loss  3.46 | ppl    31.81\n",
      "| epoch  12 |    90/  100 batches | lr 2.84 | ms/batch 26.20 | loss  3.76 | ppl    42.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  4.50s | valid loss  3.30 | valid ppl    27.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    10/  100 batches | lr 2.70 | ms/batch 28.73 | loss  3.83 | ppl    45.99\n",
      "| epoch  13 |    20/  100 batches | lr 2.70 | ms/batch 24.52 | loss  3.24 | ppl    25.55\n",
      "| epoch  13 |    30/  100 batches | lr 2.70 | ms/batch 24.55 | loss  3.64 | ppl    37.91\n",
      "| epoch  13 |    40/  100 batches | lr 2.70 | ms/batch 24.43 | loss  3.50 | ppl    33.28\n",
      "| epoch  13 |    50/  100 batches | lr 2.70 | ms/batch 24.38 | loss  3.31 | ppl    27.28\n",
      "| epoch  13 |    60/  100 batches | lr 2.70 | ms/batch 24.39 | loss  3.45 | ppl    31.46\n",
      "| epoch  13 |    70/  100 batches | lr 2.70 | ms/batch 24.40 | loss  3.35 | ppl    28.37\n",
      "| epoch  13 |    80/  100 batches | lr 2.70 | ms/batch 24.37 | loss  3.13 | ppl    22.82\n",
      "| epoch  13 |    90/  100 batches | lr 2.70 | ms/batch 25.25 | loss  3.55 | ppl    34.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  4.51s | valid loss  3.46 | valid ppl    31.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    10/  100 batches | lr 2.57 | ms/batch 27.95 | loss  3.53 | ppl    34.17\n",
      "| epoch  14 |    20/  100 batches | lr 2.57 | ms/batch 26.06 | loss  3.13 | ppl    22.86\n",
      "| epoch  14 |    30/  100 batches | lr 2.57 | ms/batch 24.85 | loss  3.42 | ppl    30.64\n",
      "| epoch  14 |    40/  100 batches | lr 2.57 | ms/batch 24.43 | loss  3.16 | ppl    23.49\n",
      "| epoch  14 |    50/  100 batches | lr 2.57 | ms/batch 24.38 | loss  3.02 | ppl    20.46\n",
      "| epoch  14 |    60/  100 batches | lr 2.57 | ms/batch 24.47 | loss  3.19 | ppl    24.34\n",
      "| epoch  14 |    70/  100 batches | lr 2.57 | ms/batch 24.41 | loss  3.08 | ppl    21.85\n",
      "| epoch  14 |    80/  100 batches | lr 2.57 | ms/batch 24.41 | loss  3.09 | ppl    22.07\n",
      "| epoch  14 |    90/  100 batches | lr 2.57 | ms/batch 24.54 | loss  3.27 | ppl    26.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  4.52s | valid loss  3.21 | valid ppl    24.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    10/  100 batches | lr 2.44 | ms/batch 26.95 | loss  3.38 | ppl    29.40\n",
      "| epoch  15 |    20/  100 batches | lr 2.44 | ms/batch 24.85 | loss  2.90 | ppl    18.21\n",
      "| epoch  15 |    30/  100 batches | lr 2.44 | ms/batch 25.23 | loss  3.19 | ppl    24.23\n",
      "| epoch  15 |    40/  100 batches | lr 2.44 | ms/batch 25.71 | loss  2.94 | ppl    18.85\n",
      "| epoch  15 |    50/  100 batches | lr 2.44 | ms/batch 25.40 | loss  2.81 | ppl    16.67\n",
      "| epoch  15 |    60/  100 batches | lr 2.44 | ms/batch 24.35 | loss  3.00 | ppl    20.12\n",
      "| epoch  15 |    70/  100 batches | lr 2.44 | ms/batch 24.56 | loss  2.96 | ppl    19.31\n",
      "| epoch  15 |    80/  100 batches | lr 2.44 | ms/batch 24.38 | loss  2.82 | ppl    16.77\n",
      "| epoch  15 |    90/  100 batches | lr 2.44 | ms/batch 24.45 | loss  2.99 | ppl    19.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  4.52s | valid loss  2.82 | valid ppl    16.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |    10/  100 batches | lr 2.32 | ms/batch 27.01 | loss  3.05 | ppl    21.08\n",
      "| epoch  16 |    20/  100 batches | lr 2.32 | ms/batch 24.40 | loss  2.70 | ppl    14.95\n",
      "| epoch  16 |    30/  100 batches | lr 2.32 | ms/batch 24.44 | loss  2.99 | ppl    19.93\n",
      "| epoch  16 |    40/  100 batches | lr 2.32 | ms/batch 25.37 | loss  2.68 | ppl    14.66\n",
      "| epoch  16 |    50/  100 batches | lr 2.32 | ms/batch 25.47 | loss  2.80 | ppl    16.46\n",
      "| epoch  16 |    60/  100 batches | lr 2.32 | ms/batch 26.02 | loss  2.79 | ppl    16.23\n",
      "| epoch  16 |    70/  100 batches | lr 2.32 | ms/batch 24.58 | loss  2.70 | ppl    14.91\n",
      "| epoch  16 |    80/  100 batches | lr 2.32 | ms/batch 24.46 | loss  2.75 | ppl    15.67\n",
      "| epoch  16 |    90/  100 batches | lr 2.32 | ms/batch 24.65 | loss  2.90 | ppl    18.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  4.53s | valid loss  2.67 | valid ppl    14.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    10/  100 batches | lr 2.20 | ms/batch 27.03 | loss  2.94 | ppl    18.93\n",
      "| epoch  17 |    20/  100 batches | lr 2.20 | ms/batch 24.40 | loss  2.56 | ppl    12.98\n",
      "| epoch  17 |    30/  100 batches | lr 2.20 | ms/batch 24.32 | loss  2.80 | ppl    16.38\n",
      "| epoch  17 |    40/  100 batches | lr 2.20 | ms/batch 24.38 | loss  2.67 | ppl    14.40\n",
      "| epoch  17 |    50/  100 batches | lr 2.20 | ms/batch 25.12 | loss  2.57 | ppl    13.09\n",
      "| epoch  17 |    60/  100 batches | lr 2.20 | ms/batch 25.21 | loss  2.68 | ppl    14.65\n",
      "| epoch  17 |    70/  100 batches | lr 2.20 | ms/batch 26.14 | loss  2.55 | ppl    12.85\n",
      "| epoch  17 |    80/  100 batches | lr 2.20 | ms/batch 24.53 | loss  2.64 | ppl    14.01\n",
      "| epoch  17 |    90/  100 batches | lr 2.20 | ms/batch 24.79 | loss  2.67 | ppl    14.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  4.53s | valid loss  2.34 | valid ppl    10.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |    10/  100 batches | lr 2.09 | ms/batch 27.00 | loss  2.78 | ppl    16.18\n",
      "| epoch  18 |    20/  100 batches | lr 2.09 | ms/batch 24.81 | loss  2.47 | ppl    11.84\n",
      "| epoch  18 |    30/  100 batches | lr 2.09 | ms/batch 24.62 | loss  2.70 | ppl    14.86\n",
      "| epoch  18 |    40/  100 batches | lr 2.09 | ms/batch 24.33 | loss  2.46 | ppl    11.74\n",
      "| epoch  18 |    50/  100 batches | lr 2.09 | ms/batch 24.39 | loss  2.51 | ppl    12.31\n",
      "| epoch  18 |    60/  100 batches | lr 2.09 | ms/batch 24.37 | loss  2.51 | ppl    12.34\n",
      "| epoch  18 |    70/  100 batches | lr 2.09 | ms/batch 25.45 | loss  2.42 | ppl    11.29\n",
      "| epoch  18 |    80/  100 batches | lr 2.09 | ms/batch 25.33 | loss  2.48 | ppl    11.91\n",
      "| epoch  18 |    90/  100 batches | lr 2.09 | ms/batch 26.14 | loss  2.52 | ppl    12.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  4.54s | valid loss  2.37 | valid ppl    10.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |    10/  100 batches | lr 1.99 | ms/batch 28.18 | loss  2.66 | ppl    14.35\n",
      "| epoch  19 |    20/  100 batches | lr 1.99 | ms/batch 24.44 | loss  2.34 | ppl    10.33\n",
      "| epoch  19 |    30/  100 batches | lr 1.99 | ms/batch 24.44 | loss  2.58 | ppl    13.14\n",
      "| epoch  19 |    40/  100 batches | lr 1.99 | ms/batch 24.39 | loss  2.43 | ppl    11.40\n",
      "| epoch  19 |    50/  100 batches | lr 1.99 | ms/batch 24.39 | loss  2.38 | ppl    10.81\n",
      "| epoch  19 |    60/  100 batches | lr 1.99 | ms/batch 24.37 | loss  2.40 | ppl    11.05\n",
      "| epoch  19 |    70/  100 batches | lr 1.99 | ms/batch 24.35 | loss  2.35 | ppl    10.52\n",
      "| epoch  19 |    80/  100 batches | lr 1.99 | ms/batch 24.85 | loss  2.37 | ppl    10.69\n",
      "| epoch  19 |    90/  100 batches | lr 1.99 | ms/batch 25.67 | loss  2.44 | ppl    11.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  4.52s | valid loss  2.21 | valid ppl     9.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    10/  100 batches | lr 1.89 | ms/batch 28.05 | loss  2.51 | ppl    12.25\n",
      "| epoch  20 |    20/  100 batches | lr 1.89 | ms/batch 25.85 | loss  2.26 | ppl     9.59\n",
      "| epoch  20 |    30/  100 batches | lr 1.89 | ms/batch 24.43 | loss  2.49 | ppl    12.10\n",
      "| epoch  20 |    40/  100 batches | lr 1.89 | ms/batch 24.37 | loss  2.33 | ppl    10.25\n",
      "| epoch  20 |    50/  100 batches | lr 1.89 | ms/batch 24.40 | loss  2.26 | ppl     9.59\n",
      "| epoch  20 |    60/  100 batches | lr 1.89 | ms/batch 24.38 | loss  2.36 | ppl    10.56\n",
      "| epoch  20 |    70/  100 batches | lr 1.89 | ms/batch 24.40 | loss  2.26 | ppl     9.55\n",
      "| epoch  20 |    80/  100 batches | lr 1.89 | ms/batch 24.46 | loss  2.21 | ppl     9.08\n",
      "| epoch  20 |    90/  100 batches | lr 1.89 | ms/batch 24.38 | loss  2.33 | ppl    10.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  4.51s | valid loss  2.15 | valid ppl     8.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |    10/  100 batches | lr 1.79 | ms/batch 27.02 | loss  2.39 | ppl    10.90\n",
      "| epoch  21 |    20/  100 batches | lr 1.79 | ms/batch 25.44 | loss  2.20 | ppl     9.03\n",
      "| epoch  21 |    30/  100 batches | lr 1.79 | ms/batch 25.64 | loss  2.39 | ppl    10.94\n",
      "| epoch  21 |    40/  100 batches | lr 1.79 | ms/batch 25.76 | loss  2.21 | ppl     9.07\n",
      "| epoch  21 |    50/  100 batches | lr 1.79 | ms/batch 24.50 | loss  2.19 | ppl     8.93\n",
      "| epoch  21 |    60/  100 batches | lr 1.79 | ms/batch 24.46 | loss  2.25 | ppl     9.53\n",
      "| epoch  21 |    70/  100 batches | lr 1.79 | ms/batch 24.40 | loss  2.15 | ppl     8.59\n",
      "| epoch  21 |    80/  100 batches | lr 1.79 | ms/batch 24.38 | loss  2.18 | ppl     8.82\n",
      "| epoch  21 |    90/  100 batches | lr 1.79 | ms/batch 24.34 | loss  2.23 | ppl     9.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  4.52s | valid loss  2.03 | valid ppl     7.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |    10/  100 batches | lr 1.70 | ms/batch 26.99 | loss  2.36 | ppl    10.61\n",
      "| epoch  22 |    20/  100 batches | lr 1.70 | ms/batch 24.47 | loss  2.08 | ppl     7.98\n",
      "| epoch  22 |    30/  100 batches | lr 1.70 | ms/batch 24.93 | loss  2.30 | ppl    10.01\n",
      "| epoch  22 |    40/  100 batches | lr 1.70 | ms/batch 25.24 | loss  2.16 | ppl     8.65\n",
      "| epoch  22 |    50/  100 batches | lr 1.70 | ms/batch 25.78 | loss  2.14 | ppl     8.51\n",
      "| epoch  22 |    60/  100 batches | lr 1.70 | ms/batch 25.50 | loss  2.16 | ppl     8.68\n",
      "| epoch  22 |    70/  100 batches | lr 1.70 | ms/batch 24.36 | loss  2.06 | ppl     7.82\n",
      "| epoch  22 |    80/  100 batches | lr 1.70 | ms/batch 24.51 | loss  2.11 | ppl     8.25\n",
      "| epoch  22 |    90/  100 batches | lr 1.70 | ms/batch 24.43 | loss  2.17 | ppl     8.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  4.53s | valid loss  2.03 | valid ppl     7.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |    10/  100 batches | lr 1.62 | ms/batch 26.97 | loss  2.28 | ppl     9.74\n",
      "| epoch  23 |    20/  100 batches | lr 1.62 | ms/batch 24.45 | loss  2.03 | ppl     7.59\n",
      "| epoch  23 |    30/  100 batches | lr 1.62 | ms/batch 24.43 | loss  2.18 | ppl     8.89\n",
      "| epoch  23 |    40/  100 batches | lr 1.62 | ms/batch 24.51 | loss  2.03 | ppl     7.65\n",
      "| epoch  23 |    50/  100 batches | lr 1.62 | ms/batch 25.02 | loss  2.04 | ppl     7.66\n",
      "| epoch  23 |    60/  100 batches | lr 1.62 | ms/batch 25.17 | loss  2.12 | ppl     8.32\n",
      "| epoch  23 |    70/  100 batches | lr 1.62 | ms/batch 25.94 | loss  1.99 | ppl     7.32\n",
      "| epoch  23 |    80/  100 batches | lr 1.62 | ms/batch 25.10 | loss  2.09 | ppl     8.07\n",
      "| epoch  23 |    90/  100 batches | lr 1.62 | ms/batch 24.32 | loss  2.10 | ppl     8.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  4.52s | valid loss  1.99 | valid ppl     7.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |    10/  100 batches | lr 1.54 | ms/batch 27.06 | loss  2.20 | ppl     9.04\n",
      "| epoch  24 |    20/  100 batches | lr 1.54 | ms/batch 24.46 | loss  1.94 | ppl     6.99\n",
      "| epoch  24 |    30/  100 batches | lr 1.54 | ms/batch 24.40 | loss  2.11 | ppl     8.23\n",
      "| epoch  24 |    40/  100 batches | lr 1.54 | ms/batch 24.39 | loss  2.00 | ppl     7.41\n",
      "| epoch  24 |    50/  100 batches | lr 1.54 | ms/batch 24.39 | loss  1.99 | ppl     7.35\n",
      "| epoch  24 |    60/  100 batches | lr 1.54 | ms/batch 24.45 | loss  2.04 | ppl     7.66\n",
      "| epoch  24 |    70/  100 batches | lr 1.54 | ms/batch 25.68 | loss  1.92 | ppl     6.84\n",
      "| epoch  24 |    80/  100 batches | lr 1.54 | ms/batch 25.65 | loss  1.93 | ppl     6.87\n",
      "| epoch  24 |    90/  100 batches | lr 1.54 | ms/batch 25.53 | loss  2.03 | ppl     7.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  4.52s | valid loss  1.92 | valid ppl     6.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |    10/  100 batches | lr 1.46 | ms/batch 27.62 | loss  2.15 | ppl     8.61\n",
      "| epoch  25 |    20/  100 batches | lr 1.46 | ms/batch 24.36 | loss  1.89 | ppl     6.64\n",
      "| epoch  25 |    30/  100 batches | lr 1.46 | ms/batch 24.72 | loss  2.05 | ppl     7.79\n",
      "| epoch  25 |    40/  100 batches | lr 1.46 | ms/batch 24.34 | loss  1.95 | ppl     7.02\n",
      "| epoch  25 |    50/  100 batches | lr 1.46 | ms/batch 24.37 | loss  1.93 | ppl     6.91\n",
      "| epoch  25 |    60/  100 batches | lr 1.46 | ms/batch 24.33 | loss  1.97 | ppl     7.16\n",
      "| epoch  25 |    70/  100 batches | lr 1.46 | ms/batch 24.43 | loss  1.85 | ppl     6.36\n",
      "| epoch  25 |    80/  100 batches | lr 1.46 | ms/batch 25.30 | loss  1.89 | ppl     6.60\n",
      "| epoch  25 |    90/  100 batches | lr 1.46 | ms/batch 25.09 | loss  1.96 | ppl     7.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  4.52s | valid loss  1.85 | valid ppl     6.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |    10/  100 batches | lr 1.39 | ms/batch 28.34 | loss  2.07 | ppl     7.95\n",
      "| epoch  26 |    20/  100 batches | lr 1.39 | ms/batch 25.70 | loss  1.85 | ppl     6.34\n",
      "| epoch  26 |    30/  100 batches | lr 1.39 | ms/batch 24.39 | loss  2.02 | ppl     7.53\n",
      "| epoch  26 |    40/  100 batches | lr 1.39 | ms/batch 24.49 | loss  1.89 | ppl     6.62\n",
      "| epoch  26 |    50/  100 batches | lr 1.39 | ms/batch 24.45 | loss  1.87 | ppl     6.52\n",
      "| epoch  26 |    60/  100 batches | lr 1.39 | ms/batch 24.55 | loss  1.90 | ppl     6.70\n",
      "| epoch  26 |    70/  100 batches | lr 1.39 | ms/batch 24.38 | loss  1.83 | ppl     6.22\n",
      "| epoch  26 |    80/  100 batches | lr 1.39 | ms/batch 24.42 | loss  1.85 | ppl     6.37\n",
      "| epoch  26 |    90/  100 batches | lr 1.39 | ms/batch 24.55 | loss  1.92 | ppl     6.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  4.53s | valid loss  1.76 | valid ppl     5.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |    10/  100 batches | lr 1.32 | ms/batch 27.57 | loss  1.99 | ppl     7.33\n",
      "| epoch  27 |    20/  100 batches | lr 1.32 | ms/batch 25.23 | loss  1.81 | ppl     6.14\n",
      "| epoch  27 |    30/  100 batches | lr 1.32 | ms/batch 25.95 | loss  1.96 | ppl     7.12\n",
      "| epoch  27 |    40/  100 batches | lr 1.32 | ms/batch 25.55 | loss  1.85 | ppl     6.37\n",
      "| epoch  27 |    50/  100 batches | lr 1.32 | ms/batch 24.43 | loss  1.83 | ppl     6.25\n",
      "| epoch  27 |    60/  100 batches | lr 1.32 | ms/batch 24.55 | loss  1.86 | ppl     6.42\n",
      "| epoch  27 |    70/  100 batches | lr 1.32 | ms/batch 24.43 | loss  1.80 | ppl     6.04\n",
      "| epoch  27 |    80/  100 batches | lr 1.32 | ms/batch 24.43 | loss  1.82 | ppl     6.16\n",
      "| epoch  27 |    90/  100 batches | lr 1.32 | ms/batch 24.58 | loss  1.86 | ppl     6.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  4.53s | valid loss  1.75 | valid ppl     5.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |    10/  100 batches | lr 1.25 | ms/batch 26.94 | loss  1.96 | ppl     7.08\n",
      "| epoch  28 |    20/  100 batches | lr 1.25 | ms/batch 24.38 | loss  1.77 | ppl     5.85\n",
      "| epoch  28 |    30/  100 batches | lr 1.25 | ms/batch 25.23 | loss  1.89 | ppl     6.64\n",
      "| epoch  28 |    40/  100 batches | lr 1.25 | ms/batch 25.29 | loss  1.81 | ppl     6.09\n",
      "| epoch  28 |    50/  100 batches | lr 1.25 | ms/batch 26.17 | loss  1.78 | ppl     5.95\n",
      "| epoch  28 |    60/  100 batches | lr 1.25 | ms/batch 24.70 | loss  1.84 | ppl     6.30\n",
      "| epoch  28 |    70/  100 batches | lr 1.25 | ms/batch 24.47 | loss  1.75 | ppl     5.75\n",
      "| epoch  28 |    80/  100 batches | lr 1.25 | ms/batch 24.60 | loss  1.76 | ppl     5.82\n",
      "| epoch  28 |    90/  100 batches | lr 1.25 | ms/batch 24.37 | loss  1.81 | ppl     6.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  4.53s | valid loss  1.74 | valid ppl     5.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |    10/  100 batches | lr 1.19 | ms/batch 27.03 | loss  1.92 | ppl     6.80\n",
      "| epoch  29 |    20/  100 batches | lr 1.19 | ms/batch 24.44 | loss  1.74 | ppl     5.70\n",
      "| epoch  29 |    30/  100 batches | lr 1.19 | ms/batch 24.41 | loss  1.86 | ppl     6.40\n",
      "| epoch  29 |    40/  100 batches | lr 1.19 | ms/batch 25.01 | loss  1.76 | ppl     5.83\n",
      "| epoch  29 |    50/  100 batches | lr 1.19 | ms/batch 25.48 | loss  1.72 | ppl     5.59\n",
      "| epoch  29 |    60/  100 batches | lr 1.19 | ms/batch 26.08 | loss  1.81 | ppl     6.13\n",
      "| epoch  29 |    70/  100 batches | lr 1.19 | ms/batch 25.01 | loss  1.71 | ppl     5.52\n",
      "| epoch  29 |    80/  100 batches | lr 1.19 | ms/batch 24.57 | loss  1.74 | ppl     5.68\n",
      "| epoch  29 |    90/  100 batches | lr 1.19 | ms/batch 24.48 | loss  1.79 | ppl     6.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  4.54s | valid loss  1.68 | valid ppl     5.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |    10/  100 batches | lr 1.13 | ms/batch 27.19 | loss  1.87 | ppl     6.51\n",
      "| epoch  30 |    20/  100 batches | lr 1.13 | ms/batch 24.43 | loss  1.70 | ppl     5.50\n",
      "| epoch  30 |    30/  100 batches | lr 1.13 | ms/batch 24.55 | loss  1.83 | ppl     6.26\n",
      "| epoch  30 |    40/  100 batches | lr 1.13 | ms/batch 24.46 | loss  1.72 | ppl     5.59\n",
      "| epoch  30 |    50/  100 batches | lr 1.13 | ms/batch 24.41 | loss  1.72 | ppl     5.59\n",
      "| epoch  30 |    60/  100 batches | lr 1.13 | ms/batch 25.46 | loss  1.74 | ppl     5.68\n",
      "| epoch  30 |    70/  100 batches | lr 1.13 | ms/batch 25.76 | loss  1.64 | ppl     5.17\n",
      "| epoch  30 |    80/  100 batches | lr 1.13 | ms/batch 25.65 | loss  1.67 | ppl     5.32\n",
      "| epoch  30 |    90/  100 batches | lr 1.13 | ms/batch 24.60 | loss  1.74 | ppl     5.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  4.53s | valid loss  1.61 | valid ppl     5.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |    10/  100 batches | lr 1.07 | ms/batch 27.17 | loss  1.84 | ppl     6.30\n",
      "| epoch  31 |    20/  100 batches | lr 1.07 | ms/batch 24.42 | loss  1.66 | ppl     5.27\n",
      "| epoch  31 |    30/  100 batches | lr 1.07 | ms/batch 24.49 | loss  1.79 | ppl     5.97\n",
      "| epoch  31 |    40/  100 batches | lr 1.07 | ms/batch 24.46 | loss  1.68 | ppl     5.35\n",
      "| epoch  31 |    50/  100 batches | lr 1.07 | ms/batch 24.40 | loss  1.70 | ppl     5.46\n",
      "| epoch  31 |    60/  100 batches | lr 1.07 | ms/batch 24.54 | loss  1.72 | ppl     5.56\n",
      "| epoch  31 |    70/  100 batches | lr 1.07 | ms/batch 24.75 | loss  1.64 | ppl     5.17\n",
      "| epoch  31 |    80/  100 batches | lr 1.07 | ms/batch 25.50 | loss  1.64 | ppl     5.16\n",
      "| epoch  31 |    90/  100 batches | lr 1.07 | ms/batch 25.73 | loss  1.70 | ppl     5.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  4.53s | valid loss  1.56 | valid ppl     4.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |    10/  100 batches | lr 1.02 | ms/batch 28.44 | loss  1.81 | ppl     6.10\n",
      "| epoch  32 |    20/  100 batches | lr 1.02 | ms/batch 25.13 | loss  1.66 | ppl     5.25\n",
      "| epoch  32 |    30/  100 batches | lr 1.02 | ms/batch 24.35 | loss  1.74 | ppl     5.68\n",
      "| epoch  32 |    40/  100 batches | lr 1.02 | ms/batch 24.51 | loss  1.65 | ppl     5.21\n",
      "| epoch  32 |    50/  100 batches | lr 1.02 | ms/batch 24.45 | loss  1.66 | ppl     5.27\n",
      "| epoch  32 |    60/  100 batches | lr 1.02 | ms/batch 24.40 | loss  1.67 | ppl     5.29\n",
      "| epoch  32 |    70/  100 batches | lr 1.02 | ms/batch 24.42 | loss  1.61 | ppl     4.98\n",
      "| epoch  32 |    80/  100 batches | lr 1.02 | ms/batch 24.51 | loss  1.63 | ppl     5.13\n",
      "| epoch  32 |    90/  100 batches | lr 1.02 | ms/batch 25.15 | loss  1.66 | ppl     5.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  4.53s | valid loss  1.58 | valid ppl     4.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |    10/  100 batches | lr 0.97 | ms/batch 28.01 | loss  1.79 | ppl     6.01\n",
      "| epoch  33 |    20/  100 batches | lr 0.97 | ms/batch 25.29 | loss  1.61 | ppl     5.01\n",
      "| epoch  33 |    30/  100 batches | lr 0.97 | ms/batch 25.83 | loss  1.72 | ppl     5.56\n",
      "| epoch  33 |    40/  100 batches | lr 0.97 | ms/batch 24.58 | loss  1.65 | ppl     5.19\n",
      "| epoch  33 |    50/  100 batches | lr 0.97 | ms/batch 24.39 | loss  1.64 | ppl     5.17\n",
      "| epoch  33 |    60/  100 batches | lr 0.97 | ms/batch 24.73 | loss  1.65 | ppl     5.23\n",
      "| epoch  33 |    70/  100 batches | lr 0.97 | ms/batch 24.48 | loss  1.57 | ppl     4.82\n",
      "| epoch  33 |    80/  100 batches | lr 0.97 | ms/batch 24.51 | loss  1.60 | ppl     4.97\n",
      "| epoch  33 |    90/  100 batches | lr 0.97 | ms/batch 24.40 | loss  1.63 | ppl     5.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  4.54s | valid loss  1.59 | valid ppl     4.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |    10/  100 batches | lr 0.92 | ms/batch 26.91 | loss  1.76 | ppl     5.83\n",
      "| epoch  34 |    20/  100 batches | lr 0.92 | ms/batch 24.94 | loss  1.59 | ppl     4.90\n",
      "| epoch  34 |    30/  100 batches | lr 0.92 | ms/batch 58.49 | loss  1.70 | ppl     5.46\n",
      "| epoch  34 |    40/  100 batches | lr 0.92 | ms/batch 32.40 | loss  1.62 | ppl     5.03\n",
      "| epoch  34 |    50/  100 batches | lr 0.92 | ms/batch 24.55 | loss  1.59 | ppl     4.92\n",
      "| epoch  34 |    60/  100 batches | lr 0.92 | ms/batch 24.79 | loss  1.62 | ppl     5.07\n",
      "| epoch  34 |    70/  100 batches | lr 0.92 | ms/batch 24.58 | loss  1.53 | ppl     4.60\n",
      "| epoch  34 |    80/  100 batches | lr 0.92 | ms/batch 24.43 | loss  1.59 | ppl     4.92\n",
      "| epoch  34 |    90/  100 batches | lr 0.92 | ms/batch 24.53 | loss  1.60 | ppl     4.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  4.93s | valid loss  1.56 | valid ppl     4.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |    10/  100 batches | lr 0.87 | ms/batch 28.33 | loss  1.74 | ppl     5.68\n",
      "| epoch  35 |    20/  100 batches | lr 0.87 | ms/batch 25.09 | loss  1.58 | ppl     4.83\n",
      "| epoch  35 |    30/  100 batches | lr 0.87 | ms/batch 25.40 | loss  1.65 | ppl     5.20\n",
      "| epoch  35 |    40/  100 batches | lr 0.87 | ms/batch 25.76 | loss  1.60 | ppl     4.94\n",
      "| epoch  35 |    50/  100 batches | lr 0.87 | ms/batch 25.51 | loss  1.57 | ppl     4.81\n",
      "| epoch  35 |    60/  100 batches | lr 0.87 | ms/batch 24.37 | loss  1.59 | ppl     4.91\n",
      "| epoch  35 |    70/  100 batches | lr 0.87 | ms/batch 24.55 | loss  1.52 | ppl     4.59\n",
      "| epoch  35 |    80/  100 batches | lr 0.87 | ms/batch 24.47 | loss  1.55 | ppl     4.72\n",
      "| epoch  35 |    90/  100 batches | lr 0.87 | ms/batch 24.41 | loss  1.59 | ppl     4.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  4.67s | valid loss  1.52 | valid ppl     4.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |    10/  100 batches | lr 0.83 | ms/batch 27.24 | loss  1.71 | ppl     5.50\n",
      "| epoch  36 |    20/  100 batches | lr 0.83 | ms/batch 24.36 | loss  1.54 | ppl     4.69\n",
      "| epoch  36 |    30/  100 batches | lr 0.83 | ms/batch 25.04 | loss  1.65 | ppl     5.18\n",
      "| epoch  36 |    40/  100 batches | lr 0.83 | ms/batch 25.32 | loss  1.55 | ppl     4.71\n",
      "| epoch  36 |    50/  100 batches | lr 0.83 | ms/batch 32.70 | loss  1.55 | ppl     4.72\n",
      "| epoch  36 |    60/  100 batches | lr 0.83 | ms/batch 31.08 | loss  1.58 | ppl     4.86\n",
      "| epoch  36 |    70/  100 batches | lr 0.83 | ms/batch 35.27 | loss  1.51 | ppl     4.53\n",
      "| epoch  36 |    80/  100 batches | lr 0.83 | ms/batch 31.18 | loss  1.52 | ppl     4.57\n",
      "| epoch  36 |    90/  100 batches | lr 0.83 | ms/batch 33.62 | loss  1.58 | ppl     4.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  5.25s | valid loss  1.52 | valid ppl     4.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |    10/  100 batches | lr 0.79 | ms/batch 27.56 | loss  1.68 | ppl     5.38\n",
      "| epoch  37 |    20/  100 batches | lr 0.79 | ms/batch 25.43 | loss  1.54 | ppl     4.65\n",
      "| epoch  37 |    30/  100 batches | lr 0.79 | ms/batch 25.32 | loss  1.63 | ppl     5.11\n",
      "| epoch  37 |    40/  100 batches | lr 0.79 | ms/batch 25.74 | loss  1.54 | ppl     4.67\n",
      "| epoch  37 |    50/  100 batches | lr 0.79 | ms/batch 25.21 | loss  1.53 | ppl     4.63\n",
      "| epoch  37 |    60/  100 batches | lr 0.79 | ms/batch 24.35 | loss  1.55 | ppl     4.70\n",
      "| epoch  37 |    70/  100 batches | lr 0.79 | ms/batch 24.61 | loss  1.50 | ppl     4.46\n",
      "| epoch  37 |    80/  100 batches | lr 0.79 | ms/batch 24.44 | loss  1.49 | ppl     4.46\n",
      "| epoch  37 |    90/  100 batches | lr 0.79 | ms/batch 24.51 | loss  1.56 | ppl     4.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  4.57s | valid loss  1.48 | valid ppl     4.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |    10/  100 batches | lr 0.75 | ms/batch 32.47 | loss  1.67 | ppl     5.29\n",
      "| epoch  38 |    20/  100 batches | lr 0.75 | ms/batch 27.89 | loss  1.51 | ppl     4.51\n",
      "| epoch  38 |    30/  100 batches | lr 0.75 | ms/batch 31.14 | loss  1.60 | ppl     4.96\n",
      "| epoch  38 |    40/  100 batches | lr 0.75 | ms/batch 37.42 | loss  1.52 | ppl     4.59\n",
      "| epoch  38 |    50/  100 batches | lr 0.75 | ms/batch 30.64 | loss  1.52 | ppl     4.56\n",
      "| epoch  38 |    60/  100 batches | lr 0.75 | ms/batch 33.72 | loss  1.52 | ppl     4.58\n",
      "| epoch  38 |    70/  100 batches | lr 0.75 | ms/batch 65.08 | loss  1.48 | ppl     4.39\n",
      "| epoch  38 |    80/  100 batches | lr 0.75 | ms/batch 57.72 | loss  1.49 | ppl     4.43\n",
      "| epoch  38 |    90/  100 batches | lr 0.75 | ms/batch 37.28 | loss  1.53 | ppl     4.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  6.39s | valid loss  1.45 | valid ppl     4.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |    10/  100 batches | lr 0.71 | ms/batch 48.11 | loss  1.63 | ppl     5.12\n",
      "| epoch  39 |    20/  100 batches | lr 0.71 | ms/batch 35.59 | loss  1.48 | ppl     4.40\n",
      "| epoch  39 |    30/  100 batches | lr 0.71 | ms/batch 28.06 | loss  1.58 | ppl     4.84\n",
      "| epoch  39 |    40/  100 batches | lr 0.71 | ms/batch 32.59 | loss  1.50 | ppl     4.48\n",
      "| epoch  39 |    50/  100 batches | lr 0.71 | ms/batch 32.16 | loss  1.50 | ppl     4.46\n",
      "| epoch  39 |    60/  100 batches | lr 0.71 | ms/batch 34.96 | loss  1.52 | ppl     4.59\n",
      "| epoch  39 |    70/  100 batches | lr 0.71 | ms/batch 33.40 | loss  1.46 | ppl     4.30\n",
      "| epoch  39 |    80/  100 batches | lr 0.71 | ms/batch 34.01 | loss  1.48 | ppl     4.37\n",
      "| epoch  39 |    90/  100 batches | lr 0.71 | ms/batch 27.13 | loss  1.51 | ppl     4.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  5.38s | valid loss  1.44 | valid ppl     4.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |    10/  100 batches | lr 0.68 | ms/batch 26.99 | loss  1.62 | ppl     5.05\n",
      "| epoch  40 |    20/  100 batches | lr 0.68 | ms/batch 24.44 | loss  1.47 | ppl     4.35\n",
      "| epoch  40 |    30/  100 batches | lr 0.68 | ms/batch 24.37 | loss  1.56 | ppl     4.75\n",
      "| epoch  40 |    40/  100 batches | lr 0.68 | ms/batch 24.76 | loss  1.50 | ppl     4.47\n",
      "| epoch  40 |    50/  100 batches | lr 0.68 | ms/batch 25.02 | loss  1.48 | ppl     4.38\n",
      "| epoch  40 |    60/  100 batches | lr 0.68 | ms/batch 25.50 | loss  1.51 | ppl     4.51\n",
      "| epoch  40 |    70/  100 batches | lr 0.68 | ms/batch 26.11 | loss  1.43 | ppl     4.16\n",
      "| epoch  40 |    80/  100 batches | lr 0.68 | ms/batch 24.81 | loss  1.45 | ppl     4.26\n",
      "| epoch  40 |    90/  100 batches | lr 0.68 | ms/batch 24.46 | loss  1.49 | ppl     4.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time:  4.53s | valid loss  1.46 | valid ppl     4.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |    10/  100 batches | lr 0.64 | ms/batch 27.15 | loss  1.61 | ppl     5.00\n",
      "| epoch  41 |    20/  100 batches | lr 0.64 | ms/batch 24.50 | loss  1.45 | ppl     4.28\n",
      "| epoch  41 |    30/  100 batches | lr 0.64 | ms/batch 24.39 | loss  1.54 | ppl     4.68\n",
      "| epoch  41 |    40/  100 batches | lr 0.64 | ms/batch 24.56 | loss  1.47 | ppl     4.37\n",
      "| epoch  41 |    50/  100 batches | lr 0.64 | ms/batch 24.45 | loss  1.47 | ppl     4.34\n",
      "| epoch  41 |    60/  100 batches | lr 0.64 | ms/batch 24.61 | loss  1.48 | ppl     4.40\n",
      "| epoch  41 |    70/  100 batches | lr 0.64 | ms/batch 24.82 | loss  1.43 | ppl     4.18\n",
      "| epoch  41 |    80/  100 batches | lr 0.64 | ms/batch 25.34 | loss  1.43 | ppl     4.17\n",
      "| epoch  41 |    90/  100 batches | lr 0.64 | ms/batch 26.15 | loss  1.48 | ppl     4.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time:  4.53s | valid loss  1.44 | valid ppl     4.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |    10/  100 batches | lr 0.61 | ms/batch 29.09 | loss  1.59 | ppl     4.88\n",
      "| epoch  42 |    20/  100 batches | lr 0.61 | ms/batch 24.55 | loss  1.45 | ppl     4.25\n",
      "| epoch  42 |    30/  100 batches | lr 0.61 | ms/batch 24.54 | loss  1.52 | ppl     4.58\n",
      "| epoch  42 |    40/  100 batches | lr 0.61 | ms/batch 24.55 | loss  1.45 | ppl     4.28\n",
      "| epoch  42 |    50/  100 batches | lr 0.61 | ms/batch 24.41 | loss  1.45 | ppl     4.26\n",
      "| epoch  42 |    60/  100 batches | lr 0.61 | ms/batch 24.47 | loss  1.46 | ppl     4.30\n",
      "| epoch  42 |    70/  100 batches | lr 0.61 | ms/batch 24.50 | loss  1.42 | ppl     4.12\n",
      "| epoch  42 |    80/  100 batches | lr 0.61 | ms/batch 24.84 | loss  1.43 | ppl     4.17\n",
      "| epoch  42 |    90/  100 batches | lr 0.61 | ms/batch 25.40 | loss  1.46 | ppl     4.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time:  4.54s | valid loss  1.43 | valid ppl     4.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |    10/  100 batches | lr 0.58 | ms/batch 27.95 | loss  1.58 | ppl     4.84\n",
      "| epoch  43 |    20/  100 batches | lr 0.58 | ms/batch 25.56 | loss  1.44 | ppl     4.21\n",
      "| epoch  43 |    30/  100 batches | lr 0.58 | ms/batch 24.83 | loss  1.50 | ppl     4.50\n",
      "| epoch  43 |    40/  100 batches | lr 0.58 | ms/batch 24.35 | loss  1.44 | ppl     4.22\n",
      "| epoch  43 |    50/  100 batches | lr 0.58 | ms/batch 24.67 | loss  1.43 | ppl     4.20\n",
      "| epoch  43 |    60/  100 batches | lr 0.58 | ms/batch 24.40 | loss  1.45 | ppl     4.28\n",
      "| epoch  43 |    70/  100 batches | lr 0.58 | ms/batch 24.41 | loss  1.40 | ppl     4.05\n",
      "| epoch  43 |    80/  100 batches | lr 0.58 | ms/batch 24.43 | loss  1.41 | ppl     4.10\n",
      "| epoch  43 |    90/  100 batches | lr 0.58 | ms/batch 24.45 | loss  1.45 | ppl     4.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time:  4.53s | valid loss  1.41 | valid ppl     4.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |    10/  100 batches | lr 0.55 | ms/batch 27.09 | loss  1.56 | ppl     4.78\n",
      "| epoch  44 |    20/  100 batches | lr 0.55 | ms/batch 25.41 | loss  1.42 | ppl     4.12\n",
      "| epoch  44 |    30/  100 batches | lr 0.55 | ms/batch 25.43 | loss  1.50 | ppl     4.49\n",
      "| epoch  44 |    40/  100 batches | lr 0.55 | ms/batch 25.74 | loss  1.44 | ppl     4.21\n",
      "| epoch  44 |    50/  100 batches | lr 0.55 | ms/batch 24.45 | loss  1.42 | ppl     4.15\n",
      "| epoch  44 |    60/  100 batches | lr 0.55 | ms/batch 24.61 | loss  1.45 | ppl     4.24\n",
      "| epoch  44 |    70/  100 batches | lr 0.55 | ms/batch 24.71 | loss  1.38 | ppl     3.99\n",
      "| epoch  44 |    80/  100 batches | lr 0.55 | ms/batch 24.53 | loss  1.40 | ppl     4.06\n",
      "| epoch  44 |    90/  100 batches | lr 0.55 | ms/batch 24.43 | loss  1.44 | ppl     4.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time:  4.54s | valid loss  1.41 | valid ppl     4.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |    10/  100 batches | lr 0.52 | ms/batch 26.87 | loss  1.54 | ppl     4.69\n",
      "| epoch  45 |    20/  100 batches | lr 0.52 | ms/batch 24.41 | loss  1.40 | ppl     4.07\n",
      "| epoch  45 |    30/  100 batches | lr 0.52 | ms/batch 25.09 | loss  1.49 | ppl     4.44\n",
      "| epoch  45 |    40/  100 batches | lr 0.52 | ms/batch 25.41 | loss  1.42 | ppl     4.12\n",
      "| epoch  45 |    50/  100 batches | lr 0.52 | ms/batch 25.94 | loss  1.41 | ppl     4.11\n",
      "| epoch  45 |    60/  100 batches | lr 0.52 | ms/batch 25.47 | loss  1.42 | ppl     4.14\n",
      "| epoch  45 |    70/  100 batches | lr 0.52 | ms/batch 24.42 | loss  1.38 | ppl     3.97\n",
      "| epoch  45 |    80/  100 batches | lr 0.52 | ms/batch 24.63 | loss  1.39 | ppl     4.00\n",
      "| epoch  45 |    90/  100 batches | lr 0.52 | ms/batch 24.57 | loss  1.42 | ppl     4.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time:  4.54s | valid loss  1.38 | valid ppl     3.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |    10/  100 batches | lr 0.50 | ms/batch 27.04 | loss  1.53 | ppl     4.61\n",
      "| epoch  46 |    20/  100 batches | lr 0.50 | ms/batch 24.35 | loss  1.40 | ppl     4.04\n",
      "| epoch  46 |    30/  100 batches | lr 0.50 | ms/batch 24.48 | loss  1.47 | ppl     4.34\n",
      "| epoch  46 |    40/  100 batches | lr 0.50 | ms/batch 24.43 | loss  1.41 | ppl     4.09\n",
      "| epoch  46 |    50/  100 batches | lr 0.50 | ms/batch 25.72 | loss  1.40 | ppl     4.07\n",
      "| epoch  46 |    60/  100 batches | lr 0.50 | ms/batch 25.59 | loss  1.42 | ppl     4.16\n",
      "| epoch  46 |    70/  100 batches | lr 0.50 | ms/batch 25.36 | loss  1.36 | ppl     3.89\n",
      "| epoch  46 |    80/  100 batches | lr 0.50 | ms/batch 24.35 | loss  1.38 | ppl     3.97\n",
      "| epoch  46 |    90/  100 batches | lr 0.50 | ms/batch 24.52 | loss  1.41 | ppl     4.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time:  4.54s | valid loss  1.40 | valid ppl     4.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |    10/  100 batches | lr 0.47 | ms/batch 26.99 | loss  1.53 | ppl     4.60\n",
      "| epoch  47 |    20/  100 batches | lr 0.47 | ms/batch 24.60 | loss  1.38 | ppl     3.97\n",
      "| epoch  47 |    30/  100 batches | lr 0.47 | ms/batch 24.37 | loss  1.46 | ppl     4.30\n",
      "| epoch  47 |    40/  100 batches | lr 0.47 | ms/batch 24.42 | loss  1.40 | ppl     4.05\n",
      "| epoch  47 |    50/  100 batches | lr 0.47 | ms/batch 24.50 | loss  1.40 | ppl     4.05\n",
      "| epoch  47 |    60/  100 batches | lr 0.47 | ms/batch 24.74 | loss  1.41 | ppl     4.11\n",
      "| epoch  47 |    70/  100 batches | lr 0.47 | ms/batch 25.45 | loss  1.35 | ppl     3.86\n",
      "| epoch  47 |    80/  100 batches | lr 0.47 | ms/batch 24.97 | loss  1.36 | ppl     3.91\n",
      "| epoch  47 |    90/  100 batches | lr 0.47 | ms/batch 26.06 | loss  1.40 | ppl     4.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time:  4.55s | valid loss  1.39 | valid ppl     4.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |    10/  100 batches | lr 0.45 | ms/batch 27.28 | loss  1.52 | ppl     4.55\n",
      "| epoch  48 |    20/  100 batches | lr 0.45 | ms/batch 24.46 | loss  1.37 | ppl     3.95\n",
      "| epoch  48 |    30/  100 batches | lr 0.45 | ms/batch 24.50 | loss  1.45 | ppl     4.27\n",
      "| epoch  48 |    40/  100 batches | lr 0.45 | ms/batch 24.63 | loss  1.39 | ppl     4.00\n",
      "| epoch  48 |    50/  100 batches | lr 0.45 | ms/batch 24.55 | loss  1.38 | ppl     3.98\n",
      "| epoch  48 |    60/  100 batches | lr 0.45 | ms/batch 24.31 | loss  1.39 | ppl     4.03\n",
      "| epoch  48 |    70/  100 batches | lr 0.45 | ms/batch 24.57 | loss  1.34 | ppl     3.82\n",
      "| epoch  48 |    80/  100 batches | lr 0.45 | ms/batch 25.22 | loss  1.36 | ppl     3.88\n",
      "| epoch  48 |    90/  100 batches | lr 0.45 | ms/batch 25.34 | loss  1.40 | ppl     4.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time:  4.53s | valid loss  1.39 | valid ppl     4.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |    10/  100 batches | lr 0.43 | ms/batch 28.17 | loss  1.51 | ppl     4.53\n",
      "| epoch  49 |    20/  100 batches | lr 0.43 | ms/batch 25.97 | loss  1.37 | ppl     3.92\n",
      "| epoch  49 |    30/  100 batches | lr 0.43 | ms/batch 24.48 | loss  1.45 | ppl     4.25\n",
      "| epoch  49 |    40/  100 batches | lr 0.43 | ms/batch 24.42 | loss  1.38 | ppl     3.97\n",
      "| epoch  49 |    50/  100 batches | lr 0.43 | ms/batch 24.49 | loss  1.37 | ppl     3.95\n",
      "| epoch  49 |    60/  100 batches | lr 0.43 | ms/batch 24.34 | loss  1.39 | ppl     4.02\n",
      "| epoch  49 |    70/  100 batches | lr 0.43 | ms/batch 24.48 | loss  1.34 | ppl     3.81\n",
      "| epoch  49 |    80/  100 batches | lr 0.43 | ms/batch 24.41 | loss  1.35 | ppl     3.85\n",
      "| epoch  49 |    90/  100 batches | lr 0.43 | ms/batch 24.37 | loss  1.38 | ppl     3.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time:  4.53s | valid loss  1.38 | valid ppl     3.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |    10/  100 batches | lr 0.40 | ms/batch 27.46 | loss  1.49 | ppl     4.46\n",
      "| epoch  50 |    20/  100 batches | lr 0.40 | ms/batch 25.44 | loss  1.36 | ppl     3.89\n",
      "| epoch  50 |    30/  100 batches | lr 0.40 | ms/batch 25.78 | loss  1.44 | ppl     4.21\n",
      "| epoch  50 |    40/  100 batches | lr 0.40 | ms/batch 25.66 | loss  1.38 | ppl     3.96\n",
      "| epoch  50 |    50/  100 batches | lr 0.40 | ms/batch 24.34 | loss  1.36 | ppl     3.88\n",
      "| epoch  50 |    60/  100 batches | lr 0.40 | ms/batch 24.45 | loss  1.38 | ppl     3.99\n",
      "| epoch  50 |    70/  100 batches | lr 0.40 | ms/batch 24.69 | loss  1.33 | ppl     3.78\n",
      "| epoch  50 |    80/  100 batches | lr 0.40 | ms/batch 24.48 | loss  1.34 | ppl     3.83\n",
      "| epoch  50 |    90/  100 batches | lr 0.40 | ms/batch 24.46 | loss  1.38 | ppl     3.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time:  4.54s | valid loss  1.37 | valid ppl     3.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "Loading and saving the new best model...\n"
     ]
    }
   ],
   "source": [
    "# @markdown Loop over epochs. Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch.\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 20\n",
    "\n",
    "final_model_params_path = os.path.join(\"../models/\", \"neural_transformer_model.pt\")\n",
    "if os.path.exists(final_model_params_path):\n",
    "    print(\"Loading a previously saved model checkpoint...\")\n",
    "    model.load_state_dict(torch.load(final_model_params_path))\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print(\"-\" * 89)\n",
    "        print(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}\"\n",
    "        )\n",
    "        print(\"-\" * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Loading and saving the new best model...\")\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        os.path.join(\"../models/\", \"neural_transformer_model.pt\"),\n",
    "    )  # save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 601\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 601\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 779\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 529\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 529\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 956\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 587\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 587\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 587\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 587\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 587\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 587\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 587\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 587\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 270\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 270\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 270\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 270\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 270\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 710\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 710\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 710\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 710\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 218\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 529\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 165\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 165\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 529\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 529\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 529\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 529\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 529\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 529\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 529\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 142\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 270\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 270\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "logits: torch.Size([1, 1024])\n",
      "token_next: 6\n",
      "torch.Size([1, 2999, 302]) torch.Size([1, 3099, 302])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Generate new data using test input\n",
    "\n",
    "max_new_tokens = 100\n",
    "input_gen = model.transformer_generate(input, mask, max_new_tokens)\n",
    "\n",
    "print(input.shape, input_gen.shape, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x14705e5ea2c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(input[0, -max_new_tokens:, mask.squeeze()][:, 0].cpu().numpy(), label=\"input\")\n",
    "plt.plot(\n",
    "    input_gen[0, -max_new_tokens:, mask.squeeze()][:, 0].cpu().numpy(),\n",
    "    label=\"generated\",\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
