{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import nbimporter\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from models._utils import load_model_from_checkpoint\n",
    "from utils import NEURONS_302, DEVICE, MAX_TOKEN_LEN\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from preprocess._utils import smooth_data_preprocess, reshape_calcium_data\n",
    "from CreateSyntheticDataset import (\n",
    "    save_synthetic_dataset,\n",
    "    plot_neural_signals,\n",
    "    plot_3d_trajectory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a synthetic dataset where the neural activity is the embeddings of tokens from the tiny Shakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_tokenize_and_chunk(\n",
    "    text, max_length=510\n",
    "):  # slightly less than 512 to account for special tokens\n",
    "    pre_tokenizer = WhitespaceSplit()\n",
    "    pre_tokens = pre_tokenizer.pre_tokenize_str(text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for token, (start, end) in pre_tokens:\n",
    "        if len(current_chunk) + len(token) + 1 > max_length:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = token\n",
    "        else:\n",
    "            current_chunk += \" \" + token if current_chunk else token\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def tokenize_and_chunk(examples, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenized_batches = {\"text\": [], \"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        chunks = pre_tokenize_and_chunk(text)\n",
    "        for chunk in chunks:\n",
    "            tokenized_output = tokenizer(\n",
    "                chunk,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            tokenized_batches[\"text\"].append(chunk)\n",
    "            tokenized_batches[\"input_ids\"].append(\n",
    "                tokenized_output[\"input_ids\"][0].tolist()\n",
    "            )\n",
    "            tokenized_batches[\"attention_mask\"].append(\n",
    "                tokenized_output[\"attention_mask\"][0].tolist()\n",
    "            )\n",
    "\n",
    "    return tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_shakespeare(\n",
    "    max_timesteps: int = 1000,\n",
    "    num_signals: int = 302,\n",
    "    num_named_neurons: int = 1,\n",
    "    dataset_name: str = \"Shakespeare0000\",\n",
    "):\n",
    "    # Want to access the tokenizer and the embedding table outside this function\n",
    "    global tokenizer, embedding  # DEBUG\n",
    "    # Load the Shakespeare dataset\n",
    "    text_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "    # Create a tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    # Apply the tokenization and chunking to each split\n",
    "    text_dataset = text_dataset.map(\n",
    "        tokenize_and_chunk, batched=True, fn_kwargs=dict(tokenizer=tokenizer)\n",
    "    )\n",
    "    # Create an embedding table\n",
    "    embedding_dim = num_signals\n",
    "    embedding = torch.nn.Embedding(\n",
    "        num_embeddings=tokenizer.vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        dtype=torch.half,\n",
    "    )\n",
    "    print(embedding.weight[0, :10].detach().cpu().numpy(), end=\"\\n\\n\")  # DEBUG\n",
    "    # Extract all splots of the text dataset\n",
    "    train_tokens = text_dataset[\"train\"][\"input_ids\"]\n",
    "    validation_tokens = text_dataset[\"validation\"][\"input_ids\"]\n",
    "    test_tokens = text_dataset[\"test\"][\"input_ids\"]\n",
    "    all_tokens = train_tokens + validation_tokens + test_tokens\n",
    "    # Set up for creating the synthetic dataset\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "    smooth_method = None\n",
    "    dataset = dict()\n",
    "    # Create data for as many worms as possible\n",
    "    worm_idx = 0\n",
    "    calcium_data = []\n",
    "    total_time = 0\n",
    "    for chunk in all_tokens:\n",
    "        if worm_idx > 200:\n",
    "            break\n",
    "        embd_data = embedding(torch.LongTensor(chunk)).detach().numpy()\n",
    "        calcium_data.append(embd_data)\n",
    "        total_time += embd_data.shape[0]\n",
    "        if total_time >= max_timesteps:\n",
    "            calcium_data = np.vstack(calcium_data)\n",
    "            time_in_seconds = np.arange(total_time).reshape(-1, 1)\n",
    "            dt = np.gradient(time_in_seconds, axis=0)\n",
    "            dt[dt == 0] = np.finfo(float).eps\n",
    "            resample_dt = np.round(np.median(dt).item(), 2)\n",
    "            residual_calcium = np.gradient(calcium_data, axis=0) / dt\n",
    "            smooth_calcium_data = smooth_data_preprocess(\n",
    "                calcium_data, time_in_seconds, smooth_method\n",
    "            )\n",
    "            smooth_residual_calcium = smooth_data_preprocess(\n",
    "                residual_calcium, time_in_seconds, smooth_method\n",
    "            )\n",
    "            named_neuron_indices = random.sample(range(num_signals), num_named_neurons)\n",
    "            named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "            neuron_to_idx = {\n",
    "                (neuron) if neuron in named_neurons else str(idx): idx\n",
    "                for idx, neuron in enumerate(NEURONS_302)\n",
    "            }\n",
    "            idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "\n",
    "            worm_data = dict()\n",
    "\n",
    "            worm_data[\"worm\"] = f\"worm{worm_idx}\"\n",
    "            worm_data[\"dataset\"] = dataset_name\n",
    "            worm_data[\"smooth_method\"] = smooth_method\n",
    "            worm_data[\"calcium_data\"] = calcium_data\n",
    "            worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "            worm_data[\"residual_calcium\"] = residual_calcium\n",
    "            worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "            worm_data[\"max_timesteps\"] = total_time\n",
    "            worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "            worm_data[\"dt\"] = dt\n",
    "            worm_data[\"resample_median_dt\"] = resample_dt\n",
    "            worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "            worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "            worm_data[\"num_neurons\"] = num_signals\n",
    "            worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "            worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "\n",
    "            worm_data = reshape_calcium_data(worm_data)\n",
    "            dataset[f\"worm{worm_idx}\"] = worm_data\n",
    "\n",
    "            worm_idx += 1\n",
    "            calcium_data = []\n",
    "            total_time = 0\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5547  0.1342 -2.775   0.664   0.1761  0.4463  0.3494  1.41   -0.1451\n",
      " -1.047 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "max_timesteps = 3000\n",
    "num_signals = 302\n",
    "num_named_neurons = num_signals  # 50 #  DEBUG\n",
    "dataset_name = \"Shakespeare0000\"\n",
    "\n",
    "# Creating and saving datasets\n",
    "dataset = create_synthetic_dataset_shakespeare(\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_signals=num_signals,\n",
    "    num_named_neurons=num_named_neurons,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Get the number of worms in the dataset\n",
    "num_worms = len(dataset)\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Selecting a worm and all the neurons to plot\n",
    "# num_worms = len(dataset)\n",
    "# worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "# neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][\n",
    "#     :num_named_neurons\n",
    "# ]\n",
    "\n",
    "# # Plotting dataset\n",
    "# plot_neural_signals(\n",
    "#     data=dataset[worm_idx][\"calcium_data\"],\n",
    "#     time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "#     neuron_idx=neuron_idx,\n",
    "#     yax_limit=False,\n",
    "#     suptitle=f\"{dataset_name} - {worm_idx}\",\n",
    "# )\n",
    "\n",
    "# # Visualize covariance matrix\n",
    "# data = dataset[worm_idx][\"smooth_calcium_data\"]\n",
    "# mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "# neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "# X = data[:, mask].numpy()\n",
    "# n = X.shape[0]\n",
    "# X_bar = X - np.mean(X, axis=0)\n",
    "# cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "# plt.figure()\n",
    "# ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "# ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "# plt.show()\n",
    "\n",
    "# # Plotting 3D trajectory\n",
    "# plot_3d_trajectory(\n",
    "#     X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6160, 2000, 1996, 100, 19204, 17629, 2015, 3075, 1012, 102]\n",
      "\n",
      "[CLS] welcome to the [UNK] tokenizers library. [SEP]\n",
      "\n",
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Tokenizers Demo\n",
    "\n",
    "output = tokenizer.encode(\"Welcome to the ðŸ¤— Tokenizers library.\")\n",
    "encoding = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "print(output, end=\"\\n\\n\")\n",
    "print(tokenizer.decode(output), end=\"\\n\\n\")\n",
    "print(encoding, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "\n",
      "train: <class 'list'> 1 <class 'str'> 1003854\n",
      "\n",
      "validation: <class 'list'> 1 <class 'str'> 55770\n",
      "\n",
      "test: <class 'list'> 1 <class 'str'> 55770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Datasets Demo\n",
    "\n",
    "text_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\n",
    "    \"train:\",\n",
    "    type(text_dataset[\"train\"][\"text\"]),\n",
    "    len(text_dataset[\"train\"][\"text\"]),\n",
    "    type(text_dataset[\"train\"][\"text\"][0]),\n",
    "    len(text_dataset[\"train\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"validation:\",\n",
    "    type(text_dataset[\"validation\"][\"text\"]),\n",
    "    len(text_dataset[\"validation\"][\"text\"]),\n",
    "    type(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    len(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"test:\",\n",
    "    type(text_dataset[\"test\"][\"text\"]),\n",
    "    len(text_dataset[\"test\"][\"text\"]),\n",
    "    type(text_dataset[\"test\"][\"text\"][0]),\n",
    "    len(text_dataset[\"test\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1963\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "})\n",
      "\n",
      "First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? All: Resolved. resolved. First Citizen: First, you know Caius Marcius is chief enemy to the people. All: We know't, we know't. First Citizen: Let us kill him, and we'll have corn at our own price. Is't a verdict? All: No more talking on't; let it be done: away, away! Second Citizen: One word, good citizens. First Citizen: We are accounted poor citizens, the\n",
      "\n",
      "[CLS] first citizen : before we proceed any further, hear me speak. all : speak, speak. first citizen : you are all resolved rather to die than to famish? all : resolved. resolved. first citizen : first, you know caius marcius is chief enemy to the people. all : we know't, we know't. first citizen : let us kill him, and we'll have corn at our own price. is't a verdict? all : no more talking on't ; let it be done : away, away! second citizen : one word, good citizens. first citizen : we are accounted poor citizens, the [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Tokenization and Chunking Demo\n",
    "# @markdown Apply the tokenization and chunking to each split.\n",
    "\n",
    "text_dataset = text_dataset.map(\n",
    "    tokenize_and_chunk, batched=True, fn_kwargs=dict(tokenizer=tokenizer)\n",
    ")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(text_dataset[\"train\"][\"text\"][0], end=\"\\n\\n\")\n",
    "print(tokenizer.decode(text_dataset[\"train\"][\"input_ids\"][0]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Initialize test input\n",
    "\n",
    "chunk = text_dataset[\"train\"][\"input_ids\"][-1]\n",
    "print(tokenizer.decode(chunk), end=\"\\n\\n\")\n",
    "embedding = embedding.to(DEVICE)\n",
    "print(embedding.weight[0, :10].detach().cpu().numpy(), end=\"\\n\\n\")  # DEBUG\n",
    "idx = torch.LongTensor(chunk).unsqueeze(0).to(DEVICE)\n",
    "input = embedding(idx)\n",
    "mask = torch.ones(input.shape[-1], dtype=torch.bool).unsqueeze(0).to(DEVICE)\n",
    "print(idx.shape, input.shape, mask.shape, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title TODO: Train a Transformer model on the Dataset\n",
    "\n",
    "\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ntoken: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        d_hid: int,\n",
    "        nlayers: int,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = torch.nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, d_hid, dropout\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = torch.nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = torch.nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = torch.nn.Transformer.generate_square_subsequent_mask(\n",
    "                len(src)\n",
    "            ).to(DEVICE)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        idx: torch.LongTensor,\n",
    "        max_new_tokens: int,\n",
    "        mask: torch.Tensor,\n",
    "        embedding: torch.nn.Module,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Special generate method for the Transformer model.\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Since we trained the model to directly predict the next token we take the index as the argmin\n",
    "        over the distance between the output and the embedding table.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        # Place embedding table on the same device as idx\n",
    "        embedding = embedding.to(idx.device)\n",
    "\n",
    "        # Loop through time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= MAX_TOKEN_LEN else idx[:, -MAX_TOKEN_LEN:]\n",
    "            # get appropriate input for the model based on idx\n",
    "            input = embedding(idx_cond)  # shape (batch_size, seq_len, neurons)\n",
    "            mask = mask.to(input.device)\n",
    "            # forward the model to get the output\n",
    "            outputs = self(input, mask)\n",
    "            # we only care about the last time step\n",
    "            outputs = outputs[:, -1, :]\n",
    "            # print(\n",
    "            #     f\"outputs.shape: {outputs.shape}\\nembedding.weight: {embedding.weight.shape}\"\n",
    "            # ) # DEBUG\n",
    "            # Compute the Euclidean distances between output and embedding table\n",
    "            distances = torch.norm(outputs - embedding.weight, dim=1)\n",
    "            # print(f\"distances: {distances.shape}\")  # DEBUG\n",
    "            # Find the index of the minimizer\n",
    "            idx_next = torch.argmin(distances).view(1, 1)\n",
    "            # print(f\"idx: {idx.shape}\\nidx_next: {idx_next.shape}\") # DEBUG\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load a trained model\n",
    "\n",
    "checkpoint_path = \"/om2/user/qsimeon/worm-graph/logs/hydra/2023_12_25_16_27_08/exp0/train/checkpoints/model_best.pt\"\n",
    "model = load_model_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] and for your love to her lead apes in hell. talk not to me : i will go sit and weep till i can find occasion of revenge. baptista : was ever gentleman thus grieved as i? but who comes here [SEP]\n",
      "\n",
      "[CLS] prompt : prompt pigeons, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt, prompt,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Generate new tokens\n",
    "max_new_tokens = 100\n",
    "idx_gen = model.transformer_generate(idx, max_new_tokens, mask, embedding)\n",
    "\n",
    "print(tokenizer.decode(idx.tolist()[0]), end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx_gen.tolist()[0][-max_new_tokens:]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
