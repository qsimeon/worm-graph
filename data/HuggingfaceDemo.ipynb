{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default']\n",
      "\n",
      "['train', 'validation', 'test']\n",
      "\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1\n",
      "})\n",
      "\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1\n",
      "})\n",
      "\n",
      "40,000 lines of Shakespeare from a variety of Shakespeare's plays. Featured in Andrej Karpathy's blog post 'The Unreasonable Effectiveness of Recurrent Neural Networks': http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n",
      "\n",
      "To use for e.g. character modelling:\n",
      "\n",
      "```\n",
      "d = datasets.load_dataset(name='tiny_shakespeare')['train']\n",
      "d = d.map(lambda x: datasets.Value('strings').unicode_split(x['text'], 'UTF-8'))\n",
      "# train split includes vocabulary for other splits\n",
      "vocabulary = sorted(set(next(iter(d)).numpy()))\n",
      "d = d.map(lambda x: {'cur_char': x[:-1], 'next_char': x[1:]})\n",
      "d = d.unbatch()\n",
      "seq_len = 100\n",
      "batch_size = 2\n",
      "d = d.batch(seq_len)\n",
      "d = d.batch(batch_size)\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset_builder\n",
    "from datasets import get_dataset_split_names\n",
    "from datasets import get_dataset_config_names\n",
    "\n",
    "print(get_dataset_config_names(\"tiny_shakespeare\"), end=\"\\n\\n\")\n",
    "print(get_dataset_split_names(\"tiny_shakespeare\"), end=\"\\n\\n\")\n",
    "print(load_dataset(\"tiny_shakespeare\", split=\"train\"), end=\"\\n\\n\")\n",
    "print(load_dataset(\"tiny_shakespeare\", \"default\", split=\"train\"), end=\"\\n\\n\")\n",
    "print(load_dataset_builder(\"tiny_shakespeare\").info.description, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6160, 2000, 1996, 100, 19204, 17629, 2015, 3075, 1012, 102]\n",
      "\n",
      "[CLS] welcome to the [UNK] tokenizers library. [SEP]\n",
      "\n",
      "{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title HuggingFace Tokenizers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "output = tokenizer.encode(\"Welcome to the ðŸ¤— Tokenizers library.\")\n",
    "encoding = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "print(output, end=\"\\n\\n\")\n",
    "print(tokenizer.decode(output), end=\"\\n\\n\")\n",
    "print(encoding, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_tokenize_and_chunk(\n",
    "    text, max_length=510\n",
    "):  # slightly less than 512 to account for special tokens\n",
    "    pre_tokenizer = WhitespaceSplit()\n",
    "    pre_tokens = pre_tokenizer.pre_tokenize_str(text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for token, (start, end) in pre_tokens:\n",
    "        if len(current_chunk) + len(token) + 1 > max_length:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = token\n",
    "        else:\n",
    "            current_chunk += \" \" + token if current_chunk else token\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def tokenize_and_chunk(examples):\n",
    "    tokenized_batches = {\"text\": [], \"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        chunks = pre_tokenize_and_chunk(text)\n",
    "        for chunk in chunks:\n",
    "            tokenized_output = tokenizer(\n",
    "                chunk,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            tokenized_batches[\"text\"].append(chunk)\n",
    "            tokenized_batches[\"input_ids\"].append(\n",
    "                tokenized_output[\"input_ids\"][0].tolist()\n",
    "            )\n",
    "            tokenized_batches[\"attention_mask\"].append(\n",
    "                tokenized_output[\"attention_mask\"][0].tolist()\n",
    "            )\n",
    "\n",
    "    return tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "\n",
      "train: <class 'list'> 1 <class 'str'> 1003854\n",
      "\n",
      "validation: <class 'list'> 1 <class 'str'> 55770\n",
      "\n",
      "test: <class 'list'> 1 <class 'str'> 55770\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591f2589d09b42a3adc3be3b6e34bbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8296e67eb9b47a28678cdf467d8e0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1410e2765f4de4af90eacb9d334637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1963\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 109\n",
      "    })\n",
      "})\n",
      "\n",
      "First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: You are all resolved rather to die than to famish? All: Resolved. resolved. First Citizen: First, you know Caius Marcius is chief enemy to the people. All: We know't, we know't. First Citizen: Let us kill him, and we'll have corn at our own price. Is't a verdict? All: No more talking on't; let it be done: away, away! Second Citizen: One word, good citizens. First Citizen: We are accounted poor citizens, the\n",
      "\n",
      "[CLS] first citizen : before we proceed any further, hear me speak. all : speak, speak. first citizen : you are all resolved rather to die than to famish? all : resolved. resolved. first citizen : first, you know caius marcius is chief enemy to the people. all : we know't, we know't. first citizen : let us kill him, and we'll have corn at our own price. is't a verdict? all : no more talking on't ; let it be done : away, away! second citizen : one word, good citizens. first citizen : we are accounted poor citizens, the [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "text_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\n",
    "    \"train:\",\n",
    "    type(text_dataset[\"train\"][\"text\"]),\n",
    "    len(text_dataset[\"train\"][\"text\"]),\n",
    "    type(text_dataset[\"train\"][\"text\"][0]),\n",
    "    len(text_dataset[\"train\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"validation:\",\n",
    "    type(text_dataset[\"validation\"][\"text\"]),\n",
    "    len(text_dataset[\"validation\"][\"text\"]),\n",
    "    type(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    len(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"test:\",\n",
    "    type(text_dataset[\"test\"][\"text\"]),\n",
    "    len(text_dataset[\"test\"][\"text\"]),\n",
    "    type(text_dataset[\"test\"][\"text\"][0]),\n",
    "    len(text_dataset[\"test\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# Apply the tokenization and chunking to each split\n",
    "text_dataset = text_dataset.map(tokenize_and_chunk, batched=True)\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(text_dataset[\"train\"][\"text\"][0], end=\"\\n\\n\")\n",
    "print(tokenizer.decode(text_dataset[\"train\"][\"input_ids\"][0]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title PyTorch nn.Embedding\n",
    "import torch\n",
    "\n",
    "#  Create an embedding layer\n",
    "embedding_dim = 302\n",
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings=tokenizer.vocab_size, embedding_dim=embedding_dim, dtype=torch.half\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      " dict_keys(['worm0', 'worm1', 'worm2', 'worm3', 'worm4', 'worm5', 'worm6', 'worm7', 'worm8', 'worm9', 'worm10', 'worm11', 'worm12', 'worm13', 'worm14', 'worm15', 'worm16', 'worm17', 'worm18', 'worm19', 'worm20', 'worm21', 'worm22', 'worm23', 'worm24', 'worm25', 'worm26', 'worm27', 'worm28', 'worm29', 'worm30', 'worm31', 'worm32', 'worm33', 'worm34', 'worm35', 'worm36', 'worm37', 'worm38', 'worm39', 'worm40', 'worm41', 'worm42', 'worm43', 'worm44', 'worm45', 'worm46', 'worm47', 'worm48', 'worm49', 'worm50', 'worm51', 'worm52', 'worm53', 'worm54', 'worm55', 'worm56', 'worm57', 'worm58', 'worm59', 'worm60', 'worm61', 'worm62', 'worm63', 'worm64', 'worm65', 'worm66', 'worm67', 'worm68', 'worm69', 'worm70', 'worm71', 'worm72', 'worm73', 'worm74', 'worm75', 'worm76', 'worm77', 'worm78', 'worm79', 'worm80', 'worm81', 'worm82', 'worm83', 'worm84', 'worm85', 'worm86', 'worm87', 'worm88', 'worm89', 'worm90', 'worm91', 'worm92', 'worm93', 'worm94'])\n",
      "\n",
      "torch.Size([3036, 302])\n",
      "torch.Size([3033, 302])\n",
      "torch.Size([3013, 302])\n",
      "torch.Size([3043, 302])\n",
      "torch.Size([3024, 302])\n",
      "torch.Size([3111, 302])\n",
      "torch.Size([3089, 302])\n",
      "torch.Size([3046, 302])\n",
      "torch.Size([3084, 302])\n",
      "torch.Size([3009, 302])\n",
      "torch.Size([3124, 302])\n",
      "torch.Size([3130, 302])\n",
      "torch.Size([3125, 302])\n",
      "torch.Size([3058, 302])\n",
      "torch.Size([3107, 302])\n",
      "torch.Size([3077, 302])\n",
      "torch.Size([3003, 302])\n",
      "torch.Size([3132, 302])\n",
      "torch.Size([3051, 302])\n",
      "torch.Size([3065, 302])\n",
      "torch.Size([3047, 302])\n",
      "torch.Size([3037, 302])\n",
      "torch.Size([3037, 302])\n",
      "torch.Size([3052, 302])\n",
      "torch.Size([3023, 302])\n",
      "torch.Size([3069, 302])\n",
      "torch.Size([3080, 302])\n",
      "torch.Size([3025, 302])\n",
      "torch.Size([3112, 302])\n",
      "torch.Size([3065, 302])\n",
      "torch.Size([3134, 302])\n",
      "torch.Size([3066, 302])\n",
      "torch.Size([3052, 302])\n",
      "torch.Size([3129, 302])\n",
      "torch.Size([3131, 302])\n",
      "torch.Size([3091, 302])\n",
      "torch.Size([3061, 302])\n",
      "torch.Size([3082, 302])\n",
      "torch.Size([3054, 302])\n",
      "torch.Size([3104, 302])\n",
      "torch.Size([3104, 302])\n",
      "torch.Size([3099, 302])\n",
      "torch.Size([3051, 302])\n",
      "torch.Size([3121, 302])\n",
      "torch.Size([3063, 302])\n",
      "torch.Size([3110, 302])\n",
      "torch.Size([3102, 302])\n",
      "torch.Size([3115, 302])\n",
      "torch.Size([3009, 302])\n",
      "torch.Size([3071, 302])\n",
      "torch.Size([3060, 302])\n",
      "torch.Size([3120, 302])\n",
      "torch.Size([3116, 302])\n",
      "torch.Size([3012, 302])\n",
      "torch.Size([3077, 302])\n",
      "torch.Size([3049, 302])\n",
      "torch.Size([3005, 302])\n",
      "torch.Size([3042, 302])\n",
      "torch.Size([3106, 302])\n",
      "torch.Size([3085, 302])\n",
      "torch.Size([3022, 302])\n",
      "torch.Size([3044, 302])\n",
      "torch.Size([3008, 302])\n",
      "torch.Size([3043, 302])\n",
      "torch.Size([3032, 302])\n",
      "torch.Size([3088, 302])\n",
      "torch.Size([3072, 302])\n",
      "torch.Size([3096, 302])\n",
      "torch.Size([3103, 302])\n",
      "torch.Size([3108, 302])\n",
      "torch.Size([3054, 302])\n",
      "torch.Size([3062, 302])\n",
      "torch.Size([3128, 302])\n",
      "torch.Size([3059, 302])\n",
      "torch.Size([3080, 302])\n",
      "torch.Size([3043, 302])\n",
      "torch.Size([3076, 302])\n",
      "torch.Size([3016, 302])\n",
      "torch.Size([3114, 302])\n",
      "torch.Size([3014, 302])\n",
      "torch.Size([3051, 302])\n",
      "torch.Size([3090, 302])\n",
      "torch.Size([3064, 302])\n",
      "torch.Size([3021, 302])\n",
      "torch.Size([3059, 302])\n",
      "torch.Size([3014, 302])\n",
      "torch.Size([3053, 302])\n",
      "torch.Size([3073, 302])\n",
      "torch.Size([3021, 302])\n",
      "torch.Size([3047, 302])\n",
      "torch.Size([3017, 302])\n",
      "torch.Size([3018, 302])\n",
      "torch.Size([3096, 302])\n",
      "torch.Size([3117, 302])\n",
      "torch.Size([3043, 302])\n"
     ]
    }
   ],
   "source": [
    "# @title Synthetic dataset where neural activity is sequence of token embeddings\n",
    "import random\n",
    "import numpy as np\n",
    "from utils import NEURONS_302\n",
    "from preprocess._utils import smooth_data_preprocess, reshape_calcium_data\n",
    "\n",
    "train_tokens = text_dataset[\"train\"][\"input_ids\"]\n",
    "validation_tokens = text_dataset[\"validation\"][\"input_ids\"]\n",
    "test_tokens = text_dataset[\"test\"][\"input_ids\"]\n",
    "all_tokens = train_tokens + validation_tokens + test_tokens\n",
    "worm_dataset = dict()\n",
    "max_timesteps = 3000\n",
    "num_signals = embedding_dim\n",
    "num_named_neurons = num_signals  # 50\n",
    "num_unknown_neurons = num_signals - num_named_neurons\n",
    "smooth_method = None\n",
    "dataset_name = \"Shakespeare0000\"\n",
    "\n",
    "worm_idx = 0\n",
    "calcium_data = []\n",
    "named_neuron_indices = random.sample(range(num_signals), num_named_neurons)\n",
    "named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "neuron_to_idx = {\n",
    "    (neuron) if neuron in named_neurons else str(idx): idx\n",
    "    for idx, neuron in enumerate(NEURONS_302)\n",
    "}\n",
    "idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "total_time = 0\n",
    "for chunk in all_tokens:\n",
    "    if worm_idx > 200:\n",
    "        break\n",
    "    embd_data = embedding(torch.LongTensor(chunk)).detach().numpy()\n",
    "    calcium_data.append(embd_data)\n",
    "    total_time += embd_data.shape[0]\n",
    "    if total_time >= max_timesteps:\n",
    "        calcium_data = np.vstack(calcium_data)\n",
    "        time_in_seconds = np.arange(total_time).reshape(-1, 1)\n",
    "        dt = np.gradient(time_in_seconds, axis=0)\n",
    "        dt[dt == 0] = np.finfo(float).eps\n",
    "        resample_dt = np.round(np.median(dt).item(), 2)\n",
    "        residual_calcium = np.gradient(calcium_data, axis=0) / dt\n",
    "        smooth_calcium_data = smooth_data_preprocess(\n",
    "            calcium_data, time_in_seconds, smooth_method\n",
    "        )\n",
    "        smooth_residual_calcium = smooth_data_preprocess(\n",
    "            residual_calcium, time_in_seconds, smooth_method\n",
    "        )\n",
    "\n",
    "        worm_data = dict()\n",
    "\n",
    "        worm_data[\"worm\"] = f\"worm{worm_idx}\"\n",
    "        worm_data[\"dataset\"] = dataset_name\n",
    "        worm_data[\"smooth_method\"] = smooth_method\n",
    "        worm_data[\"calcium_data\"] = calcium_data\n",
    "        worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "        worm_data[\"residual_calcium\"] = residual_calcium\n",
    "        worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "        worm_data[\"max_timesteps\"] = total_time\n",
    "        worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "        worm_data[\"dt\"] = dt\n",
    "        worm_data[\"resample_median_dt\"] = resample_dt\n",
    "        worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "        worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "        worm_data[\"num_neurons\"] = num_signals\n",
    "        worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "        worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "\n",
    "        worm_data = reshape_calcium_data(worm_data)\n",
    "        worm_dataset[f\"worm{worm_idx}\"] = worm_data\n",
    "\n",
    "        worm_idx += 1\n",
    "        calcium_data = []\n",
    "        total_time = 0\n",
    "\n",
    "############\n",
    "\n",
    "print(f\"{worm_idx}\\n {worm_dataset.keys()}\", end=\"\\n\\n\")\n",
    "for worm in worm_dataset:\n",
    "    print(worm_dataset[worm][\"calcium_data\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_synthetic_dataset_shakespeare(\n",
    "#     max_timesteps: int = 1000,\n",
    "#     num_worms: int = 1,\n",
    "#     num_signals: int = 302,\n",
    "#     num_named_neurons: int = 1,\n",
    "#     add_noise: bool = False,\n",
    "#     noise_std: float = 0.01,\n",
    "#     random_walk: bool = False,\n",
    "#     random_freqs: bool = False,\n",
    "#     delta_seconds: float = 0.5,\n",
    "#     dataset_name: str = \"Shakespeare0000\",\n",
    "# ):\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
