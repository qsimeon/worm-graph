{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import nbimporter\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import NEURONS_302, DEVICE\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from models._utils import load_model_from_checkpoint\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "from preprocess._utils import smooth_data_preprocess, reshape_calcium_data\n",
    "from CreateSyntheticDataset import (\n",
    "    save_synthetic_dataset,\n",
    "    plot_neural_signals,\n",
    "    plot_3d_trajectory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title HuggingFace Tokenizers\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "output = tokenizer.encode(\"Welcome to the ðŸ¤— Tokenizers library.\")\n",
    "encoding = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "print(output, end=\"\\n\\n\")\n",
    "print(tokenizer.decode(output), end=\"\\n\\n\")\n",
    "print(encoding, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "\n",
    "def pre_tokenize_and_chunk(\n",
    "    text, max_length=510\n",
    "):  # slightly less than 512 to account for special tokens\n",
    "    pre_tokenizer = WhitespaceSplit()\n",
    "    pre_tokens = pre_tokenizer.pre_tokenize_str(text)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for token, (start, end) in pre_tokens:\n",
    "        if len(current_chunk) + len(token) + 1 > max_length:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = token\n",
    "        else:\n",
    "            current_chunk += \" \" + token if current_chunk else token\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def tokenize_and_chunk(examples, tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenized_batches = {\"text\": [], \"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        chunks = pre_tokenize_and_chunk(text)\n",
    "        for chunk in chunks:\n",
    "            tokenized_output = tokenizer(\n",
    "                chunk,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            tokenized_batches[\"text\"].append(chunk)\n",
    "            tokenized_batches[\"input_ids\"].append(\n",
    "                tokenized_output[\"input_ids\"][0].tolist()\n",
    "            )\n",
    "            tokenized_batches[\"attention_mask\"].append(\n",
    "                tokenized_output[\"attention_mask\"][0].tolist()\n",
    "            )\n",
    "\n",
    "    return tokenized_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load the dataset\n",
    "\n",
    "text_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(\n",
    "    \"train:\",\n",
    "    type(text_dataset[\"train\"][\"text\"]),\n",
    "    len(text_dataset[\"train\"][\"text\"]),\n",
    "    type(text_dataset[\"train\"][\"text\"][0]),\n",
    "    len(text_dataset[\"train\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"validation:\",\n",
    "    type(text_dataset[\"validation\"][\"text\"]),\n",
    "    len(text_dataset[\"validation\"][\"text\"]),\n",
    "    type(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    len(text_dataset[\"validation\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    \"test:\",\n",
    "    type(text_dataset[\"test\"][\"text\"]),\n",
    "    len(text_dataset[\"test\"][\"text\"]),\n",
    "    type(text_dataset[\"test\"][\"text\"][0]),\n",
    "    len(text_dataset[\"test\"][\"text\"][0]),\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Tokenization and Chunking\n",
    "# @markdown Apply the tokenization and chunking to each split.\n",
    "\n",
    "text_dataset = text_dataset.map(tokenize_and_chunk, batched=True)\n",
    "print(text_dataset, end=\"\\n\\n\")\n",
    "print(text_dataset[\"train\"][\"text\"][0], end=\"\\n\\n\")\n",
    "print(tokenizer.decode(text_dataset[\"train\"][\"input_ids\"][0]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Embedding Table\n",
    "\n",
    "#  Create an embedding layer\n",
    "embedding_dim = 302\n",
    "embedding = torch.nn.Embedding(\n",
    "    num_embeddings=tokenizer.vocab_size, embedding_dim=embedding_dim, dtype=torch.half\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Synthetic dataset\n",
    "# @markdown Neural activity is sequence of token embeddings.\n",
    "\n",
    "train_tokens = text_dataset[\"train\"][\"input_ids\"]\n",
    "validation_tokens = text_dataset[\"validation\"][\"input_ids\"]\n",
    "test_tokens = text_dataset[\"test\"][\"input_ids\"]\n",
    "all_tokens = train_tokens + validation_tokens + test_tokens\n",
    "worm_dataset = dict()\n",
    "max_timesteps = 3000\n",
    "num_signals = embedding_dim\n",
    "num_named_neurons = 302\n",
    "num_unknown_neurons = num_signals - num_named_neurons\n",
    "smooth_method = None\n",
    "dataset_name = \"Shakespeare0000\"\n",
    "\n",
    "worm_idx = 0\n",
    "calcium_data = []\n",
    "total_time = 0\n",
    "for chunk in all_tokens:\n",
    "    if worm_idx > 200:\n",
    "        break\n",
    "    embd_data = embedding(torch.LongTensor(chunk)).detach().numpy()\n",
    "    calcium_data.append(embd_data)\n",
    "    total_time += embd_data.shape[0]\n",
    "    if total_time >= max_timesteps:\n",
    "        calcium_data = np.vstack(calcium_data)\n",
    "        time_in_seconds = np.arange(total_time).reshape(-1, 1)\n",
    "        dt = np.gradient(time_in_seconds, axis=0)\n",
    "        dt[dt == 0] = np.finfo(float).eps\n",
    "        resample_dt = np.round(np.median(dt).item(), 2)\n",
    "        residual_calcium = np.gradient(calcium_data, axis=0) / dt\n",
    "        smooth_calcium_data = smooth_data_preprocess(\n",
    "            calcium_data, time_in_seconds, smooth_method\n",
    "        )\n",
    "        smooth_residual_calcium = smooth_data_preprocess(\n",
    "            residual_calcium, time_in_seconds, smooth_method\n",
    "        )\n",
    "        named_neuron_indices = random.sample(range(num_signals), num_named_neurons)\n",
    "        named_neurons = set(NEURONS_302[i] for i in named_neuron_indices)\n",
    "        neuron_to_idx = {\n",
    "            (neuron) if neuron in named_neurons else str(idx): idx\n",
    "            for idx, neuron in enumerate(NEURONS_302)\n",
    "        }\n",
    "        idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "\n",
    "        worm_data = dict()\n",
    "\n",
    "        worm_data[\"worm\"] = f\"worm{worm_idx}\"\n",
    "        worm_data[\"dataset\"] = dataset_name\n",
    "        worm_data[\"smooth_method\"] = smooth_method\n",
    "        worm_data[\"calcium_data\"] = calcium_data\n",
    "        worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "        worm_data[\"residual_calcium\"] = residual_calcium\n",
    "        worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "        worm_data[\"max_timesteps\"] = total_time\n",
    "        worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "        worm_data[\"dt\"] = dt\n",
    "        worm_data[\"resample_median_dt\"] = resample_dt\n",
    "        worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "        worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "        worm_data[\"num_neurons\"] = num_signals\n",
    "        worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "        worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "\n",
    "        worm_data = reshape_calcium_data(worm_data)\n",
    "        worm_dataset[f\"worm{worm_idx}\"] = worm_data\n",
    "\n",
    "        worm_idx += 1\n",
    "        calcium_data = []\n",
    "        total_time = 0\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", worm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plotting the dataset\n",
    "\n",
    "# Selecting a worm and all the neurons to plot\n",
    "num_worms = len(worm_dataset)\n",
    "worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "neuron_idx = [idx for idx in worm_dataset[worm_idx][\"slot_to_neuron\"].keys()][\n",
    "    :num_named_neurons\n",
    "]\n",
    "\n",
    "# Plotting dataset\n",
    "plot_neural_signals(\n",
    "    data=worm_dataset[worm_idx][\"calcium_data\"],\n",
    "    time_tensor=worm_dataset[worm_idx][\"time_in_seconds\"],\n",
    "    neuron_idx=neuron_idx,\n",
    "    yax_limit=False,\n",
    "    suptitle=f\"{dataset_name} - {worm_idx}\",\n",
    ")\n",
    "\n",
    "# Visualize covariance matrix\n",
    "data = worm_dataset[worm_idx][\"smooth_calcium_data\"]\n",
    "mask = worm_dataset[worm_idx][\"named_neurons_mask\"]\n",
    "neurons = sorted(worm_dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "X = data[:, mask].numpy()\n",
    "n = X.shape[0]\n",
    "X_bar = X - np.mean(X, axis=0)\n",
    "cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting 3D trajectory\n",
    "plot_3d_trajectory(\n",
    "    X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load a trained model\n",
    "\n",
    "checkpoint_path = \"/om2/user/qsimeon/worm-graph/logs/hydra/2023_12_25_06_45_54/exp10/train/checkpoints/model_best.pt\"\n",
    "model = load_model_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(chunk), end=\"\\n\\n\")\n",
    "embedding = embedding.to(DEVICE)\n",
    "idx = torch.LongTensor(chunk).unsqueeze(0).to(DEVICE)\n",
    "input = embedding(idx)\n",
    "mask = torch.ones(input.shape[-1], dtype=torch.bool).unsqueeze(0).to(DEVICE)\n",
    "print(idx.shape, input.shape, mask.shape, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 100\n",
    "idx_gen = model.transformer_generate(idx, max_new_tokens, mask, embedding)\n",
    "\n",
    "print(tokenizer.decode(idx.tolist()[0]), end=\"\\n\\n\")\n",
    "print(tokenizer.decode(idx_gen.tolist()[0][-max_new_tokens:]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanoGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
