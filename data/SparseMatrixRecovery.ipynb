{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring recovering connectivity from sparse observation data\n",
    "\n",
    "**What does it take to reverse engineer a worm: Power and friends**\n",
    "\n",
    "Running perturbation experiments to reverse engineer the input output functions of all neurons in C. elegans has recently been proposed as a strategy towards whole nervous system emulation. However, this leads to difficult inverse problems where causal interactions have to be inferred from observational data. Here we ask how hard it is to reverse engineer such systems using both theoretical and experimental approaches. We find that reverse engineering benefits considerably from stimulation, requires the ability to integrate experimental data across animals, and should require thousands of experiments. Nonetheless, the resulting numbers should allow reverse engineering the neuronal input output functions under relatively moderate assumptions about simplicity of neurons in C. elegans. \n",
    "\n",
    "The connectome of C. elegans is known to be roughly the same across genotypically matched animals, yet how that connectivity ultimately determines neural dynamics and behavior across different animals is unknown. We consider the problem of reverse engineering the nervous system of C. elegans as a systems identification problem where multiple partially observed instances of the dynamics of a common system are to be used to reconstruct the true underlying system. We think that this work is important for thinking about causal structure in the brain.\n",
    "\n",
    "*Last updated: 6 June 2024*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries & modules\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device found.\n",
      "\t GPU: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Union\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from preprocess._utils import smooth_data_preprocess, reshape_calcium_data\n",
    "from utils import NUM_NEURONS, NEURON_LABELS, init_random_seeds\n",
    "\n",
    "# Initialize the random seeds\n",
    "init_random_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper functions\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neural_signals(data, time_tensor, neuron_idx=None, yax_limit=True, suptitle=None):\n",
    "    assert isinstance(data, torch.Tensor), \"data must be a PyTorch tensor\"\n",
    "    assert isinstance(time_tensor, torch.Tensor), \"time_tensor must be a PyTorch tensor\"\n",
    "    assert data.dim() == 2, \"data must be a 2D tensor\"\n",
    "    assert isinstance(neuron_idx, (int, list)), \"neuron_idx must be an integer or list\"\n",
    "\n",
    "    time_tensor = time_tensor.squeeze()\n",
    "    assert data.size(0) == time_tensor.size(0), \"Number of rows in data and time_tensor must match\"\n",
    "\n",
    "    num_neurons = data.size(1)\n",
    "\n",
    "    # Randomly select the column indices if not provided\n",
    "    if isinstance(neuron_idx, int):\n",
    "        assert neuron_idx <= num_neurons, \"neuron_idx cannot exceed the number of neurons\"\n",
    "        column_indices = np.random.choice(num_neurons, neuron_idx, replace=False)\n",
    "    elif isinstance(neuron_idx, list):\n",
    "        assert len(neuron_idx) <= num_neurons, \"neuron_idx cannot exceed the number of neurons\"\n",
    "        column_indices = np.array(neuron_idx)\n",
    "\n",
    "    num_columns = len(column_indices)\n",
    "\n",
    "    # Extract the selected columns from the data tensor\n",
    "    selected_columns = data[:, column_indices]\n",
    "\n",
    "    # Define the color palette using scientific colors\n",
    "    colors = sns.color_palette(\"bright\", num_columns)\n",
    "\n",
    "    # Plotting subplots vertically\n",
    "    fig, axs = plt.subplots(num_columns, 1, figsize=(12, num_columns))\n",
    "    fig.tight_layout(pad=0.0)\n",
    "\n",
    "    # If num_columns is 1, make ax iterable by wrapping it in a list\n",
    "    if num_columns == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Now your existing loop should work without modification\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.plot(time_tensor, selected_columns[:, i], color=colors[i])\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position(\"none\")\n",
    "        if yax_limit:\n",
    "            ax.set_ylim(-1.0, 1.0)\n",
    "        ax.set_ylabel(\"{}\".format(NEURON_LABELS[column_indices[i]]))\n",
    "\n",
    "        if i < num_columns - 1:\n",
    "            ax.set_xticks([])\n",
    "        else:\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "\n",
    "    # Add a super title to the figure if provided\n",
    "    if suptitle is not None:\n",
    "        fig.suptitle(suptitle, fontsize=16)\n",
    "\n",
    "    plt.tight_layout(pad=1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_trajectory(X, axis_labels=(\"Time\", \"Value\", \"Z Axis\"), title=\"Trajectory\", show=True):\n",
    "    \"\"\"\n",
    "    Plot a trajectory from a dataset, which can be 1D, 2D, or 3D.\n",
    "\n",
    "    Parameters:\n",
    "    - X: A 2D numpy array containing the trajectory data. Must be shaped as (time, features).\n",
    "    - axis_labels: A tuple containing the labels for the axes. Default is ('Time', 'Value', 'Z Axis').\n",
    "    - title: Title of the plot.\n",
    "    - show: If True, the plot will be displayed. If False, the plot object will be returned.\n",
    "\n",
    "    Returns:\n",
    "    - fig, ax: The figure and axis objects of the plot if show is False.\n",
    "    \"\"\"\n",
    "    max_timesteps = X.shape[0]\n",
    "    dims = X.shape[1] if len(X.shape) > 1 else 1\n",
    "\n",
    "    # Create a new figure for the plot\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    if dims >= 3:\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        plot_dims = 3\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "        plot_dims = dims\n",
    "\n",
    "    # Create a color map based on the time progression\n",
    "    norm = plt.Normalize(0, max_timesteps)\n",
    "    colors = plt.cm.viridis(norm(np.arange(max_timesteps)))\n",
    "\n",
    "    # Extract coordinates from the data and plot accordingly\n",
    "    if plot_dims == 1:\n",
    "        x = np.arange(max_timesteps)\n",
    "        y = X.flatten()\n",
    "        z = np.zeros(max_timesteps)\n",
    "        ax_labels = (axis_labels[0], \"Value\", \"Fixed Z\")\n",
    "        # Plot the 1D trajectory with a color gradient\n",
    "        for i in range(1, max_timesteps):\n",
    "            ax.plot(x[i - 1 : i + 1], y[i - 1 : i + 1], color=colors[i], lw=0.5)\n",
    "        # Mark the start and end of the trajectory\n",
    "        ax.plot(x[0], y[0], \"g*\", markersize=8)  # Start with a green star\n",
    "        ax.plot(x[-1], y[-1], \"ro\", markersize=7)  # End with a red circle\n",
    "    elif plot_dims == 2:\n",
    "        # Pick two random columns to plot\n",
    "        ind_x, ind_y = np.random.choice(X.shape[1], 2, replace=False)\n",
    "        x, y = X[:, ind_x], X[:, ind_y]\n",
    "        z = np.zeros(max_timesteps)\n",
    "        ax_labels = (axis_labels[0], axis_labels[1], \"Fixed Z\")\n",
    "        # Plot the 2D trajectory\n",
    "        for i in range(1, max_timesteps):\n",
    "            ax.plot(x[i - 1 : i + 1], y[i - 1 : i + 1], color=colors[i], lw=0.5)\n",
    "        # Mark the start and end of the trajectory\n",
    "        ax.plot(x[0], y[0], \"g*\", markersize=8)  # Start with a green star\n",
    "        ax.plot(x[-1], y[-1], \"ro\", markersize=7)  # End with a red circle\n",
    "    else:  # plot_dims == 3\n",
    "        ind_x, ind_y, ind_z = np.random.choice(X.shape[1], 3, replace=False)\n",
    "        x, y, z = X[:, ind_x], X[:, ind_y], X[:, ind_z]\n",
    "        ax_labels = axis_labels\n",
    "        # Plot the 3D trajectory\n",
    "        for i in range(1, max_timesteps):\n",
    "            ax.plot(\n",
    "                x[i - 1 : i + 1],\n",
    "                y[i - 1 : i + 1],\n",
    "                z[i - 1 : i + 1],\n",
    "                color=colors[i],\n",
    "                lw=0.5,\n",
    "            )\n",
    "        # Mark the start and end of the trajectory\n",
    "        ax.plot(x[0], y[0], z[0], \"g*\", markersize=8)  # Start with a green star\n",
    "        ax.plot(x[-1], y[-1], z[-1], \"ro\", markersize=7)  # End with a red circle\n",
    "\n",
    "    # Set labels for the axes\n",
    "    ax.set_xlabel(ax_labels[0])\n",
    "    ax.set_ylabel(ax_labels[1])\n",
    "    if plot_dims == 3:\n",
    "        ax.set_zlabel(ax_labels[2])\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_synthetic_dataset(file_name, dataset):\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spectral_radius(matrix):\n",
    "    eigenvalues = np.linalg.eigvals(matrix).real\n",
    "    spectral_radius = max(abs(eigenvalues))\n",
    "    return spectral_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_matrix_to_edge_of_chaos(matrix, target_radius=1.0):\n",
    "    # Calculate the current spectral radius\n",
    "    current_radius = calculate_spectral_radius(matrix)\n",
    "\n",
    "    # Calculate the gain needed to adjust the spectral radius to the target\n",
    "    gain = target_radius / current_radius\n",
    "\n",
    "    # Scale the matrix by the gain\n",
    "    adjusted_matrix = matrix * gain\n",
    "\n",
    "    return adjusted_matrix, gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get default parameter values from configs and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"../configs/submodule/preprocess.yaml\")\n",
    "DELTA_T = config.preprocess.resample_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Network dataset\n",
    "\n",
    "Dynamics evolve according to\n",
    "\n",
    "$$ \\tau \\frac{d\\mathbf{x}}{dt} = -\\mathbf{x} + \\mathbf{M} f(\\mathbf{x}) + \\mathbf{b} $$\n",
    "\n",
    "where $\\mathbf{b}$ is a vector of external inputs and $\\mathbf{M}$ is a connectivity matrix. $\\mathbf{M}$ is a sparse random graph with non-zero weights chosen i.i.d $\\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "We define $\\alpha = \\frac{\\Delta t}{\\tau}$. The discrete-time update equation of the RNN is given by\n",
    "\n",
    "$$ \\mathbf{x}(t + \\Delta t) = \\left(1 - \\alpha\\right) \\mathbf{x}(t) + \\alpha \\mathbf{M} f\\left(\\mathbf{x}(t)\\right) + \\alpha \\mathbf{b}(t) $$\n",
    "\n",
    "**Considerations:**\n",
    "- The time constant $\\tau$ may be different for different neurons in the network, in which case $\\mathbf{\\tau}$ should be viewed as a vector $\\mathbf{\\tau}$. Furthermore, $\\tau$ could be time-dependent $\\tau(t)$, which may be due to neuromodulation.\n",
    "- For our simulations, and also with real experimental data, we generally know (or can resample) the sampling interval (i.e. measurement timestep) $\\Delta t$. But since we don't know (or don't have good estimates of) the time constant $\\tau$, this makes $\\alpha$ unknown. \n",
    "- However, a common assumption from rate-based modeling is that $\\Delta t$ is much shorter $\\tau$.\n",
    "- Therefore $\\alpha$  (or $\\mathbf{\\alpha}$ in the vectorized case) is non-negative and bounded $0 < \\alpha <1$.\n",
    "- Using $\\alpha=1$ is simpler and makes things easier without changing the expressivity of the network.\n",
    "\n",
    "---\n",
    "\n",
    "We will simplify the problem by assuming $\\alpha=1$. It turns out that this may not reduce the expressivity as the effect of the time constant is subsumed by the eigenvalues of the matrix $\\mathbf{M}$:\n",
    "\n",
    "$$ \\mathbf{x}(t + \\Delta t) =  \\mathbf{M} f\\left(\\mathbf{x}(t)\\right) + \\mathbf{b}(t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_dataset_recurrent(\n",
    "    max_timesteps: int = 1000,\n",
    "    num_worms: int = 1,\n",
    "    num_signals: int = NUM_NEURONS,\n",
    "    num_named_neurons: Union[None, int] = None,\n",
    "    add_noise: bool = False,\n",
    "    noise_std: float = 0.01,\n",
    "    random_walk: bool = False,\n",
    "    # # >>> any special arguments for this function should go here >>>\n",
    "    input_signal_gain: float = 1.0,\n",
    "    intrinsic_noise_gain: float = 0.0,\n",
    "    num_sensors: int = 1,\n",
    "    # signal_type: Union[None, str] = \"step\", # options: {\"step\", \"sine\"}\n",
    "    # # <<< any special arguments for this function should go here <<<\n",
    "    delta_seconds: float = DELTA_T,\n",
    "    smooth_method: Union[None, str] = \"exponential\",\n",
    "    transform: Union[None, callable] = StandardScaler(),\n",
    "    dataset_name: str = \"Recurrent0000\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a synthetic worm datasets using the Lorenz attractor.\n",
    "    Three neurons are chosen randomly to represent x, y, z trajectories from the Lorenz system.\n",
    "\n",
    "    :param max_timesteps: The number of timepoints of synthetic data to generate.\n",
    "    :param num_worms: The number of synthetic worms to create datasets for.\n",
    "    :param num_signals: The number of signals corresponding to number of neurons.\n",
    "    :param num_named_neurons: The number of measured neurons to create non-zero signals for.\n",
    "    :param add_noise: Whether to simulate measurement noise by adding Gaussian noise to the synthetic data.\n",
    "    :param noise_std: The standard deviation of the i.i.d Gaussian measurement noise.\n",
    "    :param random_walk: If True, use a random walk to generate the noise. Otherwise, use iid noise.\n",
    "    :param delta_seconds: The constant time difference (in seconds) between each measurement of the system.\n",
    "    :param smooth_method: The method to use for smoothing the data.\n",
    "    :param transform: The sklearn method to scale or transform the data before use.\n",
    "    :param dataset_name: The name to give the synthetic dataset.\n",
    "    :return: A dictionary containing the synthetic worm datasets.\n",
    "    \"\"\"\n",
    "    # ### DEBUG ###\n",
    "    # # For when doing more general (not C. elegans) graphs\n",
    "    # from string import printable # up to 100 characters\n",
    "    # NEURON_LABELS = printable[:num_signals]\n",
    "    # print(NEURON_LABELS)\n",
    "    # ### DEBUG ###\n",
    "    eps = np.finfo(float).eps\n",
    "    dataset = {}\n",
    "    # Determine the timepoints for sampling the data (i.e. \"measurement\" times)\n",
    "    time_in_seconds = delta_seconds * np.arange(max_timesteps).reshape(-1, 1)  # column vector\n",
    "    # Calculate number of named and unknown neurons\n",
    "    if num_named_neurons is None or num_named_neurons > num_signals:  # default to all neurons\n",
    "        num_named_neurons = num_signals\n",
    "    elif num_named_neurons < 0:  # default to no neurons\n",
    "        num_named_neurons = 0\n",
    "    num_unknown_neurons = num_signals - num_named_neurons\n",
    "    # Define a fixed minimal connectivity matrix which ensures that the network is connected\n",
    "    sparsity = 0.0\n",
    "    connected = False\n",
    "    while not connected:\n",
    "        sparsity += 1 / num_signals\n",
    "        sparse_mask = np.random.choice(\n",
    "            [0, 1], size=(num_signals, num_signals), p=[1 - sparsity, sparsity]\n",
    "        )\n",
    "        # Enforcing autapses (diagonal connections) ensures the connectivity matrix is full rank\n",
    "        sparse_mask[np.diag_indices(num_signals)] = 1\n",
    "        G = nx.from_numpy_array(sparse_mask)\n",
    "        connected = nx.is_connected(G)\n",
    "    connectivity_matrix = sparse_mask * np.random.rand(\n",
    "        num_signals, num_signals\n",
    "    )  # non-negative weights\n",
    "    # Adjust the connectivity matrix to the edge of chaos\n",
    "    connectivity_matrix, _ = adjust_matrix_to_edge_of_chaos(connectivity_matrix, target_radius=1.0)\n",
    "    spectral_radius = calculate_spectral_radius(connectivity_matrix) # should be close to 1.0\n",
    "    ### DEBUG ###\n",
    "    # Evolve the dynamics specified by the fixed connectivity and prespecified time constants\n",
    "    ### DEBUG ###\n",
    "    # Some warmup timesteps to allow the system to reach a steady state\n",
    "    warmup_timesteps = max_timesteps // 6\n",
    "    simulation_steps = max_timesteps + warmup_timesteps\n",
    "    ### DEBUG ###\n",
    "    # Calculate the signal-to-noise ratio\n",
    "    signal_noise_ratio = input_signal_gain / max(\n",
    "        eps, intrinsic_noise_gain\n",
    "    )  # avoid division by zero\n",
    "    # Choose which neurons to be the sensory interface (i.e receive inputs)\n",
    "    sensorium = sorted(np.random.choice(num_signals, size=num_sensors, replace=False))\n",
    "    # Signal (i.e. control law) will only be applied at the sensorium\n",
    "    sensory_mask = np.zeros(num_signals)\n",
    "    sensory_mask[sensorium] = 1\n",
    "    # Add a uniqe phase shift to each neuron\n",
    "    phase_shift = np.random.uniform(low=0, high=2 * np.pi, size=num_signals)\n",
    "    ### DEBUG ###\n",
    "    # Create data for each worm\n",
    "    # TODO: This can be parallelized since each worm is independent\n",
    "    for worm_idx in range(num_worms):\n",
    "        # Initialize worm data\n",
    "        worm = f\"worm{worm_idx}\"\n",
    "        worm_data = dict()\n",
    "        calcium_data = np.zeros((max_timesteps + warmup_timesteps, num_signals))\n",
    "        # Choose a random subset of neurons to record / observe / measure\n",
    "        named_neuron_indices = random.sample(\n",
    "            range(num_signals), num_named_neurons\n",
    "        )  # without replacement\n",
    "        named_neurons = set(NEURON_LABELS[i] for i in named_neuron_indices)\n",
    "        # Create neuron to idx mapping and vice versa\n",
    "        neuron_to_idx = {\n",
    "            (neuron) if neuron in named_neurons else str(idx): idx\n",
    "            for idx, neuron in enumerate(NEURON_LABELS)\n",
    "        }\n",
    "        idx_to_neuron = {idx: neuron for neuron, idx in neuron_to_idx.items()}\n",
    "        # We define \"input\" to as a signal/no signal applied on top of background intrinsic noise\n",
    "        input_matrix = np.zeros_like(calcium_data)  # initialize with noise\n",
    "        # Create calcium data by evolving the dynamics\n",
    "        for t in range(simulation_steps):\n",
    "            # Initial conditions\n",
    "            if t == 0:\n",
    "                signal = input_signal_gain * np.zeros(num_signals)  # zero input signal at the start\n",
    "                noise = intrinsic_noise_gain * np.random.randn(num_signals)  # random noise\n",
    "                # Apply input signal only at the sensorium but intrinsic noise everywhere\n",
    "                inputs = sensory_mask * signal + noise\n",
    "                # Set the initial state of the network\n",
    "                # state = np.zeros(num_signals)  # zeros initialization\n",
    "                state = np.random.uniform(\n",
    "                    low=-5.0, high=5.0, size=num_signals\n",
    "                )  # random initialization\n",
    "            # Evolve recurrent dynamics\n",
    "            else:\n",
    "                # Specify the input signal to apply at each neuron for this timestep\n",
    "                # TODO: In reality, we want to design the control input u = signal to achieve some desired trajectory.\n",
    "                # ### DEBUG ###\n",
    "                # # Sinusoidal input signal # TODO: make an argument of the function\n",
    "                # freq = 10 # number of cycles in the simulation # TODO: make an argument of the function\n",
    "                # signal = input_signal_gain * np.sin(2 * np.pi * freq * t / (simulation_steps - 1) + phase_shift/freq) # unique phase shift per neuron\n",
    "                # ### DEBUG ###\n",
    "                ### DEBUG ###\n",
    "                # Step input signal # TODO: make an argument of the function\n",
    "                modulo = 100  # period (in timesteps) of one cycle # TODO: make an argument of the function\n",
    "                signal = input_signal_gain * np.heaviside(\n",
    "                    ((t + modulo * phase_shift) % modulo) - modulo / 2, 0\n",
    "                )  # unique phase shift per neuron\n",
    "                ### DEBUG ###\n",
    "                noise = intrinsic_noise_gain * np.random.randn(num_signals)\n",
    "                # Apply input signal only at the sensorium but intrinsic noise everywhere\n",
    "                inputs = sensory_mask * signal + noise\n",
    "                # Integrate the inputs and current state to get the next state\n",
    "                state = connectivity_matrix @ np.tanh(state) + inputs\n",
    "            # Record or 'measure' the state of the named neurons and all the inputs\n",
    "            calcium_data[t][named_neuron_indices] = state[named_neuron_indices]\n",
    "            input_matrix[t] = inputs\n",
    "        # Discard warmup timesteps\n",
    "        calcium_data = calcium_data[warmup_timesteps:]\n",
    "        input_matrix = input_matrix[warmup_timesteps:]\n",
    "        # Add i.i.d measurement/observation noise\n",
    "        if add_noise:\n",
    "            for neuron_index in named_neuron_indices:\n",
    "                if random_walk:\n",
    "                    noise_walk = np.cumsum(\n",
    "                        [0]\n",
    "                        + np.random.normal(loc=0, scale=noise_std, size=max_timesteps - 1).tolist()\n",
    "                    )\n",
    "                    calcium_data[:, neuron_index] += noise_walk\n",
    "                else:\n",
    "                    noise_iid = np.random.normal(0, noise_std, max_timesteps)\n",
    "                    calcium_data[:, neuron_index] += noise_iid\n",
    "        # Normalize the data\n",
    "        if transform:\n",
    "            calcium_data = transform.fit_transform(calcium_data)\n",
    "        # Calculate residuals\n",
    "        dt = np.diff(time_in_seconds, axis=0, prepend=0.0)\n",
    "        resample_dt = np.median(dt[1:]).item()\n",
    "        residual_calcium = np.gradient(calcium_data, time_in_seconds.squeeze(), axis=0)\n",
    "        # Smooth the data and convert to tensors\n",
    "        smooth_calcium_data = smooth_data_preprocess(\n",
    "            calcium_data,\n",
    "            time_in_seconds,\n",
    "            smooth_method,\n",
    "            **dict(alpha=0.5, window_size=15, sigma=5),\n",
    "        )\n",
    "        smooth_residual_calcium = smooth_data_preprocess(\n",
    "            residual_calcium,\n",
    "            time_in_seconds,\n",
    "            smooth_method,\n",
    "            **dict(alpha=0.5, window_size=15, sigma=5),\n",
    "        )\n",
    "        # Save the data\n",
    "        worm_data[\"worm\"] = worm\n",
    "        worm_data[\"source_dataset\"] = dataset_name\n",
    "        worm_data[\"smooth_method\"] = smooth_method\n",
    "        worm_data[\"calcium_data\"] = calcium_data\n",
    "        worm_data[\"smooth_calcium_data\"] = smooth_calcium_data\n",
    "        worm_data[\"residual_calcium\"] = residual_calcium\n",
    "        worm_data[\"smooth_residual_calcium\"] = smooth_residual_calcium\n",
    "        worm_data[\"max_timesteps\"] = max_timesteps\n",
    "        worm_data[\"time_in_seconds\"] = time_in_seconds\n",
    "        worm_data[\"dt\"] = dt\n",
    "        worm_data[\"median_dt\"] = resample_dt\n",
    "        worm_data[\"neuron_to_idx\"] = neuron_to_idx\n",
    "        worm_data[\"idx_to_neuron\"] = idx_to_neuron\n",
    "        worm_data[\"num_neurons\"] = num_signals\n",
    "        worm_data[\"num_named_neurons\"] = num_named_neurons\n",
    "        worm_data[\"num_unknown_neurons\"] = num_unknown_neurons\n",
    "        worm_data[\"extra_info\"] = {\n",
    "            \"adjacency_matrix\": sparse_mask,\n",
    "            \"connection_weights\": connectivity_matrix,\n",
    "            \"input_matrix\": input_matrix,\n",
    "            \"spectral_radius\": spectral_radius,\n",
    "            \"sparsity\": sparsity,\n",
    "            ### DEBUG ###\n",
    "            \"sensorium\": sensorium,\n",
    "            \"sensory_mask\": sensory_mask,\n",
    "            \"signal_noise_ratio\": signal_noise_ratio,\n",
    "            ### DEBUG ###\n",
    "            \"measurement_noise\": add_noise,\n",
    "            \"iid_noise_std\": noise_std * int(add_noise),\n",
    "            \"meta_text\": \"`input_matrix` is a matrix where each row is the pattern of input applied to the network.\\n\"\n",
    "            \"adjacency_matrix` is binary.\\n`connection_weights` has the edge strengths.\\n\"\n",
    "            \"`spectral_radius` is the maximum absolute value of the eigenvalues of `connection_weights`.\\n\"\n",
    "            \"`measurement_noise` indicates whether i.i.d Gaussian noise with mean 0 and variance `iid_noise_std`^2 was added to the observed 'neural' data.\\n\",\n",
    "        }\n",
    "        # Reshape the data to the standardized format\n",
    "        worm_data = reshape_calcium_data(worm_data)\n",
    "        # Save the data\n",
    "        dataset[worm] = worm_data\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Matrix $A$ Definition and Eigenvalues\n",
    "\n",
    "Let $A$ be some sqaure matrix defined as:\n",
    "$$A = -I + D$$\n",
    "where $I$ is the identity matrix and $D$ is a general square matrix (not necessarily diagonal) with spectral radius $\\rho(D) \\leq 1$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Spectral Radius and Eigenvalues\n",
    "\n",
    "The spectral radius of a general matrix $M$ is defined as:\n",
    "$$\\rho(M) = \\max_{\\lambda \\in \\sigma(M)} |\\lambda|$$\n",
    "where $\\sigma(M)$ denotes the spectrum (set of eigenvalues) of $M$.\n",
    "\n",
    "Given $\\rho(D) \\leq 1$, the absolute value of any eigenvalue of $D$ does not exceed 1. This bounds the eigenvalues $\\lambda_D$ of $D$ such that:\n",
    "$$|\\lambda_D| \\leq 1$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Impact on Eigenvalues of $A$\n",
    "\n",
    "The eigenvalues of $A$, denoted as $\\lambda_A$, are not necessarily the simple arithmetic transformations of the eigenvalues of $D$ as in the diagonal case, but they still relate to the eigenvalues of $D$. Specifically, if $\\lambda_D$ is an eigenvalue of $D$, then $\\lambda_A = -1 + \\lambda_D$ is an eigenvalue of $A$, arising from the relationship:\n",
    "$$\\det(A - \\lambda I) = \\det((-I + D) - \\lambda I) = \\det(D - (1 + \\lambda) I) = 0$$\n",
    "\n",
    "This results in the eigenvalues  $\\lambda_A$ of $A$ being:\n",
    "$$\\lambda_A = -1 + \\lambda_D$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Stability Analysis\n",
    "\n",
    "To check the stability:\n",
    "- If $\\lambda_D$ ranges in absolute value up to 1, the real part of $\\lambda_A = -1 + \\lambda_D$ will be at most zero when $\\lambda_D$ reaches its maximum magnitude (and is real).\n",
    "- This yields the largest real part of $\\lambda_A$ as zero when $\\lambda_D = 1$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Conclusion\n",
    "\n",
    "With the real part of the eigenvalues of $A$ ranging up to zero, we arrive at a similar conclusion as in the diagonal case:\n",
    "- The system characterized by $\\frac{dx}{dt} = Ax(t)$ is **marginally stable** if the maximum eigenvalue $\\lambda_D$ of $D$ reaches exactly 1.\n",
    "- If all eigenvalues of $D$ have magnitudes strictly less than 1, then all eigenvalues of $A$ will have negative real parts, leading to a **stable** system.\n",
    "\n",
    "Thus, the system's stability hinges critically on the exact values of $D$'s eigenvalues. If $\\rho(D) < 1$ (strictly less), the system is stable. If $\\rho(D) = 1$, the system is marginally stable, particularly dependent on the nature and algebraic multiplicity of the eigenvalue(s) at the boundary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_NEURONS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m max_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      3\u001b[0m num_worms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 4\u001b[0m num_signals \u001b[38;5;241m=\u001b[39m \u001b[43mNUM_NEURONS\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Reflect a cost-benefit tradeoff between throughput and accuracy\u001b[39;00m\n\u001b[1;32m      6\u001b[0m num_named_neurons \u001b[38;5;241m=\u001b[39m num_signals \u001b[38;5;241m-\u001b[39m num_worms \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_NEURONS' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "max_timesteps = 1000\n",
    "num_worms = 100\n",
    "num_signals = NUM_NEURONS\n",
    "# Reflect a cost-benefit tradeoff between throughput and accuracy\n",
    "num_named_neurons = num_signals - num_worms + 1\n",
    "add_noise = False  # measurement noise\n",
    "noise_std = 0.01\n",
    "random_walk = False\n",
    "input_signal_gain = 1.0\n",
    "intrinsic_noise_gain = 0.01\n",
    "num_sensors = 1\n",
    "delta_seconds = DELTA_T\n",
    "smooth_method = None\n",
    "transform = StandardScaler()\n",
    "dataset_name = \"Recurrent0000\"\n",
    "\n",
    "# Creating and saving datasets\n",
    "dataset = create_synthetic_dataset_recurrent(\n",
    "    max_timesteps=max_timesteps,\n",
    "    num_worms=num_worms,\n",
    "    num_signals=num_signals,\n",
    "    num_named_neurons=num_named_neurons,\n",
    "    add_noise=add_noise,\n",
    "    noise_std=noise_std,\n",
    "    random_walk=random_walk,\n",
    "    input_signal_gain=input_signal_gain,\n",
    "    intrinsic_noise_gain=intrinsic_noise_gain,\n",
    "    num_sensors=num_sensors,\n",
    "    delta_seconds=delta_seconds,\n",
    "    smooth_method=smooth_method,\n",
    "    transform=transform,\n",
    "    dataset_name=dataset_name,\n",
    ")\n",
    "\n",
    "# Save the dataset\n",
    "save_synthetic_dataset(f\"processed/neural/{dataset_name}.pickle\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper code for plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thunk():\n",
    "    global num_worms, worm_idx, neuron_idx\n",
    "    # Selecting a worm and all the neurons to plot\n",
    "    num_worms = len(dataset)\n",
    "    worm_idx = random.choice([f\"worm{i}\" for i in range(num_worms)])\n",
    "    neuron_idx = [idx for idx in dataset[worm_idx][\"slot_to_neuron\"].keys()][:num_named_neurons]\n",
    "\n",
    "    # Plotting dataset\n",
    "    plot_neural_signals(\n",
    "        data=dataset[worm_idx][\"calcium_data\"],\n",
    "        time_tensor=dataset[worm_idx][\"time_in_seconds\"],\n",
    "        neuron_idx=neuron_idx,\n",
    "        yax_limit=False,\n",
    "        suptitle=f\"{dataset_name} - {worm_idx}\",\n",
    "    )\n",
    "\n",
    "    # Visualize covariance matrix\n",
    "    data = dataset[worm_idx][\"calcium_data\"]\n",
    "    mask = dataset[worm_idx][\"named_neurons_mask\"]\n",
    "    neurons = sorted(dataset[worm_idx][\"named_neuron_to_slot\"])\n",
    "\n",
    "    # X = data[:, mask].numpy()\n",
    "    # n = X.shape[0]\n",
    "    # # centering the data here is redundant if StandardScaler was used when creating the dataset\n",
    "    # X_bar = X - np.mean(X, axis=0, keepdims=True)\n",
    "    # cov = 1 / (n - 1) * X_bar.T @ X_bar\n",
    "\n",
    "    # plt.figure()\n",
    "    # ax = sns.heatmap(cov, cmap=\"coolwarm\", xticklabels=neurons, yticklabels=neurons)\n",
    "    # ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "    # plt.show()\n",
    "\n",
    "    # # Plotting 3D trajectory\n",
    "    # plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")\n",
    "\n",
    "    ### DEBUG ###\n",
    "    V = data.numpy()\n",
    "    n = V.shape[0]\n",
    "    # Centering the data here is redundant if StandardScaler was used when creating the dataset\n",
    "    V_bar = V - np.mean(V, axis=0, keepdims=True)\n",
    "    cov = 1 / (n - 1) * V_bar.T @ V_bar\n",
    "    X = V[:, mask.numpy()]\n",
    "\n",
    "    heat_mask = (mask.unsqueeze(1).numpy() * 1) @ (mask.unsqueeze(0).numpy() * 1)\n",
    "    heat_mask = ~heat_mask.astype(bool)\n",
    "    cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "    cmap.set_bad(color=\"black\")  # Set the color for NaN values\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    ax = sns.heatmap(\n",
    "        cov, cmap=cmap, mask=heat_mask, xticklabels=NEURON_LABELS, yticklabels=NEURON_LABELS\n",
    "    )\n",
    "    # Adjust the font size of x and y tick labels\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=4)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=4)\n",
    "    ax.set_title(f\"Covariance matrix : {dataset_name}, {worm_idx}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting 3D trajectory\n",
    "    plot_3d_trajectory(X, axis_labels=tuple(neurons), title=f\"{dataset_name} neural trajectory\")\n",
    "    ### DEBUG ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the dataset\n",
    "thunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of recurrent network system recovery from sparse data\n",
    "\n",
    "Let us rearrange the discrete-time update equation from earlier:\n",
    "\n",
    "$$ \\mathbf{x}(t + \\Delta t) =  \\mathbf{M} f\\left(\\mathbf{x}(t)\\right) + \\mathbf{b}(t) $$\n",
    "\n",
    "in a way that makes the \"push-forward\" function more obvious and makes the matrix we wish to recover from the data more explicit. \n",
    "\n",
    "**N.B.**: What we refer to as the \"push-forward\" is the function the maps from the current state at time $t$ to the subsequent state at time $t+\\Delta t$.\n",
    "\n",
    "We will make use of the linear approximation $f(x) \\approx x$ for small $x$:\n",
    "\n",
    "$$ \\mathbf{M} \\mathbf{x}(t) + \\mathbf{b}(t) = \\mathbf{x}(t + \\Delta t) $$\n",
    "\n",
    "Right multiplying on both sides by $\\mathbf{x}(t)^\\intercal$ gives:\n",
    "\n",
    "$$ \\mathbf{M} \\mathbf{x}(t)\\mathbf{x}(t)^\\intercal + \\mathbf{b}(t)\\mathbf{x}(t)^\\intercal = \\mathbf{x}(t + \\Delta t)\\mathbf{x}(t)^\\intercal $$\n",
    "\n",
    "Since $\\mathbf{x}(t)$ is a vector we know that the matrices $\\mathbf{x}(t)\\mathbf{x}(t)^\\intercal, \\mathbf{b}(t)\\mathbf{x}(t)^\\intercal, \\mathbf{x}(t + \\Delta t)\\mathbf{x}(t)^\\intercal$ are all rank-$1$ and thus not invertible. But since we have multiple samples (a.k.a observations or measurements) of the state $\\mathbf{x}(t), t \\in \\{0,1,2, ...,T\\}$ the corresponding sample covariance matrices:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\mathbf{\\Sigma}}_x &= \\frac{1}{T} \\sum_{t=0}^{T}\\mathbf{x}(t)\\mathbf{x}(t)^\\intercal \\\\\n",
    "\\hat{\\mathbf{\\Sigma}}_{bx} &= \\frac{1}{T} \\sum_{t=0}^{T} {\\mathbf{b}(t)\\mathbf{x}(t)^\\intercal} \\\\\n",
    "\\hat{\\mathbf{\\Sigma}}_{{\\Delta t}x} &= \\frac{1}{T} \\sum_{t=0}^{T}\\mathbf{x}(t + \\Delta t)\\mathbf{x}(t)^\\intercal\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "will be full rank, and thus invertible, if a minimal set $\\geq \\operatorname{dim}(\\mathbf{x})$ of linearly independent states are sampled. If suffficiently rich dynamics are present then the probability of this being true $\\to 1$ as $T \\to \\infty$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\mathbf{M}}\\hat{\\mathbf{\\Sigma}}_x + \\hat{\\mathbf{\\Sigma}}_{bx} &= \\hat{\\mathbf{\\Sigma}}_{{\\Delta t}x} \\\\\n",
    "\\rightarrow \\hat{\\mathbf{M}} &= \\left( \\hat{\\mathbf{\\Sigma}}_{{\\Delta t}x} - \\hat{\\mathbf{\\Sigma}}_{bx} \\right) \\left( \\hat{\\mathbf{\\Sigma}}_x \\right)^{-1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can think of the external input $\\mathbf{b}$ as the environment, which is impossible to measure perfectly and often not measured at all. However, we can simplify the problem of recovering the matrix $\\mathbf{B}$ by assuming either: \n",
    " 1. no external input $\\mathbf{b}(t) = \\mathbf{0}$, or\n",
    " 2. the input and state are uncorrelated $\\hat{\\mathbf{\\Sigma}}_{bx} = \\mathbf{0}$.\n",
    "\n",
    "**N.B.** Neither of these assumptions are fully valid in the real world but the second one is a slightly more reasonable. The second assumption essentially says that the environment is independent of the network state.\n",
    "\n",
    "With the above assumptions, the equation for recovering the matrix $\\mathbf{M}$ from observations/measurement data simplifies to:\n",
    "$$\n",
    "\\hat{\\mathbf{M}} = \\hat{\\mathbf{\\Sigma}}_{{\\Delta t} x} \\left( \\hat{\\mathbf{\\Sigma}}_x \\right)^{-1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG ###\n",
    "s = dataset[worm_idx][\"extra_info\"][\"sensorium\"]  # (num_sensors,)\n",
    "print(f\"sensory neurons: {np.array(NEURON_LABELS)[s]}\\n\")\n",
    "\n",
    "not_s = list(set(range(num_signals)) - set(s))  # (num_signal - num_sensors,)\n",
    "print(f\"non-sensory neurons: {np.array(NEURON_LABELS)[not_s]}\\n\")\n",
    "\n",
    "b = dataset[worm_idx][\"extra_info\"][\"input_matrix\"]  # (T, num_signals)\n",
    "print(f\"input matrix shape: {b.shape}\\n\")\n",
    "\n",
    "X = dataset[worm_idx][\"calcium_data\"].numpy()  # (T, num_signals)\n",
    "print(f\"measurement data shape: {X.shape}\\n\")\n",
    "\n",
    "if len(s) > 0:\n",
    "    plt.figure()\n",
    "    plt.plot(b[:, s[0]])\n",
    "    plt.title(f\"input at {np.array(NEURON_LABELS)[s[0]]}\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(X[:, s[0]])\n",
    "    plt.title(f\"measurement at {np.array(NEURON_LABELS)[s[0]]}\")\n",
    "    plt.show()\n",
    "\n",
    "if len(not_s) > 0:\n",
    "    plt.figure()\n",
    "    plt.plot(b[:, not_s[0]])\n",
    "    plt.title(f\"input at {np.array(NEURON_LABELS)[not_s[0]]}\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(X[:, not_s[0]])\n",
    "    plt.title(f\"measurement at {np.array(NEURON_LABELS)[not_s[0]]}\")\n",
    "    plt.show()\n",
    "### DEBUG ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_x = np.zeros((num_signals, num_signals))\n",
    "cov_dtx = np.zeros((num_signals, num_signals))\n",
    "cov_bx = np.zeros((num_signals, num_signals))  # oracle knowledge\n",
    "total_mask = np.zeros((num_signals, num_signals))\n",
    "M = None\n",
    "cap = 200  # < num_worms\n",
    "for idx in range(min(cap, num_worms)):\n",
    "    # Get current worm ID\n",
    "    worm_idx = f\"worm{idx}\"\n",
    "    # Check that \"connectome\" is the same for all worms\n",
    "    if M is None:\n",
    "        M = dataset[worm_idx][\"extra_info\"][\"connection_weights\"]\n",
    "        A = dataset[worm_idx][\"extra_info\"][\"adjacency_matrix\"]\n",
    "    else:\n",
    "        M_ = dataset[worm_idx][\"extra_info\"][\"connection_weights\"]\n",
    "        assert np.allclose(M, M_), \"Inconsisent connection weights!\"\n",
    "        M = M_\n",
    "        A_ = dataset[worm_idx][\"extra_info\"][\"adjacency_matrix\"]\n",
    "        assert np.allclose(A, A_), \"Inconsistent adjacency matrix!\"\n",
    "        A = A_\n",
    "    # Compute matrices needed to estimate the connectivity matrix\n",
    "    mask = dataset[worm_idx][\"named_neurons_mask\"].numpy().reshape(1, -1)\n",
    "    data = dataset[worm_idx][\"calcium_data\"].numpy()\n",
    "    # Having the inputs requires an oracle or perfect meaasurement of the environment\n",
    "    inputs = dataset[worm_idx][\"extra_info\"][\"input_matrix\"]  # (T, num_signals)\n",
    "    b = inputs\n",
    "    X = data\n",
    "    T = data.shape[0]\n",
    "    S = mask.T @ mask\n",
    "    total_mask += S\n",
    "    # # (num_signals, T) x (T, num_signals) -> (num_signals, num_signals)\n",
    "    # cov_x += ( (X[:-1].T @ X[:-1]) / T ) * S\n",
    "    # cov_dtx += ( (X[1:].T @ X[:-1]) / T ) * S\n",
    "    # cov_bx += ( (b[:-1].T @ X[:-1] ) / T ) * S\n",
    "    ### DEBUG ###\n",
    "    # Idea: Reduced dependence between spaced apart state samples\n",
    "    inds = np.unique(np.linspace(1, T - 1, 2 * num_signals, dtype=int))\n",
    "    cov_x += ((X[inds - 1].T @ X[inds - 1]) / len(inds)) * S\n",
    "    cov_dtx += ((X[inds].T @ X[inds - 1]) / len(inds)) * S\n",
    "    cov_bx += ((b[inds - 1].T @ X[inds - 1]) / len(inds)) * S\n",
    "    ### DEBUG ###\n",
    "total_mask = np.clip(total_mask, a_min=1, a_max=None)\n",
    "\n",
    "# Average covariance matrices over repetitions of the same neuron across all worms\n",
    "cov_x = np.divide(cov_x, total_mask)\n",
    "cov_dtx = np.divide(cov_dtx, total_mask)\n",
    "cov_bx = np.divide(cov_bx, total_mask)\n",
    "\n",
    "##################################################################################################################################\n",
    "\n",
    "# Estimator of M matrix\n",
    "approx_M = cov_dtx @ np.linalg.pinv(cov_x)\n",
    "\n",
    "# Plot figures\n",
    "plt.figure()\n",
    "ax = sns.heatmap(M, cmap=\"coolwarm\")\n",
    "ax.set_title(f\"Ground truth connectivity matrix : {dataset_name}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(approx_M, cmap=\"coolwarm\")\n",
    "ax.set_title(f\"Approximate connectivity matrix : {dataset_name}\")\n",
    "plt.show()\n",
    "\n",
    "##################################################################################################################################\n",
    "\n",
    "print(f\"total_mask: {total_mask}\\n\")\n",
    "print(f\"num_worms: {num_worms}\\n\")\n",
    "print(\"number of neurons never observed:\", (np.diag(total_mask) <= 1).sum(), end=\"\\n\\n\")\n",
    "\n",
    "print(\"cov_x symmetric:\", scp.linalg.issymmetric(cov_x))\n",
    "print(\"cov_x rank:\", np.linalg.matrix_rank(cov_x))\n",
    "print(\"cov_x determinant\", np.linalg.det(cov_x))\n",
    "print(\"~\" * 50)\n",
    "print(\"cov_dtx symmetric:\", scp.linalg.issymmetric(cov_dtx))\n",
    "print(\"cov_dtx rank:\", np.linalg.matrix_rank(cov_dtx))\n",
    "print(\"cov_dtx determinant\", np.linalg.det(cov_dtx), end=\"\\n\\n\")\n",
    "\n",
    "print(\n",
    "    f\"True M diagonal (min, max): {np.diag(M).min(), np.diag(M).max()}\\n\"\n",
    "    f\"Approximate M diagonal (min, max): {np.diag(approx_M).min(), np.diag(approx_M).max()}\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"True M eigenvalues (min, max): {np.linalg.eigvals(M).real.min(), np.linalg.eigvals(M).real.max()}\\n\"\n",
    "    f\"Approximate M eigenvalues (min, max): {np.linalg.eigvals(approx_M).real.min(), np.linalg.eigvals(approx_M).real.max()}\\n\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# ### DEBUG ###\n",
    "# # Estimate of M matrix given oracle knowledge of the input b\n",
    "# # TODO: WHY IS THIS WORSE THAN THE NON-ORACLE APPROXIMATION?!\n",
    "# oracle_M = (cov_dtx - cov_bx) @ np.linalg.pinv(cov_x)\n",
    "# plt.figure()\n",
    "# ax = sns.heatmap(oracle_M, cmap=\"coolwarm\")\n",
    "# ax.set_title(f\"Oracle M matrix : {dataset_name}\")\n",
    "# plt.show()\n",
    "# print(f\"Oracle M iagonal (min, max): {np.diag(oracle_M).min(), np.diag(oracle_M).max()}\\n\")\n",
    "# print(\n",
    "#     f\"Oracle M eigenvalues (min, max): {np.linalg.eigvals(oracle_M).real.min(), np.linalg.eigvals(oracle_M).real.max()}\\n\",\n",
    "#     end=\"\\n\\n\",\n",
    "# )\n",
    "# print(\n",
    "#     \"estimate oracle distance (M - oracle_M):\", np.linalg.norm(M - oracle_M, ord=\"fro\"), end=\"\\n\\n\"\n",
    "# )\n",
    "# ### DEBUG ###\n",
    "\n",
    "##################################################################################################################################\n",
    "\n",
    "print(\"estimate full distance (M - approx_M):\", np.linalg.norm(M - approx_M, ord=\"fro\"))\n",
    "\n",
    "print(\n",
    "    \"(distribution) chance distance:\", np.linalg.norm(M - np.random.randn(*M.shape), ord=\"fro\")\n",
    ")  # if all you knew was that the weights were ~N(0,1)\n",
    "print(\n",
    "    \"(adjacency + distribution) chance distance:\",\n",
    "    np.linalg.norm(M - A * np.random.randn(*M.shape), ord=\"fro\"),\n",
    ")  # if you additionally knew the adjacency matrix\n",
    "print(\n",
    "    \"(sign + adjacency + distribution) chance distance:\",\n",
    "    np.linalg.norm(M- A * np.sign(M) * np.abs(np.random.randn(*M.shape)), ord=\"fro\"),\n",
    "    end=\"\\n\\n\",\n",
    ")  # if you additionally knew the signs of the weights\n",
    "\n",
    "##################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways:**\n",
    "\n",
    "* Solving for the connectivity matrix is impossible without input noise, even though when we wrote the equation we ignore the noise because we don't know it!\n",
    "\n",
    "* Minimally connectedness seems to be extremely necessary to be able to estimate the connectivity matrix. Densely connected networks are near impossible to estimate. Although we don't understand mathematically why, it is likely because of the multiple pathways that influence a node's activity. \n",
    "\n",
    "* It seems more important to have more recorded neurons per worm and have fewer worms than to have have many worms with only a few recorded neurons. So the optimizing the number of neurons recorded per animal is more important than improving the throughput of animals.\n",
    "    * Update: It is still possible to estimate densely connected networks but there now needs to be noise injected at almost every node independently.\n",
    "    * So it seems that the less sparse/more dense/more connected the network is, the larger the percentage of the network that is the sensorium needs to be so that noise can get injected there.\n",
    "\n",
    "* A critical threshold of sensory neurons (i.e. nodes alowed to receive external inputs) needs to be passed ($\\approx 200$ out of the $300$ at maximum sparsity for connectedness) for the connections weights to be recoverable. It remains to be seen whether the external inputs $b$ must be Gaussian white nose or whether they can be a time-varying smooth signal.\n",
    "    * System recovery works optimially for time-independent Gaussian noise input signal.\n",
    "    * System recovery fails for high-frequency time-varying input signal, even when frequency is on the order of the number of measurement timesteps.\n",
    "    * Adding noise to a high-frequency time-varying input signal make improves system recovery.\n",
    "    * System recovery fails for a low-frequency time-varying input signal.\n",
    "    * Adding noise to a low-frequency time-varying input signal allows some system recovery.\n",
    "\n",
    "* So really all you need is away to inject a significant amount of noise into your system and you can recover the weights. The signal-to-noise ratio for the external input needs to be quite small for us to be able to estimate the connectivity weights.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
