{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from utils import DEVICE\n",
    "from models._utils import print_parameters, LinearNN, NeuralTransformer, NetworkLSTM\n",
    "from fvcore.nn import (\n",
    "    FlopCountAnalysis,\n",
    "    ActivationCountAnalysis,\n",
    "    flop_count_table,\n",
    "    flop_count_str,\n",
    ")\n",
    "\n",
    "# Display the DEVICE\n",
    "print(f\"DEVICE: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple PyTorch model: Linear(in_features=302, out_features=512, bias=True)\n",
      "\n",
      "Input: torch.Size([1, 200, 302]) \t Output: torch.Size([1, 200, 512])\n",
      "\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Custom model: LinearNN(\n",
      "  (identity): Identity()\n",
      "  (input_hidden): Sequential(\n",
      "    (0): Linear(in_features=302, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (hidden_hidden): FeedForward(\n",
      "    (ffwd): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (inner_hidden_model): InnerHiddenModel(\n",
      "    (hidden_hidden): FeedForward(\n",
      "      (ffwd): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=302, bias=True)\n",
      "  (embedding): Linear(in_features=302, out_features=512, bias=True)\n",
      ")\n",
      "\n",
      "Input: torch.Size([1, 200, 302]) \t Mask: torch.Size([1, 302]) \t Output: torch.Size([1, 200, 302])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Prepare model and input\n",
    "# @markdown Make sure the model and input are on the same device.\n",
    "\n",
    "# Set shapes for model and input\n",
    "seq_len = 200\n",
    "input_size = 302\n",
    "hidden_size = 512\n",
    "\n",
    "# Use a standard PyTorch model\n",
    "model = torch.nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # switch to eval mode\n",
    "print(f\"Simple PyTorch model: {model}\\n\")\n",
    "\n",
    "# Create input of the correct shape for the model\n",
    "input = torch.randn(1, seq_len, input_size).to(DEVICE)  # batch_size=1\n",
    "mask = None\n",
    "print(f\"Input: {input.shape} \\t Output: {model(input).shape}\", end=\"\\n\\n\")\n",
    "print(f\"\\n{'~'*100}\\n\")\n",
    "\n",
    "# Load one of our custom models instead\n",
    "model_args = dict(input_size=input_size, hidden_size=hidden_size, loss=\"MSE\")\n",
    "model = LinearNN(**model_args)\n",
    "# model = NeuralTransformer(**model_args)\n",
    "# model = NetworkLSTM(**model_args)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # switch to eval mode\n",
    "print(f\"Custom model: {model}\\n\")\n",
    "\n",
    "# Create input of the correct shape for the model\n",
    "input = torch.randn(1, seq_len, input_size).to(DEVICE)  # batch_size=1\n",
    "mask = torch.ones(1, input_size).to(bool).to(DEVICE)\n",
    "print(\n",
    "    f\"Input: {input.shape} \\t Mask: {mask.shape} \\t Output: {model(input, mask).shape}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::expand_as encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "Unsupported operator aten::expand_as encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 2 time(s)\n",
      "Unsupported operator aten::layer_norm encountered 1 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All params: 573742\n",
      "Trainable params: 573742\n",
      "\n",
      "FLOPs: (114790400, Counter({'linear': 114278400, 'layer_norm': 512000}), Counter({'': 114790400, 'hidden_hidden': 52428800, 'hidden_hidden.ffwd': 52428800, 'hidden_hidden.ffwd.0': 52428800, 'input_hidden': 31436800, 'input_hidden.0': 30924800, 'linear': 30924800, 'input_hidden.2': 512000, 'identity': 0, 'input_hidden.1': 0, 'hidden_hidden.ffwd.1': 0, 'hidden_hidden.ffwd.2': 0, 'inner_hidden_model': 0}), {'': Counter({'linear': 114278400, 'layer_norm': 512000}), 'identity': Counter(), 'input_hidden': Counter({'linear': 30924800, 'layer_norm': 512000}), 'input_hidden.0': Counter({'linear': 30924800}), 'input_hidden.1': Counter(), 'input_hidden.2': Counter({'layer_norm': 512000}), 'hidden_hidden': Counter({'linear': 52428800}), 'hidden_hidden.ffwd': Counter({'linear': 52428800}), 'hidden_hidden.ffwd.0': Counter({'linear': 52428800}), 'hidden_hidden.ffwd.1': Counter(), 'hidden_hidden.ffwd.2': Counter(), 'inner_hidden_model': Counter(), 'linear': Counter({'linear': 30924800})})\n",
      "\n",
      "N/A indicates a possibly missing statistic due to how the module was called. Missing values are still included in the parent's total.\n",
      "LinearNN(\n",
      "  #params: 0.57M, #flops: 0.11G\n",
      "  (identity): Identity(#params: 0, #flops: N/A)\n",
      "  (input_hidden): Sequential(\n",
      "    #params: 0.16M, #flops: 31.44M\n",
      "    (0): Linear(\n",
      "      in_features=302, out_features=512, bias=True\n",
      "      #params: 0.16M, #flops: 30.92M\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm(\n",
      "      (512,), eps=1e-05, elementwise_affine=True\n",
      "      #params: 1.02K, #flops: 0.51M\n",
      "    )\n",
      "  )\n",
      "  (hidden_hidden): FeedForward(\n",
      "    #params: 0.26M, #flops: 52.43M\n",
      "    (ffwd): Sequential(\n",
      "      #params: 0.26M, #flops: 52.43M\n",
      "      (0): Linear(\n",
      "        in_features=512, out_features=512, bias=True\n",
      "        #params: 0.26M, #flops: 52.43M\n",
      "      )\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (inner_hidden_model): InnerHiddenModel(\n",
      "    (hidden_hidden): FeedForward(\n",
      "      (ffwd): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(\n",
      "    in_features=512, out_features=302, bias=True\n",
      "    #params: 0.15M, #flops: 30.92M\n",
      "  )\n",
      "  (embedding): Linear(in_features=302, out_features=512, bias=True)\n",
      ")\n",
      "\n",
      "| module                        | #parameters or shape   | #flops    |\n",
      "|:------------------------------|:-----------------------|:----------|\n",
      "| model                         | 0.574M                 | 0.115G    |\n",
      "|  input_hidden                 |  0.156M                |  31.437M  |\n",
      "|   input_hidden.0              |   0.155M               |   30.925M |\n",
      "|    input_hidden.0.weight      |    (512, 302)          |           |\n",
      "|    input_hidden.0.bias        |    (512,)              |           |\n",
      "|   input_hidden.2              |   1.024K               |   0.512M  |\n",
      "|    input_hidden.2.weight      |    (512,)              |           |\n",
      "|    input_hidden.2.bias        |    (512,)              |           |\n",
      "|  hidden_hidden.ffwd.0         |  0.263M                |  52.429M  |\n",
      "|   hidden_hidden.ffwd.0.weight |   (512, 512)           |           |\n",
      "|   hidden_hidden.ffwd.0.bias   |   (512,)               |           |\n",
      "|  linear                       |  0.155M                |  30.925M  |\n",
      "|   linear.weight               |   (302, 512)           |           |\n",
      "|   linear.bias                 |   (302,)               |           |\n",
      "\n",
      "\tParams: 573742\n",
      "\n",
      "Activations: (265200, Counter({'linear': 265200}), Counter({'': 265200, 'input_hidden': 102400, 'input_hidden.0': 102400, 'hidden_hidden': 102400, 'hidden_hidden.ffwd': 102400, 'hidden_hidden.ffwd.0': 102400, 'linear': 60400, 'identity': 0, 'input_hidden.1': 0, 'input_hidden.2': 0, 'hidden_hidden.ffwd.1': 0, 'hidden_hidden.ffwd.2': 0, 'inner_hidden_model': 0}), {'': Counter({'linear': 265200}), 'identity': Counter(), 'input_hidden': Counter({'linear': 102400}), 'input_hidden.0': Counter({'linear': 102400}), 'input_hidden.1': Counter(), 'input_hidden.2': Counter(), 'hidden_hidden': Counter({'linear': 102400}), 'hidden_hidden.ffwd': Counter({'linear': 102400}), 'hidden_hidden.ffwd.0': Counter({'linear': 102400}), 'hidden_hidden.ffwd.1': Counter(), 'hidden_hidden.ffwd.2': Counter(), 'inner_hidden_model': Counter(), 'linear': Counter({'linear': 60400})})\n",
      "\n",
      "\tParams: 573742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Using fvcore\n",
    "\n",
    "# Adjust input based on if we use standard PyTorch model or custom model\n",
    "input = (input, mask) if mask is not None else input\n",
    "\n",
    "# Count the total and number of trainable parameters\n",
    "all_params_ct, train_params_ct = print_parameters(model)\n",
    "\n",
    "print(f\"\\nAll params: {all_params_ct}\\nTrainable params: {train_params_ct}\", end=\"\\n\\n\")\n",
    "\n",
    "# Perform FLOP Counting: Use the FlopCountAnalysis class to analyze your model:\n",
    "flops = FlopCountAnalysis(model, input)\n",
    "\n",
    "# Print Results: You can now print out the FLOPs and parameter information:\n",
    "print(\n",
    "    f\"FLOPs: {flops.total(), flops.by_operator(), flops.by_module(), flops.by_module_and_operator()}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(flop_count_str(flops), end=\"\\n\\n\")\n",
    "print(flop_count_table(flops), end=\"\\n\\n\")\n",
    "print(\n",
    "    f\"\\tParams: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "# Perform Activations Counting: Use the ActivationCountAnalysis class to analyze your model:\n",
    "acts = ActivationCountAnalysis(model, input)\n",
    "\n",
    "# Print Results: You can now print out the FLOPs and parameter information:\n",
    "print(\n",
    "    f\"Activations: {acts.total(), acts.by_operator(), acts.by_module(), acts.by_module_and_operator()}\",\n",
    "    end=\"\\n\\n\",\n",
    ")\n",
    "print(\n",
    "    f\"\\tParams: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\",\n",
    "    end=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
