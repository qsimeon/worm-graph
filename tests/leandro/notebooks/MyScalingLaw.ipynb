{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Scaling Law Plots\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to vary:\n",
    "1. Sequence length (time steps recorded)\n",
    "2. Number of worms (number of individuals)\n",
    "3. Number of neurons recorded (labeling neurons)\n",
    "4. Do we need labeled neurons or we can atribute randomly?\n",
    "\n",
    "Another question could be:\n",
    "1. How the optimization time evolves with the amount of data?\n",
    "2. And with the amount of used neurons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from data._main import *\n",
    "from models._main import *\n",
    "from train._main import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config_train = OmegaConf.load(\"../../../conf/train.yaml\")\n",
    "print(\"config:\", OmegaConf.to_yaml(config_train), end=\"\\n\\n\")\n",
    "\n",
    "model = get_model(OmegaConf.load(\"../../../conf/model.yaml\"))\n",
    "\n",
    "dataset = get_dataset(OmegaConf.load(\"../../../conf/dataset.yaml\"))\n",
    "\n",
    "model, log_dir, config_train = train_model(\n",
    "    config_train,\n",
    "    model,\n",
    "    dataset,\n",
    "    shuffle_worms=False,\n",
    "    log_dir=os.path.join(\"logs\", \"{}\".format(datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\"))),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.leandro.mini_connectome import *\n",
    "from tests.leandro.plots import *\n",
    "from omegaconf import OmegaConf\n",
    "from data._main import *\n",
    "from train._utils import split_train_test\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/Z3JhcGggVEQ7CiAgICBNM0wgLS0+IE5TTUwgJiBOU01SOwogICAgTlNNTCAtLT4gTTNMOwogICAgY2xhc3NEZWYgSSBmaWxsOiAjRkYxRjVCLCBzdHJva2U6IzAwMDAwMCwgc3Ryb2tlLXdpZHRoOjFweDsKICAgIGNsYXNzRGVmIE0gZmlsbDogIzAwQ0Q2Qywgc3Ryb2tlOiMwMDAwMDAsIHN0cm9rZS13aWR0aDoxcHg7CiAgICBjbGFzc0RlZiBTIGZpbGw6ICMwMDlBREUsIHN0cm9rZTojMDAwMDAwLCBzdHJva2Utd2lkdGg6MXB4OwogICAgY2xhc3NEZWYgUCBmaWxsOiAjQUY1OEJBLCBzdHJva2U6IzAwMDAwMCwgc3Ryb2tlLXdpZHRoOjFweDsKICAgIGNsYXNzRGVmIE1JIGZpbGw6ICNGRkM2MUUsIHN0cm9rZTojMDAwMDAwLCBzdHJva2Utd2lkdGg6MXB4OwogICAgY2xhc3NEZWYgU00gZmlsbDogI0YyODUyMiwgc3Ryb2tlOiMwMDAwMDAsIHN0cm9rZS13aWR0aDoxcHg7CiAgICBjbGFzc0RlZiBTSSBmaWxsOiAjQTBCMUJBLCBzdHJva2U6IzAwMDAwMCwgc3Ryb2tlLXdpZHRoOjFweDsKICAgIGNsYXNzRGVmIFNNSSBmaWxsOiAjQTY3NjFELCBzdHJva2U6IzAwMDAwMCwgc3Ryb2tlLXdpZHRoOjFweDsKICAgIGNsYXNzRGVmIFUgZmlsbDogI0U5MDAyRCwgc3Ryb2tlOiMwMDAwMDAsIHN0cm9rZS13aWR0aDoxcHg7CiAgICBjbGFzcyBNM0wgUDsKICAgIGNsYXNzIE5TTVIgUDsKICAgIGNsYXNzIE5TTUwgUDsK\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "miniconnectome = MiniConnectome(direction='TD', group_by='four')\n",
    "connected_to_M3L = miniconnectome.get_connected_neurons('M3L', index=False)\n",
    "miniconnectome.minigraph.add_nodes('M3L', connected_to_M3L)\n",
    "miniconnectome.minigraph.add_nodes(connected_to_M3L[0], miniconnectome.get_connected_neurons(connected_to_M3L[0], index=False)[2:3])\n",
    "\n",
    "miniconnectome.minigraph.display(save=False, filename='M3L.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataloader\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen dataset(s): ['Flavell2023']\n",
      "Num. worms: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dict dataset\n",
    "config = OmegaConf.load(\"../../../conf/dataset.yaml\")\n",
    "dict_dataset = get_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one worm relevant data for training\n",
    "\n",
    "def get_calcium_data(worm_data, neuron_names=None, use_residual=False):\n",
    "\n",
    "    if use_residual:\n",
    "        calcium_data = worm_data['smooth_residual_calcium']\n",
    "    else:\n",
    "        calcium_data = worm_data['smooth_calcium_data']\n",
    "\n",
    "    time_vector = worm_data['time_in_seconds']\n",
    "\n",
    "    # Verify if we have the neurons in the dataset\n",
    "    if neuron_names is not None:\n",
    "        for neuron in neuron_names:\n",
    "            assert neuron in worm_data['named_neuron_to_slot'], f\"We don't have data of neuron {neuron} for this worm\"\n",
    "        source_neurons_idx = [worm_data['named_neuron_to_slot'][neuron] for neuron in neuron_names] # Subset of labeled neurons\n",
    "        return time_vector, calcium_data[:, source_neurons_idx]\n",
    "    \n",
    "    else:\n",
    "        return time_vector, calcium_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vec, calcium_data = get_calcium_data(dict_dataset['worm0'], neuron_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort 0: 50 worms\n",
      "Train dataset: 5024 samples\n",
      "Test dataset: 5024 samples\n",
      "Cohort 1: 50 worms\n",
      "Train dataset: 5024 samples\n",
      "Test dataset: 5024 samples\n"
     ]
    }
   ],
   "source": [
    "# === Train and test loaders ===\n",
    "\n",
    "# One cohort = one epoch\n",
    "# In one cohort we have X worms\n",
    "# We can extract Y time series samples from each worm\n",
    "# In each epoch we have sum_i=1^X Y_i samples\n",
    "\n",
    "number_cohorts = 2\n",
    "\n",
    "# This is a way of keeping track of the number of worms that we need.\n",
    "\n",
    "# TODO: In order to chose the number of worms in a cohort, we can select the desired ones in the dict_dataset\n",
    "\n",
    "\n",
    "# === Parameters ===\n",
    "use_residual = False\n",
    "k_splits = 2 # Number of chunks to split the data: 1 chunk = 1 train/test split. Order: train, test, train, test, ...\n",
    "seq_len = 100 # Number of time steps to extract from each chunk (time steps of each example)\n",
    "num_samples = 100 # Total number of sample pairs (input,target) to extract from each worm\n",
    "reverse = False # If True, the time series is reversed\n",
    "tau = 10 # Number of time steps to shift the target (number of time steps we want to predict ahead)\n",
    "\n",
    "batch_size = 32\n",
    "shuffle_samples = True\n",
    "\n",
    "desired_neurons_to_train_with = ['M3L']\n",
    "\n",
    "shuffle_worms = True\n",
    "\n",
    "cohorts = sorted(dict_dataset.items()) * number_cohorts\n",
    "\n",
    "assert (len(cohorts) == number_cohorts * len(dict_dataset)), \"Invalid number of worms.\"\n",
    "\n",
    "if shuffle_worms == True:\n",
    "\tcohorts = random.sample(cohorts, k=len(cohorts))\n",
    "\n",
    "# Split one cohort per epoch\n",
    "cohorts = np.array(np.array_split(cohorts, number_cohorts)) # Shape: (number_cohorts, number_worms, 2 - wormID and wormData)\n",
    "\n",
    "# === Creating the datasets ===\n",
    "\n",
    "# Memoize creation of data loaders and masks for speedup\n",
    "memo = {}\n",
    "\n",
    "cohort_trainloaders = np.empty(cohorts.shape[0], dtype=object)\n",
    "cohort_testloaders = np.empty(cohorts.shape[0], dtype=object)\n",
    "\n",
    "for cohort_idx, cohort in enumerate(cohorts):\n",
    "\t# Train and test datasets for each worm\n",
    "\ttrain_datasets = np.empty(cohort.shape[0], dtype=object)\n",
    "\tif cohort_idx == 0:  # Keep the validation dataset the same\n",
    "\t\ttest_datasets = np.empty(cohort.shape[0], dtype=object)\n",
    "\t# Used neurons masks for each worm\n",
    "\tneuron_masks = np.empty(cohort.shape[0], dtype=object)\n",
    "\t\n",
    "\t# Iterate over worms\n",
    "\tfor worm_idx, (worm_id, worm_data) in enumerate(cohort):\n",
    "\t\t# If we have already loaded the worm...\n",
    "\t\tif worm_id in memo:\n",
    "\t\t\ttrain_datasets[worm_idx] = memo[worm_id][\"train_dataset\"] # Recover the train dataset of this worm\n",
    "\t\t\tif cohort_idx == 0:\n",
    "\t\t\t\ttest_datasets[worm_idx] = memo[worm_id][\"test_dataset\"] # Recover the test dataset of this worm\n",
    "\t\t\n",
    "\t\t# If we have not loaded the worm...\n",
    "\t\telse:\n",
    "\n",
    "\t\t\ttime_vec, calcium_data = get_calcium_data(worm_data, neuron_names=desired_neurons_to_train_with)\n",
    "\n",
    "\t\t\ttrain_datasets[worm_idx], test_dataset_tmp, _, _ = split_train_test(\n",
    "\t\t\t\tdata = calcium_data,\n",
    "\t\t\t\tk_splits = k_splits,\n",
    "\t\t\t\tseq_len = seq_len,\n",
    "\t\t\t\tnum_samples = num_samples,\n",
    "\t\t\t\ttime_vec = time_vec,\n",
    "\t\t\t\treverse = reverse,\n",
    "\t\t\t\ttau = tau,\n",
    "\t\t\t\tuse_residual = use_residual,\n",
    "\t\t\t)\n",
    "\t\t\tif cohort_idx == 0:  # Keep the validation dataset the same\n",
    "\t\t\t\t\ttest_datasets[worm_idx] = test_dataset_tmp\n",
    "\n",
    "\t\t\t# Add to memo\n",
    "\t\t\tmemo[worm_id] = dict(\n",
    "\t\t\t\ttrain_dataset=train_datasets[worm_idx],\n",
    "\t\t\t\ttest_dataset=test_dataset_tmp,\n",
    "\t\t\t)\n",
    "\n",
    "\tcohort_train_dataset = ConcatDataset(list(train_datasets))\n",
    "\tcohort_test_dataset = ConcatDataset(list(test_datasets))\n",
    "\n",
    "\tcohort_trainloaders[cohort_idx] = DataLoader(\n",
    "            cohort_train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_samples,\n",
    "            pin_memory=True,\n",
    "            num_workers=0,\n",
    "        )  # returns (X, Y, Dict) when iterating over it\n",
    "\t\n",
    "\tcohort_testloaders[cohort_idx] = DataLoader(\n",
    "            cohort_test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_samples,\n",
    "            pin_memory=True,\n",
    "            num_workers=0,\n",
    "        )  # returns (X, Y, Dict) when iterating over it\n",
    "\t\n",
    "# Print number of examples in each cohort\n",
    "for cohort_idx, cohort in enumerate(cohorts):\n",
    "    print(f\"Cohort {cohort_idx}: {len(cohort)} worms\")\n",
    "    print(f\"Train dataset: {len(cohort_trainloaders[cohort_idx])*batch_size} samples\")\n",
    "    print(f\"Test dataset: {len(cohort_testloaders[cohort_idx])*batch_size} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(cohort_trainloaders[0]))[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worm-graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
