{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import *\n",
    "from omegaconf import OmegaConf\n",
    "from pprint import PrettyPrinter\n",
    "from models._main import get_model\n",
    "from models._utils import print_parameters\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "pp = PrettyPrinter(indent=4, width=100, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color and Marker code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Returns the legend code that is used across all plots.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_code = legend_code()\n",
    "pp.pprint(leg_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = OmegaConf.load(\"../../configs/submodule/model.yaml\")\n",
    "\n",
    "model_config.model.use_this_pretrained_model = None\n",
    "model = get_model(model_config.model)\n",
    "print_parameters(model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot information from datasets. I'm loading information from datasets that used all available worms and neurons in the analysis (in this case, the data scaling experiment datasets, that used the maximum amount of data)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1_path_dict = {\n",
    "    \"train_dataset_info\": \"/om2/user/qsimeon/worm-graph/data/custom_train/train_dataset_info.csv\",  # Path to train dataset info => extract number of train time steps\n",
    "    \"val_dataset_info\": \"/om2/user/qsimeon/worm-graph/data/custom_validation/val_dataset_info.csv\",  # Path to val. dataset info => extract number of val. time steps\n",
    "    \"combined_dataset_info\": \"/om2/user/qsimeon/worm-graph/data/custom_combined/combined_dataset_info.csv\",  # Path to combined dataset info => extract total number of worms and time step interval\n",
    "}\n",
    "\n",
    "# NOTE: Error bars on bar plots show +/- 2 SEM\n",
    "dataset_info = dataset_information(path_dict=fig1_path_dict, legend_code=leg_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data scaling plot. To generate the dataframe with the relevant results, firstly for each model you want to plot, you need to furnish the paths for each trial. Then you can call the plot function to obtain a graphic with mean and standard deviation bars. You need to provide the legend code for color standardization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do experiments, for each model architecture, by varying the sizes of subsets of the combined datasets.\n",
    "# Each subset will have a distribution of timesteps per neuron (i.e. each worm has a timestep per neuron metric).\n",
    "# For each subset, repeat the train pipeline n times (here you did n=3) and save the validation loss each time.\n",
    "nts_experiments = {\n",
    "    \"LSTM\": [\n",
    "        os.path.join(\"results\", \"NTS_LSTM\"),  # LSTM trial 1\n",
    "        os.path.join(\"results\", \"NTS_LSTM_1\"),  # LSTM trial 2\n",
    "        os.path.join(\"results\", \"NTS_LSTM_2\"),  # LSTM trial 3\n",
    "    ],\n",
    "    \"Transformer\": [\n",
    "        os.path.join(\"results\", \"NTS_TRANSFORMER\"),  # Transformer trial 1\n",
    "        os.path.join(\"results\", \"NTS_TRANSFORMER_1\"),  # Transformer trial 2\n",
    "        os.path.join(\"results\", \"NTS_TRANSFORMER_2\"),  # Transformer trial 3\n",
    "    ],\n",
    "    \"Linear\": [\n",
    "        os.path.join(\"results\", \"NTS_LINEAR\"),  # Linear trial 1\n",
    "        os.path.join(\"results\", \"NTS_LINEAR_1\"),  # Linear trial 2\n",
    "        os.path.join(\"results\", \"NTS_LINEAR_2\"),  # Linear trial 3\n",
    "    ],\n",
    "}\n",
    "\n",
    "data_results = data_scaling_df(\n",
    "    nts_experiments\n",
    ")  # Retrieve the relevant results for the data scaling (and hidden scaling) plot\n",
    "data_results.head(2)  # Took ~1s in my computer (small experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaling_plot(data_results, legend_code=leg_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hidden dimension scaling plot. To generate the dataframe with the relevant results, firstly for each model you want to plot, you need to furnish the paths for each trial (it can take some minutes). Then you can call the plot function to obtain a graphic with mean and standard deviation bars. You can chose the degree of the polynomial fit used. You need to provide the legend code for color standardization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of plotting against the hidden size, plot against the number of parameters.\n",
    "# The hidden size is still the variable that we will vary, but it is simply a way to change the number of parameters.\n",
    "\n",
    "hidden_experiments = {\n",
    "    \"LSTM\": [\n",
    "        os.path.join(\"results\", \"HIDDEN_LSTM_1\"),  # LSTM trial 1\n",
    "        os.path.join(\"results\", \"HIDDEN_LSTM_2\"),  # LSTM trial 2\n",
    "        os.path.join(\"results\", \"HIDDEN_LSTM_3\"),  # LSTM trial 3\n",
    "    ],\n",
    "    \"Transformer\": [\n",
    "        os.path.join(\"results\", \"HIDDEN_TRANSFORMER_1\"),  # Transformer trial 1\n",
    "        os.path.join(\"results\", \"HIDDEN_TRANSFORMER_2\"),  # Transformer trial 2\n",
    "        os.path.join(\"results\", \"HIDDEN_TRANSFORMER_3\"),  # Transformer trial 3\n",
    "    ],\n",
    "    \"Linear\": [\n",
    "        os.path.join(\"results\", \"HIDDEN_LINEAR_1\"),  # Linear trial 1\n",
    "        os.path.join(\"results\", \"HIDDEN_LINEAR_2\"),  # Linear trial 2\n",
    "        os.path.join(\"results\", \"HIDDEN_LINEAR_3\"),  # Linear trial 3\n",
    "    ],\n",
    "}\n",
    "\n",
    "hidden_results = data_scaling_df(\n",
    "    hidden_experiments\n",
    ")  # Retrieve the relevant results for the hidden scaling (and data scaling) plot\n",
    "hidden_results.head(2)  # Took ~4min in my computer (big experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_scaling_plot(hidden_results, legend_code=leg_code, fit_deg=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scaling slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data scaling slopes plot. To generate the dataframe with the relevant results, firstly for each model you want to plot, you need to furnish the paths for each trial (it can take some minutes). Then you can call the plot function to obtain a graphic with mean and standard deviation bars. You need to provide the legend code for color standardization.*\n",
    "\n",
    "*To create this plot, after a model has been trained (with a given amount of data), we validate it on individual experimental datasets. We then make a plot for each model by experimental dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nts_experiments = {\n",
    "    \"LSTM\": [\n",
    "        os.path.join(\"results\", \"NTS_LSTM\"),  # LSTM trial 1\n",
    "        os.path.join(\"results\", \"NTS_LSTM_1\"),  # LSTM trial 2\n",
    "        os.path.join(\"results\", \"NTS_LSTM_2\"),  # LSTM trial 3\n",
    "    ],\n",
    "    \"Transformer\": [\n",
    "        os.path.join(\"results\", \"NTS_TRANSFORMER\"),  # Transformer trial 1\n",
    "        os.path.join(\"results\", \"NTS_TRANSFORMER_1\"),  # Transformer trial 2\n",
    "        os.path.join(\"results\", \"NTS_TRANSFORMER_2\"),  # Transformer trial 3\n",
    "    ],\n",
    "    \"Linear\": [\n",
    "        os.path.join(\"results\", \"NTS_LINEAR\"),  # Linear trial 1\n",
    "        os.path.join(\"results\", \"NTS_LINEAR_1\"),  # Linear trial 2\n",
    "        os.path.join(\"results\", \"NTS_LINEAR_2\"),  # Linear trial 3\n",
    "    ],\n",
    "}\n",
    "\n",
    "scaling_slope_results = scaling_slopes_df(nts_experiments)  # Took ~10s\n",
    "scaling_slope_results.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_slopes_plot(scaling_slope_results, legend_code=leg_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-dataset Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This function automatically loads the results from the indicated experiments and displays a heatmap plot showing the validation loss on individual experimental datasets after a model has been trained (from scratch) using a single experimental dataset.*\n",
    "\n",
    "*You need to specify the model names respective to each experiment and also the legend color code.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_log_folders = [\n",
    "    os.path.join(\"results\", \"CDS_LSTM\"),  # LSTM trial 1\n",
    "    os.path.join(\"results\", \"CDS_TRANSFORMER\"),  # Transformer trial 1\n",
    "    os.path.join(\"results\", \"CDS_LINEAR\"),  # Linear trial 1\n",
    "]\n",
    "\n",
    "model_names = [\"LSTM\", \"Transformer\", \"Linear\"]\n",
    "\n",
    "cross_dataset(\n",
    "    experiment_log_folders=experiment_log_folders,\n",
    "    model_names=model_names,\n",
    "    legend_code=leg_code,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load and plot the predictions for the same neuron across different datasets. You need to determine which neurons are common to all datasets (I did it manually).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_log_folders = [\n",
    "    os.path.join(\"results\", \"NTS_LSTM_2\"),  # LSTM trial 1\n",
    "    os.path.join(\"results\", \"NTS_TRANSFORMER_2\"),  # Transformer trial 1\n",
    "    os.path.join(\"results\", \"NTS_LINEAR_2\"),  # Linear trial 1\n",
    "]\n",
    "\n",
    "model_names = [\"LSTM\", \"Transformer\", \"Linear\"]\n",
    "\n",
    "ds_type = \"val\"  # we are plotting the predictions using the validation dataset (data never seen during training, second split)\n",
    "exp = \"exp5\"  # model trained with maximum amount of data (best model to use for predictions)\n",
    "neuron_to_plot = \"AVER\"  # neuron we want to plot\n",
    "\n",
    "teacher_forcing(\n",
    "    experiment_log_folders=experiment_log_folders,\n",
    "    model_names=model_names,\n",
    "    legend_code=leg_code,\n",
    "    ds_type=ds_type,\n",
    "    exp=exp,\n",
    "    neuron_to_plot=neuron_to_plot,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoregressive(\n",
    "    experiment_log_folders=experiment_log_folders,\n",
    "    model_names=model_names,\n",
    "    legend_code=leg_code,\n",
    "    ds_type=ds_type,\n",
    "    exp=exp,\n",
    "    neuron_to_plot=neuron_to_plot,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Sup. fig) Prediction gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Loads the relevant results and plots the refinement of the prediction plot as the amount of training data for the LSTM model increases.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gap_df = prediction_gap(\n",
    "    exp_nts_log_dir=os.path.join(\"results\", \"NTS_LSTM_1\"),\n",
    "    legend_code=leg_code,\n",
    "    neuronID=\"AVER\",\n",
    "    wormID=\"worm1\",\n",
    "    datasetID=\"Flavell2023\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worm-graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
