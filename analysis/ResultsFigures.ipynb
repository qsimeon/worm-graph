{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from scipy import stats\n",
    "from pprint import PrettyPrinter\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from visualize._utils import experiment_parameter\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "pp = PrettyPrinter(indent=4, width=100, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color and Marker Legend\n",
    "Returns the common legend code used for all plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Marker and Dataset color codes\n",
    "def legend_code():\n",
    "    markers = {\n",
    "        \"<\": \"Naive\",\n",
    "        \">\": \"Linear\",\n",
    "        \"p\": \"Feedforward\",\n",
    "        \"s\": \"Transformer\",\n",
    "        \"P\": \"Attention\",\n",
    "        \"o\": \"LSTM\",\n",
    "        \"*\": \"CTRNN\",\n",
    "        \"X\": \"Liquid\",\n",
    "    }\n",
    "\n",
    "    model_labels = {\n",
    "        \"NaivePredictor\": \"Naive\",\n",
    "        \"LinearRegression\": \"Linear\",\n",
    "        \"FeatureFFNN\": \"Feedforward\",\n",
    "        \"NeuralTransformer\": \"Transformer\",\n",
    "        \"PureAttention\": \"Attention\",\n",
    "        \"NetworkLSTM\": \"LSTM\",\n",
    "        \"NetworkCTRNN\": \"CTRNN\",\n",
    "        \"LiquidCfC\": \"Liquid\",\n",
    "    }\n",
    "\n",
    "    dataset_labels = {\n",
    "        \"Kato2015\": \"Kato (2015)\",\n",
    "        \"Nichols2017\": \"Nichols (2017)\",\n",
    "        \"Skora2018\": \"Skora (2018)\",\n",
    "        \"Kaplan2020\": \"Kaplan (2020)\",\n",
    "        \"Yemini2021\": \"Yemini (2021)\",\n",
    "        \"Uzel2022\": \"Uzel (2022)\",\n",
    "        \"Flavell2023\": \"Flavell (2023)\",\n",
    "        \"Leifer2023\": \"Leifer (2023)\",\n",
    "    }\n",
    "\n",
    "    marker_colors = sns.color_palette(\"tab10\", n_colors=len(markers))\n",
    "\n",
    "    # Create custom markers for models\n",
    "    marker_legend = [\n",
    "        Line2D([0], [0], marker=m, color=marker_colors[i], label=l, linestyle=\"None\")\n",
    "        for i, (m, l) in enumerate(markers.items())\n",
    "    ]\n",
    "\n",
    "    # Plot the marker legends\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(4, 1))\n",
    "\n",
    "    # Plot marker legend on the left subplot\n",
    "    axs[0].legend(handles=marker_legend, loc=\"center\", title=\"Model\")\n",
    "    # Legend title italic\n",
    "    axs[0].get_legend().get_title().set_fontsize(\"large\")\n",
    "    axs[0].get_legend().get_title().set_fontstyle(\"italic\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    color_palette = sns.color_palette(\"tab10\", n_colors=len(dataset_labels))\n",
    "    # Add black color to the end of color palette\n",
    "    color_palette.append((0, 0, 0))\n",
    "\n",
    "    # Create rectangular color patches for datasets\n",
    "    color_legend = [\n",
    "        Patch(facecolor=c, edgecolor=c, label=l) for c, l in zip(color_palette, dataset_labels)\n",
    "    ]\n",
    "\n",
    "    # Plot color legend on the right subplot\n",
    "    axs[1].legend(handles=color_legend, loc=\"center\", title=\"Experimental datasets\")\n",
    "    axs[1].get_legend().get_title().set_fontsize(\"large\")\n",
    "    axs[1].get_legend().get_title().set_fontstyle(\"italic\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    dataset_color_code = {\n",
    "        dataset: color for dataset, color in zip(dataset_labels.keys(), color_palette)\n",
    "    }\n",
    "    model_marker_code = {model: marker for model, marker in zip(markers.values(), markers.keys())}\n",
    "    model_color_code = {model: color for model, color in zip(markers.values(), marker_colors)}\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    leg_code = {\n",
    "        \"dataset_color_code\": dataset_color_code,\n",
    "        \"model_marker_code\": model_marker_code,\n",
    "        \"model_color_code\": model_color_code,\n",
    "        \"color_legend\": color_legend,\n",
    "        \"dataset_labels\": dataset_labels,\n",
    "        \"marker_colors\": marker_colors,\n",
    "        \"marker_legend\": marker_legend,\n",
    "        \"model_labels\": model_labels,\n",
    "    }\n",
    "\n",
    "    return leg_code\n",
    "\n",
    "\n",
    "# Usage example\n",
    "leg_code = legend_code()\n",
    "pp.pprint(leg_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cudf.pandas.profile\n",
    "\n",
    "\n",
    "# 2. Plotting summary information about the datasets\n",
    "def dataset_information(path_dict, legend_code):\n",
    "    \"\"\"\n",
    "    path_dict: dictionary with the path to train_dataset_info,\n",
    "        validation_dataset_info and combined_dataset_info.\n",
    "    \"\"\"\n",
    "\n",
    "    # ### LOAD IN DATASET INFORMATION ###\n",
    "    train_dataset_info = pd.read_csv(\n",
    "        path_dict[\"train_dataset_info\"],\n",
    "        converters={\"neurons\": ast.literal_eval},\n",
    "    )\n",
    "    val_dataset_info = pd.read_csv(\n",
    "        path_dict[\"val_dataset_info\"],\n",
    "        converters={\"neurons\": ast.literal_eval},\n",
    "    )\n",
    "    combined_dataset_info = pd.read_csv(\n",
    "        path_dict[\"combined_dataset_info\"],\n",
    "        converters={\"neurons\": ast.literal_eval},\n",
    "    )\n",
    "\n",
    "    train_dataset_info[\"total_time_steps\"] = (\n",
    "        train_dataset_info[\"train_time_steps\"] + val_dataset_info[\"val_time_steps\"]\n",
    "    )\n",
    "    train_dataset_info[\"time_steps_per_neuron\"] = (\n",
    "        train_dataset_info[\"total_time_steps\"] / train_dataset_info[\"num_neurons\"]\n",
    "    )\n",
    "    amount_of_data_distribution = (\n",
    "        train_dataset_info[[\"dataset\", \"total_time_steps\"]]\n",
    "        .groupby(\"dataset\")\n",
    "        .sum()\n",
    "        .sort_values(by=\"total_time_steps\", ascending=False)\n",
    "    )\n",
    "    amount_of_data_distribution[\"percentage\"] = (\n",
    "        amount_of_data_distribution[\"total_time_steps\"]\n",
    "        / amount_of_data_distribution[\"total_time_steps\"].sum()\n",
    "    )\n",
    "\n",
    "    # ########### SET UP FOR FIGURES ###########\n",
    "    # Get color code and legend from legend_code\n",
    "    dataset_color_code = legend_code[\"dataset_color_code\"]\n",
    "    dataset_labels = legend_code[\"dataset_labels\"]\n",
    "    color_legend = legend_code[\"color_legend\"]\n",
    "\n",
    "    # Initialize figure\n",
    "    fig = plt.figure(figsize=(20, 7))\n",
    "    gs = gridspec.GridSpec(2, 4, height_ratios=[1, 1], width_ratios=[1, 1, 1, 0.25])\n",
    "\n",
    "    # Assigning the subplots to positions in the grid\n",
    "    ax1 = plt.subplot(gs[0, 0])  # Top left, 'Number of worms analyzed' pie chart\n",
    "    ax2 = plt.subplot(gs[0, 1])  # Top right, 'Number of neurons per worm' bar plot\n",
    "    ax3 = plt.subplot(\n",
    "        gs[1, 0]\n",
    "    )  # Bottom left, 'Total duration of recorded neural activity' pie chart\n",
    "    ax4 = plt.subplot(\n",
    "        gs[1, 1]\n",
    "    )  # Bottom middle, 'Duration of recorded neural activity per worm' bar plot\n",
    "    ax5 = plt.subplot(gs[0, 3])  # Bottom right, legend\n",
    "    ax6 = plt.subplot(gs[0, 2])\n",
    "    ax7 = plt.subplot(gs[1, 2:4])\n",
    "\n",
    "    # ########### FOR WORMS PIE CHART ###############\n",
    "    num_worms_per_dataset = combined_dataset_info[[\"dataset\", \"original_index\"]]\n",
    "    # Count the unique 'original_index' for each 'dataset'\n",
    "    num_worms_per_dataset = (\n",
    "        num_worms_per_dataset.groupby(\"dataset\")[\"original_index\"]\n",
    "        .nunique()\n",
    "        .reset_index(name=\"num_worms\")\n",
    "    )\n",
    "    # Calculate the percentage for each dataset\n",
    "    num_worms_per_dataset[\"percentage\"] = (\n",
    "        num_worms_per_dataset[\"num_worms\"] / num_worms_per_dataset[\"num_worms\"].sum()\n",
    "    )\n",
    "    # Sort the values by percentage in descending order\n",
    "    num_worms_per_dataset = num_worms_per_dataset.sort_values(by=\"percentage\", ascending=False)\n",
    "    worm_count_label = num_worms_per_dataset[\"num_worms\"][:7].tolist() + [\"\"]\n",
    "    # Plotting the worms per dataset pie chart\n",
    "    ax1.pie(\n",
    "        num_worms_per_dataset[\"num_worms\"],\n",
    "        labels=[f\"{percentage:.1%}\" for percentage in num_worms_per_dataset[\"percentage\"]],\n",
    "        labeldistance=1.075,\n",
    "        startangle=45,\n",
    "        colors=[dataset_color_code[dataset] for dataset in num_worms_per_dataset[\"dataset\"]],\n",
    "    )\n",
    "    ax1.pie(\n",
    "        num_worms_per_dataset[\"num_worms\"],\n",
    "        labels=[f\"{n}\" for n in worm_count_label],\n",
    "        labeldistance=0.70,\n",
    "        startangle=45,\n",
    "        colors=[dataset_color_code[dataset] for dataset in num_worms_per_dataset[\"dataset\"]],\n",
    "    )\n",
    "    ax1.set_title(\"(A) Number of worms in dataset\", fontsize=14)\n",
    "\n",
    "    # ########### NEURON POPULATION DISTRIBUTION BAR PLOT ###############\n",
    "    ax2 = plt.subplot(gs[0, 1])  # Subplot for 'Number of neurons per worm' bar plot\n",
    "    # Compute data for the neuron population distribution bar plot\n",
    "    neuron_pop_stats = (\n",
    "        train_dataset_info.groupby(\"dataset\")[\"num_neurons\"]\n",
    "        .agg([\"mean\", \"sem\"])\n",
    "        .reset_index()\n",
    "        .sort_values(by=\"mean\", ascending=False)\n",
    "    )\n",
    "    neuron_pop_colors = neuron_pop_stats[\"dataset\"].apply(\n",
    "        lambda x: dataset_color_code.get(x, \"grey\")\n",
    "    )\n",
    "    ax2.bar(\n",
    "        neuron_pop_stats[\"dataset\"],\n",
    "        neuron_pop_stats[\"mean\"],\n",
    "        yerr=2 * neuron_pop_stats[\"sem\"],\n",
    "        color=neuron_pop_colors,\n",
    "        capsize=5,\n",
    "    )\n",
    "    # Add a dashed horizontal line at y=302\n",
    "    ax2.axhline(y=302, color=\"black\", linestyle=\"dashed\", linewidth=1, alpha=0.5)\n",
    "    # Annotate the line\n",
    "    # Use the right edge of the subplot for text annotation to prevent overflow\n",
    "    right_edge = ax2.get_xlim()[1]\n",
    "    ax2.text(\n",
    "        right_edge,  # x position at the right edge\n",
    "        302,  # y position at the line\n",
    "        \"Number of neurons in C. elegans hermaphrodite\",\n",
    "        horizontalalignment=\"right\",  # Align text to the right\n",
    "        fontsize=10,\n",
    "        fontstyle=\"italic\",\n",
    "    )\n",
    "    ax2.set_title(\"(B) Number of recorded neurons per worm\", fontsize=14)\n",
    "    ax2.set_ylabel(\"Neuron population size\")\n",
    "    ax2.set_xticklabels(neuron_pop_stats[\"dataset\"], rotation=45, ha=\"right\")\n",
    "    ax2.set_xticks([])  # Delete xticks\n",
    "\n",
    "    # ########### TOTAL DURATION OF RECORDED NEURAL ACTIVITY PIE CHART ###############\n",
    "    ax3 = plt.subplot(\n",
    "        gs[1, 0]\n",
    "    )  # Subplot for 'Total duration of recorded neural activity' pie chart\n",
    "    # Compute data for total duration pie chart\n",
    "    total_duration_stats = (\n",
    "        train_dataset_info.groupby(\"dataset\")[\"total_time_steps\"]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .sort_values(by=\"total_time_steps\", ascending=False)\n",
    "    )\n",
    "    total_duration_stats[\"percentage\"] = (\n",
    "        total_duration_stats[\"total_time_steps\"] / total_duration_stats[\"total_time_steps\"].sum()\n",
    "    )\n",
    "\n",
    "    # Determine the smallest slice\n",
    "    smallest_slice_index = total_duration_stats[\"percentage\"].idxmin()\n",
    "\n",
    "    # Generate labels, omitting the smallest slice\n",
    "    labels = [\n",
    "        f\"{percentage:.1%}\" if i != smallest_slice_index else \"\"\n",
    "        for i, percentage in enumerate(total_duration_stats[\"percentage\"])\n",
    "    ]\n",
    "\n",
    "    total_duration_colors = total_duration_stats[\"dataset\"].apply(\n",
    "        lambda x: dataset_color_code.get(x, \"grey\")\n",
    "    )\n",
    "    # Plotting the total duration pie chart\n",
    "    ax3.pie(\n",
    "        total_duration_stats[\"total_time_steps\"],\n",
    "        labels=labels,\n",
    "        labeldistance=1.075,\n",
    "        startangle=90,\n",
    "        colors=total_duration_colors,\n",
    "    )\n",
    "    ax3.set_title(\"(C) Total duration of recorded neural activity\", fontsize=14)\n",
    "\n",
    "    # ########### DURATION OF RECORDED NEURAL ACTIVITY PER WORM BAR PLOT ###############\n",
    "    ax4 = plt.subplot(\n",
    "        gs[1, 1]\n",
    "    )  # Subplot for 'Duration of recorded neural activity per worm' bar plot\n",
    "    # Compute data for recording duration bar plot\n",
    "    recording_duration_stats = (\n",
    "        train_dataset_info.groupby(\"dataset\")[\"total_time_steps\"]\n",
    "        .agg([\"mean\", \"sem\"])\n",
    "        .reset_index()\n",
    "        .sort_values(by=\"mean\", ascending=False)\n",
    "    )\n",
    "    recording_duration_colors = recording_duration_stats[\"dataset\"].apply(\n",
    "        lambda x: dataset_color_code.get(x, \"grey\")\n",
    "    )\n",
    "    ax4.bar(\n",
    "        recording_duration_stats[\"dataset\"],\n",
    "        recording_duration_stats[\"mean\"],\n",
    "        yerr=2 * recording_duration_stats[\"sem\"],\n",
    "        color=recording_duration_colors,\n",
    "        capsize=5,\n",
    "    )\n",
    "    # Add a dashed horizontal line at y=3600\n",
    "    ax4.axhline(y=3600, color=\"black\", linestyle=\"dashed\", linewidth=1, alpha=0.5)\n",
    "    # Annotate the line\n",
    "    # Use the right edge of the subplot for text annotation to prevent overflow\n",
    "    right_edge = ax4.get_xlim()[1]\n",
    "    ax4.text(\n",
    "        right_edge,  # x position at the right edge\n",
    "        3600,  # y position at the line\n",
    "        \"3600 seconds = 1 hour of calcium imaging\",\n",
    "        horizontalalignment=\"right\",  # Align text to the right\n",
    "        fontsize=10,\n",
    "        fontstyle=\"italic\",\n",
    "    )\n",
    "    ax4.set_title(\"(D) Duration of recorded neural activity per worm\", fontsize=14)\n",
    "    ax4.set_ylabel(\"Time (s)\")\n",
    "    ax4.set_xticklabels(recording_duration_stats[\"dataset\"], rotation=45, ha=\"right\")\n",
    "    ax4.set_xticks([])  # Delete xticks\n",
    "\n",
    "    # ########### TIME STEPS PER NEURON BAR PLOT ###############\n",
    "    ax6 = plt.subplot(gs[0, 2])  # Subplot for 'Number of time steps per recorded neuron' bar plot\n",
    "    # Compute data for time steps per neuron bar plot\n",
    "    tsn_stats = (\n",
    "        train_dataset_info.groupby(\"dataset\")[\"time_steps_per_neuron\"]\n",
    "        .agg([\"mean\", \"sem\"])\n",
    "        .reset_index()\n",
    "        .sort_values(by=\"mean\", ascending=False)\n",
    "    )\n",
    "    tsn_colors = tsn_stats[\"dataset\"].apply(lambda x: dataset_color_code.get(x, \"grey\"))\n",
    "    ax6.bar(\n",
    "        tsn_stats[\"dataset\"],\n",
    "        tsn_stats[\"mean\"],\n",
    "        yerr=2 * tsn_stats[\"sem\"],\n",
    "        color=tsn_colors,\n",
    "        capsize=5,\n",
    "    )\n",
    "    # Add a dashed horizontal line at y=100\n",
    "    ax6.axhline(y=100, color=\"black\", linestyle=\"dashed\", linewidth=1, alpha=0.5)\n",
    "    # Annotate the line\n",
    "    # Use the right edge of the subplot for text annotation to prevent overflow\n",
    "    right_edge = ax6.get_xlim()[1]\n",
    "    ax6.text(\n",
    "        right_edge,  # x position at the right edge\n",
    "        100,  # y position at the line\n",
    "        \"Sequence length $L=100$ was used in our experiments\",\n",
    "        horizontalalignment=\"right\",  # Align text to the right\n",
    "        fontsize=10,\n",
    "        fontstyle=\"italic\",\n",
    "    )\n",
    "    ax6.set_title(\"(E) Number of time steps per recorded neuron\", fontsize=14)\n",
    "    ax6.set_ylabel(\"Time steps per neuron\")\n",
    "    ax6.set_xticklabels(tsn_stats[\"dataset\"], rotation=45, ha=\"right\")\n",
    "    ax6.set_xticks([])  # Delete xticks\n",
    "\n",
    "    # ########### SAMPLING INTERVAL BAR PLOT ###############\n",
    "    ax7 = plt.subplot(\n",
    "        gs[1, 2:4]\n",
    "    )  # Subplot for 'Sampling interval of recorded neural activity' bar plot\n",
    "    # Compute data for the sampling interval bar plot\n",
    "    dt_stats = (\n",
    "        train_dataset_info.groupby(\"dataset\")[\"original_median_dt\"]\n",
    "        .agg([\"mean\", \"sem\"])\n",
    "        .reset_index()\n",
    "        .sort_values(by=\"mean\", ascending=False)\n",
    "    )\n",
    "    dt_colors = dt_stats[\"dataset\"].apply(lambda x: dataset_color_code.get(x, \"grey\"))\n",
    "    ax7.bar(\n",
    "        dt_stats[\"dataset\"],\n",
    "        dt_stats[\"mean\"],\n",
    "        yerr=2 * dt_stats[\"sem\"],\n",
    "        color=dt_colors,\n",
    "        capsize=5,\n",
    "    )\n",
    "    # Add a dashed horizontal line at y=1.0\n",
    "    ax7.axhline(y=1.0, color=\"black\", linestyle=\"dashed\", linewidth=1, alpha=0.5)\n",
    "    # Annotate the line\n",
    "    # Use the right edge of the subplot for text annotation to prevent overflow\n",
    "    right_edge = ax7.get_xlim()[1]\n",
    "    ax7.text(\n",
    "        right_edge,  # x position at the right edge\n",
    "        1.0,  # y position at the line\n",
    "        \"We downsampled all data to $\\Delta s = 1.0s$ (1 Hz)\",\n",
    "        horizontalalignment=\"right\",  # Align text to the right\n",
    "        fontsize=10,\n",
    "        fontstyle=\"italic\",\n",
    "    )\n",
    "    ax7.set_title(\"(F) Sampling interval of recorded neural activity\", fontsize=14)\n",
    "    ax7.set_ylabel(r\"Mean sampling interval ($\\Delta$s)\")\n",
    "    ax7.set_xticklabels(dt_stats[\"dataset\"], rotation=45, ha=\"right\")\n",
    "    ax7.set_xticks([])  # Delete xticks\n",
    "\n",
    "    # ########### LEGEND SUBPLOT ###############\n",
    "    ax5 = plt.subplot(gs[0, 3])  # Subplot for legend\n",
    "    ax5.legend(\n",
    "        handles=color_legend,\n",
    "        labels=dataset_labels.values(),  # DEBUG\n",
    "        loc=\"center\",\n",
    "        title=\"Experimental datasets\",\n",
    "        fontsize=11,\n",
    "        title_fontsize=12,\n",
    "    )\n",
    "    ax5.get_legend().get_title().set_fontstyle(\"italic\")\n",
    "    ax5.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Construct and return the dataset_info dictionary with all the computed stats\n",
    "    dataset_info = {\n",
    "        \"train_dataset_info\": train_dataset_info,\n",
    "        \"amount_of_data_distribution\": amount_of_data_distribution,\n",
    "        \"num_worms_per_dataset\": num_worms_per_dataset,\n",
    "        \"total_duration_stats\": total_duration_stats,\n",
    "        \"neuron_pop_stats\": neuron_pop_stats,\n",
    "        \"recording_duration_stats\": recording_duration_stats,\n",
    "        \"tsn_stats\": tsn_stats,\n",
    "        \"dt_stats\": dt_stats,\n",
    "    }\n",
    "\n",
    "    return dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "fig1_path_dict = {\n",
    "    \"train_dataset_info\": \"/om2/user/qsimeon/worm-graph/data/train_AllExperimental/train_dataset_info.csv\",  # Path to train dataset info => extract number of train time steps\n",
    "    \"val_dataset_info\": \"/om2/user/qsimeon/worm-graph/data/validation_AllExperimental/val_dataset_info.csv\",  # Path to val. dataset info => extract number of val. time steps\n",
    "    \"combined_dataset_info\": \"/om2/user/qsimeon/worm-graph/data/combined_AllExperimental/combined_dataset_info.csv\",  # Path to combined dataset info => extract total number of worms and time step interval\n",
    "}\n",
    "\n",
    "# NOTE: Error bars on bar plots show +/- 2 SEM\n",
    "dataset_info = dataset_information(path_dict=fig1_path_dict, legend_code=leg_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Law Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cudf.pandas.profile\n",
    "\n",
    "\n",
    "def get_results_df(results_root_dir):\n",
    "    \"\"\"\n",
    "    Recursively traverses the experiment directories, reads train_metrics.csv and\n",
    "    validation_loss_per_dataset.csv files, and combines the data into a comprehensive DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - results_root_dir: The root directory where the experiment directories are located.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing all the combined results from the experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    def recursive_search(path, results):\n",
    "        \"\"\"\n",
    "        Helper function to perform a recursive search for csv files in the directory tree.\n",
    "        \"\"\"\n",
    "        for entry in os.scandir(path):\n",
    "            if entry.is_dir() and any(os.scandir(entry.path)):\n",
    "                recursive_search(entry.path, results)\n",
    "            elif entry.is_file():\n",
    "                if entry.name == \"train_metrics.csv\":\n",
    "                    train_metrics_path = entry.path\n",
    "                    validation_loss_path = os.path.join(\n",
    "                        os.path.dirname(os.path.dirname(train_metrics_path)),\n",
    "                        \"analysis\",\n",
    "                        \"validation_loss_per_dataset.csv\",\n",
    "                    )\n",
    "\n",
    "                    if os.path.exists(validation_loss_path):\n",
    "                        # Read the csv files\n",
    "                        train_metrics_df = pd.read_csv(train_metrics_path)\n",
    "                        validation_loss_df = pd.read_csv(validation_loss_path)\n",
    "\n",
    "                        # Extract parameters using the provided experiment_parameter function\n",
    "                        exp_path = os.path.dirname(os.path.dirname(train_metrics_path))\n",
    "                        experiment_seed = experiment_parameter(exp_path, \"experiment_seed\")[0]\n",
    "                        train_dataset = experiment_parameter(exp_path, \"train_dataset\")[0]\n",
    "                        num_worms = experiment_parameter(exp_path, \"num_worms\")[0]\n",
    "                        num_time_steps = experiment_parameter(exp_path, \"num_time_steps\")[0]\n",
    "                        num_named_neurons = experiment_parameter(exp_path, \"num_named_neurons\")[0]\n",
    "                        time_steps_per_neuron = experiment_parameter(\n",
    "                            exp_path, \"time_steps_per_neuron\"\n",
    "                        )[0]\n",
    "                        train_split_first = experiment_parameter(exp_path, \"train_split_first\")[0]\n",
    "                        hidden_size = experiment_parameter(exp_path, \"hidden_size\")[0]\n",
    "                        batch_size = experiment_parameter(exp_path, \"batch_size\")[0]\n",
    "                        seq_len = experiment_parameter(exp_path, \"seq_len\")[0]\n",
    "                        resample_dt = experiment_parameter(exp_path, \"resample_dt\")[0]\n",
    "                        time_last_epoch = experiment_parameter(exp_path, \"time_last_epoch\")[0]\n",
    "                        computation_flops = experiment_parameter(exp_path, \"computation_flops\")[0]\n",
    "                        num_parameters = experiment_parameter(exp_path, \"num_parameters\")[0]\n",
    "                        model_type = experiment_parameter(exp_path, \"model\")[0]\n",
    "\n",
    "                        # Calculate the min_val_loss and val_baseline\n",
    "                        min_val_loss = train_metrics_df[\"val_loss\"].min()\n",
    "                        val_baseline = train_metrics_df[\"val_baseline\"].mean()\n",
    "\n",
    "                        # Append data to the results list\n",
    "                        for _, row in validation_loss_df.iterrows():\n",
    "                            results.append(\n",
    "                                {\n",
    "                                    \"experiment_ID\": os.path.basename(exp_path),\n",
    "                                    \"experiment_seed\": experiment_seed,\n",
    "                                    \"train_dataset\": train_dataset,\n",
    "                                    \"num_worms\": num_worms,\n",
    "                                    \"num_time_steps\": num_time_steps,\n",
    "                                    \"num_named_neurons\": num_named_neurons,\n",
    "                                    \"time_steps_per_neuron\": time_steps_per_neuron,\n",
    "                                    \"train_split_first\": train_split_first,\n",
    "                                    \"hidden_size\": hidden_size,\n",
    "                                    \"batch_size\": batch_size,\n",
    "                                    \"seq_len\": seq_len,\n",
    "                                    \"resample_dt\": resample_dt,\n",
    "                                    \"time_last_epoch\": time_last_epoch,\n",
    "                                    \"computation_flops\": computation_flops,\n",
    "                                    \"num_parameters\": num_parameters,\n",
    "                                    \"model_type\": model_type,\n",
    "                                    \"min_val_loss\": min_val_loss,\n",
    "                                    \"val_baseline\": val_baseline,\n",
    "                                    \"validation_dataset\": row[\"dataset\"],\n",
    "                                    \"validation_loss\": row[\"validation_loss\"],\n",
    "                                    \"validation_baseline\": row[\"validation_baseline\"],\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "    # List to store result dictionaries\n",
    "    results = []\n",
    "    recursive_search(results_root_dir, results)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "results_root_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/version_1\"\n",
    "combined_results_df = get_results_df(results_root_directory)\n",
    "combined_results_df  # .head()  # To display the first few rows of the resulting DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cudf.pandas.profile\n",
    "\n",
    "\n",
    "# We varied the hidden size as a \"knob\" to vary the number of parameters.\n",
    "def parameters_scaling_plot(combined_results_df, legend_code, title=None):\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(title, fontsize=14)\n",
    "\n",
    "    # Group the DataFrame by 'model_type' and plot\n",
    "    for model_type, group in combined_results_df.groupby(\"model_type\"):\n",
    "        # Skip if unrecognized type of model\n",
    "        if model_type not in legend_code[\"model_labels\"]:\n",
    "            continue\n",
    "        # Plot settings from legend_code\n",
    "        model_label = legend_code[\"model_labels\"][model_type]\n",
    "        marker = legend_code[\"model_marker_code\"][model_label]\n",
    "        color = legend_code[\"model_color_code\"][model_label]\n",
    "\n",
    "        # Scatter plot of validation loss vs. number of parameters\n",
    "        ax.scatter(\n",
    "            group[\"num_parameters\"],\n",
    "            group[\"min_val_loss\"],\n",
    "            marker=marker,\n",
    "            s=10,\n",
    "            color=color,\n",
    "            label=model_label,\n",
    "            alpha=0.5,  # Adjust alpha to your preference\n",
    "        )\n",
    "\n",
    "    baseline = np.round(combined_results_df[\"val_baseline\"], 3).unique()\n",
    "    # DEBUG: There should be one baseline but we may have two if we ran two sets of experiments (train_split_first = True or False)\n",
    "    for bsl in baseline:\n",
    "        ax.plot(\n",
    "            [\n",
    "                combined_results_df[\"num_parameters\"].min(),\n",
    "                combined_results_df[\"num_parameters\"].max(),\n",
    "            ],\n",
    "            [bsl, bsl],\n",
    "            label=\"Baseline\",\n",
    "            color=\"black\",\n",
    "            alpha=0.7,\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "\n",
    "    # Create legends\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.set_xlabel(\"Num. trainable parameters\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Minimum Validation Loss (MSE)\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`results_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/version_1/num_parameters/experimental\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the relevant results\n",
    "results_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/version_1/num_parameters/experimental\"  # real worms AllExperimental\n",
    "results_df = get_results_df(results_directory)\n",
    "results_df  # .head()\n",
    "\n",
    "# Example usage:\n",
    "parameters_scaling_plot(results_df, leg_code, title=\"All Experimental\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`results_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/version_1/num_parameters/synthetic\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve the relevant results\n",
    "# results_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/version_1/num_parameters/synthetic/seed_0\"  # synthetic data Sines\n",
    "# results_df = get_results_df(results_directory)\n",
    "# results_df  # .head()\n",
    "\n",
    "# # Example usage:\n",
    "# parameters_scaling_plot(results_df, leg_code, title=\"Sines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve the relevant results\n",
    "# results_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/num_parameters/synthetic/seed_1\"  # synthetic data Lorenz\n",
    "# results_df = get_results_df(results_directory)\n",
    "# results_df  # .head()\n",
    "\n",
    "# # Example usage:\n",
    "# parameters_scaling_plot(results_df, leg_code, title=\"Lorenz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve the relevant results\n",
    "# results_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/num_parameters/synthetic/seed_2\"  # synthetic data White Noise\n",
    "# results_df = get_results_df(results_directory)\n",
    "# results_df  # .head()\n",
    "\n",
    "# # Example usage:\n",
    "# parameters_scaling_plot(results_df, leg_code, title=\"White Noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve the relevant results\n",
    "# results_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/num_parameters/synthetic/seed_3\"  # synthetic data Random Walk\n",
    "# results_df = get_results_df(results_directory)\n",
    "# results_df  # .head()\n",
    "\n",
    "# # Example usage:\n",
    "# parameters_scaling_plot(results_df, leg_code, title=\"Random Walk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve the relevant results\n",
    "# results_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/num_parameters/synthetic/seed_5\"  # synthetic data Shakespeare\n",
    "# results_df = get_results_df(results_directory)\n",
    "# results_df  # .head()\n",
    "\n",
    "# # Example usage:\n",
    "# parameters_scaling_plot(results_df, leg_code, title=\"Shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cudf.pandas.profile\n",
    "\n",
    "\n",
    "def print_best_model_info(df):\n",
    "    \"\"\"\n",
    "    Prints the hidden size and number of parameters for the entry with the lowest min_val_loss\n",
    "    for each unique model type in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: A pandas DataFrame containing the model results.\n",
    "    \"\"\"\n",
    "    model_types = df[\"model_type\"].unique()\n",
    "    for model_type in model_types:\n",
    "        # Filter for the current model type\n",
    "        model_df = df[df[\"model_type\"] == model_type]\n",
    "\n",
    "        # Find the index of the minimum validation loss\n",
    "        idx_min_val_loss = model_df[\"min_val_loss\"].idxmin()\n",
    "\n",
    "        # Get the row for the minimum validation loss\n",
    "        min_val_loss_row = model_df.loc[idx_min_val_loss]\n",
    "\n",
    "        # Extract the hidden_size and num_parameters\n",
    "        hidden_size = min_val_loss_row[\"hidden_size\"]\n",
    "        num_parameters = min_val_loss_row[\"num_parameters\"]\n",
    "        min_val_loss = min_val_loss_row[\"min_val_loss\"]\n",
    "\n",
    "        print(\n",
    "            f\"The {model_type} model achieved its lowest min_val_loss of {min_val_loss:.6f} \"\n",
    "            f\"with a hidden_size of {hidden_size} and num_parameters of {num_parameters}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "print_best_model_info(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Dataset Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cudf.pandas.profile\n",
    "\n",
    "\n",
    "def mixed_scaling_plot(combined_results_df, legend_code):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    for model_type, model_group in combined_results_df.groupby(\"model_type\"):\n",
    "        # Skip if unrecognized type of model\n",
    "        if model_type not in legend_code[\"model_labels\"]:\n",
    "            continue\n",
    "\n",
    "        # Obtain values for plotting\n",
    "        nts = model_group[\"num_time_steps\"]\n",
    "        val_loss = model_group[\"min_val_loss\"]\n",
    "        baseline_loss = model_group[\"val_baseline\"].iloc[0]  # Baseline is constant\n",
    "\n",
    "        # Plot settings from legend_code\n",
    "        model_label = legend_code[\"model_labels\"][model_type]\n",
    "        marker = legend_code[\"model_marker_code\"][model_label]\n",
    "        color = legend_code[\"model_color_code\"][model_label]\n",
    "\n",
    "        # Scatter plot\n",
    "        ax.scatter(\n",
    "            nts,\n",
    "            val_loss,\n",
    "            marker=marker,\n",
    "            color=color,  # TODO: is this preventing the alpha from kicking in?\n",
    "            s=7,\n",
    "            label=None,\n",
    "            alpha=0.3,\n",
    "        )\n",
    "\n",
    "        # Baseline plot\n",
    "        ax.axhline(\n",
    "            baseline_loss,\n",
    "            label=\"Baseline\"\n",
    "            if model_type == list(legend_code[\"model_labels\"].values())[0]\n",
    "            else \"_nolegend_\",\n",
    "            color=\"black\",\n",
    "            alpha=0.7,\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "\n",
    "        # Regression line\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "            np.log(nts), np.log(val_loss)\n",
    "        )\n",
    "        fit_label = f\"y = {slope:.2f}x + {intercept:.1f}\\n\" f\"$R^2={r_value**2:.2f}$\"\n",
    "        x_fit = np.linspace(nts.min(), nts.max(), 10000)\n",
    "        y_fit = np.exp(intercept + slope * np.log(x_fit))\n",
    "\n",
    "        ax.plot(x_fit, y_fit, color=color, label=fit_label)\n",
    "\n",
    "    # Handles and labels for first legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    # Create the second legend\n",
    "    legend2 = ax.legend(\n",
    "        handles=legend_code[\"marker_legend\"],\n",
    "        loc=\"center right\",\n",
    "        bbox_to_anchor=(0.995, 0.85),\n",
    "        title=\"Model architecture\",\n",
    "        fontsize=\"x-small\",\n",
    "    )\n",
    "    ax.get_legend().get_title().set_fontstyle(\"italic\")\n",
    "    ax.get_legend().get_title().set_fontsize(\"x-small\")\n",
    "    ax.add_artist(legend2)\n",
    "\n",
    "    # Display both legends\n",
    "    ax.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(0.01, 0.25), fontsize=\"xx-small\")\n",
    "\n",
    "    ax.set_xlabel(\"Num. train time steps\", fontsize=12)\n",
    "    ax.set_ylabel(\"Validation MSE Loss\", fontsize=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`results_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/version_1/num_time_steps/experimental\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the relevant results\n",
    "results_directory = (\n",
    "    \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/version_1/num_time_steps/experimental\"\n",
    ")\n",
    "results_df = get_results_df(results_directory)\n",
    "results_df  # .head()\n",
    "\n",
    "# Example usage\n",
    "mixed_scaling_plot(results_df, leg_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Dataset Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%cudf.pandas.profile\n",
    "\n",
    "\n",
    "def individual_scaling_plot(combined_results_df, legend_code):\n",
    "    fig, axs = plt.subplots(1, len(combined_results_df[\"model_type\"].unique()), figsize=(15, 5))\n",
    "\n",
    "    if len(combined_results_df[\"model_type\"].unique()) > 1:\n",
    "        axs = axs.flatten()\n",
    "    else:\n",
    "        axs = [axs]\n",
    "\n",
    "    for idx, (model_type, model_group) in enumerate(combined_results_df.groupby(\"model_type\")):\n",
    "        # Skip if unrecognized type of model\n",
    "        if model_type not in legend_code[\"model_labels\"]:\n",
    "            continue\n",
    "\n",
    "        model_label = legend_code[\"model_labels\"][model_type]\n",
    "\n",
    "        ax = axs[idx]\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "        for dataset_name, dataset_group in model_group.groupby(\"validation_dataset\"):\n",
    "            color = legend_code[\"dataset_color_code\"][dataset_name]\n",
    "\n",
    "            # Scatter plot for individual dataset\n",
    "            ax.scatter(\n",
    "                dataset_group[\"num_time_steps\"],\n",
    "                dataset_group[\"validation_loss\"],\n",
    "                s=7,\n",
    "                color=color,\n",
    "                label=dataset_name,\n",
    "                alpha=0.3,\n",
    "            )\n",
    "\n",
    "            # Baseline horizontal line\n",
    "            baseline_loss = dataset_group[\"validation_baseline\"].iloc[0]\n",
    "            ax.axhline(\n",
    "                y=baseline_loss,\n",
    "                color=color,\n",
    "                linestyle=\"--\",\n",
    "                linewidth=1,\n",
    "                alpha=0.7,\n",
    "                label=f\"{dataset_name} baseline\",\n",
    "            )\n",
    "\n",
    "            # Linear regression\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "                np.log(dataset_group[\"num_time_steps\"]),\n",
    "                np.log(dataset_group[\"validation_loss\"]),\n",
    "            )\n",
    "\n",
    "            # Plot regression line\n",
    "            x_reg = np.linspace(\n",
    "                dataset_group[\"num_time_steps\"].min(),\n",
    "                dataset_group[\"num_time_steps\"].max(),\n",
    "                100,\n",
    "            )\n",
    "            y_reg = np.exp(intercept + slope * np.log(x_reg))\n",
    "            ax.plot(x_reg, y_reg, color=color)\n",
    "\n",
    "        ax.set_title(model_label, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Num. train time steps\")\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel(\"Validation Loss\")\n",
    "        else:\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "    # Adjust the layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`results_directory = \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/version_1/num_time_steps/experimental\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the relevant results\n",
    "results_directory = (\n",
    "    \"/om2/vast/yanglab/shared/qsimeon/worm-graph-experiments/version_1/num_time_steps/experimental\"\n",
    ")\n",
    "results_df = get_results_df(results_directory)\n",
    "results_df.head()\n",
    "\n",
    "# Example usage:\n",
    "individual_scaling_plot(results_df, leg_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Cross Dataset Generalization\n",
    "A heatmap showing the validation loss on all of the experimental datasets after a model has been trained on just one of the experimental datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Cross-dataset Generalization plot\n",
    "# def cross_dataset(experiment_log_folders, model_names, legend_code):\n",
    "#     dataset_labels = legend_code[\"dataset_labels\"]\n",
    "\n",
    "#     analysis_df = pd.DataFrame(\n",
    "#         columns=[\n",
    "#             \"experiment_ID\",\n",
    "#             \"model_type\",\n",
    "#             \"train_dataset\",\n",
    "#             \"val_dataset\",\n",
    "#             \"val_loss\",\n",
    "#             \"val_baseline\",\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     for exp_log_dir, model in zip(experiment_log_folders, model_names):\n",
    "#         for experiment_ID in sorted(\n",
    "#             os.listdir(exp_log_dir), key=lambda x: x.strip(\"exp_\")\n",
    "#         ):\n",
    "#             # Skip if not starts with exp\n",
    "#             if not experiment_ID.startswith(\"exp\") or experiment_ID.startswith(\"exp_\"):\n",
    "#                 continue\n",
    "\n",
    "#             val_url = os.path.join(\n",
    "#                 exp_log_dir,\n",
    "#                 experiment_ID,\n",
    "#                 \"analysis\",\n",
    "#                 \"validation_loss_per_dataset.csv\",\n",
    "#             )\n",
    "#             train_ds_url = os.path.join(\n",
    "#                 exp_log_dir, experiment_ID, \"dataset\", \"train_dataset_info.csv\"\n",
    "#             )\n",
    "\n",
    "#             val_df = pd.read_csv(val_url)\n",
    "#             val_df[\"experiment_ID\"] = experiment_ID\n",
    "#             val_df[\"model_type\"] = model\n",
    "\n",
    "#             train_dataset_info = pd.read_csv(train_ds_url)\n",
    "#             val_df[\"train_dataset\"] = train_dataset_info[\"dataset\"].unique()[0]\n",
    "\n",
    "#             # Change 'dataset' column name to 'val_dataset'\n",
    "#             val_df = val_df.rename(columns={\"dataset\": \"val_dataset\"})\n",
    "\n",
    "#             # Swap uzel and kaplan\n",
    "#             val_df = val_df.iloc[[0, 1, 2, 4, 3, 5, 6], :]\n",
    "#             val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "#             analysis_df = pd.concat([analysis_df, val_df], axis=0)\n",
    "\n",
    "#     train_ds_names = [\n",
    "#         \"Leifer2023\",\n",
    "#         \"Flavell2023\",\n",
    "#         \"Uzel2022\",\n",
    "#         \"Yemini2021\",\n",
    "#         \"Kaplan2020\",\n",
    "#         \"Skora2018\",\n",
    "#         \"Nichols2017\",\n",
    "#         \"Kato2015\",\n",
    "#     ]\n",
    "#     val_ds_names = analysis_df[\"val_dataset\"].unique()\n",
    "#     models = analysis_df[\"model_type\"].unique()\n",
    "\n",
    "#     # Figure size\n",
    "#     fig, ax = plt.subplots(1, len(models), figsize=(12, 4), sharex=\"col\", sharey=\"row\")\n",
    "\n",
    "#     # Initialize vmin and vmax for the color scale\n",
    "#     vmin = analysis_df[\"val_loss\"].min()\n",
    "#     vmax = analysis_df[\"val_loss\"].max()\n",
    "\n",
    "#     for i, model_name in enumerate(models):\n",
    "#         # Filter data for the specific model\n",
    "#         df_model_subset = analysis_df[analysis_df[\"model_type\"] == model_name]\n",
    "\n",
    "#         # Create an empty matrix for the heatmap data\n",
    "#         heatmap_data = pd.DataFrame(columns=train_ds_names, index=val_ds_names)\n",
    "\n",
    "#         for train_ds in train_ds_names:\n",
    "#             for val_ds in val_ds_names:\n",
    "#                 value = df_model_subset[\n",
    "#                     (df_model_subset[\"train_dataset\"] == train_ds)\n",
    "#                     & (df_model_subset[\"val_dataset\"] == val_ds)\n",
    "#                 ][\"val_loss\"].values\n",
    "#                 if value:\n",
    "#                     heatmap_data.at[val_ds, train_ds] = value[0]\n",
    "\n",
    "#         # Plot the heatmap\n",
    "#         sns.heatmap(\n",
    "#             heatmap_data.astype(float),\n",
    "#             cmap=\"magma_r\",\n",
    "#             ax=ax[i],\n",
    "#             annot=True,\n",
    "#             square=True,\n",
    "#             cbar=False,\n",
    "#             vmin=vmin,\n",
    "#             vmax=vmax,\n",
    "#         )\n",
    "#         ax[i].set_title(\"{}\".format(model_name), fontsize=16)\n",
    "#         # Set xlabel\n",
    "#         ax[i].set_xlabel(\"Train dataset\", fontsize=14, fontweight=\"bold\")\n",
    "#         ax[i].set_xticklabels(dataset_labels, rotation=90, fontsize=10)\n",
    "#         # Set ylabel\n",
    "#         ax[0].set_ylabel(\"Validation dataset\", fontsize=14, fontweight=\"bold\")\n",
    "#         ax[i].set_yticklabels(dataset_labels, rotation=0, fontsize=10)\n",
    "\n",
    "#     # Add a single colorbar at the rightmost part\n",
    "#     cbar_ax = fig.add_axes(\n",
    "#         [0.92, 0.125, 0.02, 0.755]\n",
    "#     )  # [left, bottom, width, height] of the colorbar axes in figure coordinates.\n",
    "#     fig.colorbar(\n",
    "#         ax[-1].collections[0], cax=cbar_ax\n",
    "#     )  # ax[-1].collections[0] grabs the colormap of the last subplot\n",
    "\n",
    "#     # Add title to cmap\n",
    "#     cbar_ax.set_ylabel(\n",
    "#         \"Validation loss (MSE)\",\n",
    "#         fontsize=12,\n",
    "#         fontweight=\"bold\",\n",
    "#         rotation=90,\n",
    "#         labelpad=-57,\n",
    "#     )\n",
    "\n",
    "#     plt.tight_layout(pad=0.1)\n",
    "#     plt.subplots_adjust(\n",
    "#         right=0.9\n",
    "#     )  # adjust the rightmost part to make room for the colorbar\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_log_folders = [\n",
    "#     os.path.join(\"results\", \"CDS_LINEAR\"),  # Linear trial 1\n",
    "#     os.path.join(\"results\", \"CDS_LSTM\"),  # LSTM trial 1\n",
    "#     os.path.join(\"results\", \"CDS_TRANSFORMER\"),  # Transformer trial 1\n",
    "# ]\n",
    "\n",
    "# model_names = [\"Feedforward\", \"LSTM\", \"Transformer\"]\n",
    "\n",
    "# cross_dataset(\n",
    "#     experiment_log_folders=experiment_log_folders,\n",
    "#     model_names=model_names,\n",
    "#     legend_code=leg_code,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
