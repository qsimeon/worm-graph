{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from models._utils import load_model_checkpoint\n",
    "from data._utils import create_combined_dataset, split_combined_dataset\n",
    "from utils import DEVICE, NEURONS_302, VALID_DATASETS, SYNTHETIC_DATASETS, init_random_seeds\n",
    "\n",
    "# Initialize the random seeds\n",
    "init_random_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_before_train = torch.load(\n",
    "    \"/om2/user/qsimeon/worm-graph/logs/hydra/2024_01_28_18_01_59/exp0/train/checkpoints/model_epoch_0.pt\",\n",
    "    map_location=DEVICE,\n",
    ")\n",
    "\n",
    "model_after_train = torch.load(\n",
    "    \"/om2/user/qsimeon/worm-graph/logs/hydra/2024_01_28_18_01_59/exp0/train/checkpoints/model_best.pt\",\n",
    "    map_location=DEVICE,\n",
    ")\n",
    "\n",
    "model = load_model_checkpoint(\n",
    "    \"/om2/user/qsimeon/worm-graph/logs/hydra/2024_01_28_18_01_59/exp0/train/checkpoints/model_best.pt\"\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_before_train[\"model_state_dict\"][\"linear.weight\"].cpu().numpy()\n",
    "biases = model_before_train[\"model_state_dict\"][\"linear.bias\"].cpu().unsqueeze(-1).numpy()\n",
    "\n",
    "sns.heatmap(weights[:100, :100], cmap=\"RdBu_r\", center=0)\n",
    "plt.show()\n",
    "sns.heatmap(biases[:100], cmap=\"RdBu_r\", center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_after_train[\"model_state_dict\"][\"linear.weight\"].cpu().numpy()\n",
    "biases = model_after_train[\"model_state_dict\"][\"linear.bias\"].cpu().unsqueeze(-1).numpy()\n",
    "\n",
    "sns.heatmap(weights[:100, :100], cmap=\"RdBu_r\", center=0)\n",
    "plt.show()\n",
    "sns.heatmap(biases[:100], cmap=\"RdBu_r\", center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single combined dataset from the one of the 8 experimental datasets of your choice\n",
    "experimental_dataset = \"Shakespeare0000\"\n",
    "assert experimental_dataset in set(VALID_DATASETS) | set(\n",
    "    SYNTHETIC_DATASETS\n",
    "), f\"Invalid dataset: {experimental_dataset}\"\n",
    "datasets = {experimental_dataset: 1}  # a real worm neural dataset (1 worm)\n",
    "combined_dataset, dataset_info = create_combined_dataset(datasets, num_named_neurons=None)\n",
    "num_worms = len(combined_dataset)\n",
    "\n",
    "# Split the datsaet into train and validation halves\n",
    "num_train_samples = num_test_samples = num_samples = 2\n",
    "reverse = use_residual = False\n",
    "smooth_data = True\n",
    "train_split_first = False\n",
    "train_split_ratio = 0.5\n",
    "seq_len = 180\n",
    "train_dataset, test_dataset, timestep_info = split_combined_dataset(\n",
    "    combined_dataset,\n",
    "    num_train_samples,\n",
    "    num_test_samples,\n",
    "    seq_len,\n",
    "    reverse,\n",
    "    use_residual,\n",
    "    smooth_data,\n",
    "    train_split_first,\n",
    "    train_split_ratio,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get inputs and targets from dataloaders and outputs from model\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_train, Y_train, mask_train, metadata_train = next(iter(train_loader))\n",
    "    time_train = metadata_train[\"time_vec\"]\n",
    "    if model.version_2:\n",
    "        tokens = model.tokenize_neural_data(Y_train.to(DEVICE), mask_train.to(DEVICE))\n",
    "        Y_pred_train = model.neural_embedding[tokens]\n",
    "    else:\n",
    "        Y_pred_train = model(X_train.to(DEVICE), mask_train.to(DEVICE))\n",
    "    print(\n",
    "        f\"\\nX:{X_train.shape}\\nY:{Y_train.shape}\\nmask:{mask_train.shape}\\ntime:{time_train.shape}\\n\"\n",
    "    )\n",
    "\n",
    "    X_test, Y_test, mask_test, metadata_test = next(iter(test_loader))\n",
    "    time_test = metadata_test[\"time_vec\"]\n",
    "    if model.version_2:\n",
    "        tokens = model.tokenize_neural_data(Y_test.to(DEVICE), mask_test.to(DEVICE))\n",
    "        Y_pred_test = model.neural_embedding[tokens]\n",
    "    else:\n",
    "        Y_pred_test = model(X_test.to(DEVICE), mask_test.to(DEVICE))\n",
    "    print(f\"\\nX:{X_test.shape}\\nY:{Y_test.shape}\\nmask:{mask_test.shape}\\ntime:{time_test.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure for two columns of subplots\n",
    "max_neurons_to_plot = 10\n",
    "num_neurons_train, num_neurons_test = mask_train.sum().item(), mask_test.sum().item()\n",
    "num_neurons = min(max_neurons_to_plot, num_neurons_train, num_neurons_test)\n",
    "fig, axes = plt.subplots(num_neurons, 2, figsize=(15, 10), sharey=\"row\")\n",
    "\n",
    "# Plot data for each half\n",
    "for col, phase in enumerate([\"train\", \"test\"]):\n",
    "    # Convert the data to NumPy arrays if they are not already\n",
    "    time = eval(f\"time_{phase}.squeeze().cpu().numpy()\")\n",
    "    X = eval(f\"X_{phase}.detach().squeeze().cpu().numpy()\")  # input\n",
    "    Y = eval(f\"Y_{phase}.squeeze().cpu().numpy()\")  # target\n",
    "    Y_pred = eval(f\"Y_pred_{phase}.detach().squeeze().cpu().numpy()\")  # output\n",
    "    mask = eval(f\"mask_{phase}.detach().squeeze().cpu().numpy()\")\n",
    "    worm_dataset = eval(f\"metadata_{phase}['worm_dataset']\")\n",
    "    wormID = eval(f\"metadata_{phase}['wormID']\")\n",
    "\n",
    "    # Set offsets for the target data\n",
    "    y_offset_step = 5\n",
    "    x_offset = len(time) / 50\n",
    "\n",
    "    # Get the indices of the neurons to plot\n",
    "    neurons_to_plot = np.where(mask)[0][:num_neurons]\n",
    "\n",
    "    data_fragments = []\n",
    "\n",
    "    for i, neuron_idx in enumerate(neurons_to_plot):\n",
    "        neuron_label = NEURONS_302[neuron_idx]\n",
    "\n",
    "        # Collect input data\n",
    "        input_df = pd.DataFrame(\n",
    "            {\n",
    "                \"Time\": time,\n",
    "                \"Value\": X[:, neuron_idx],\n",
    "                \"Type\": \"Input\",\n",
    "                \"Neuron\": neuron_label,\n",
    "            }\n",
    "        )\n",
    "        data_fragments.append(input_df)\n",
    "\n",
    "        # Collect target data with time offset\n",
    "        target_df = pd.DataFrame(\n",
    "            {\n",
    "                \"Time\": time + x_offset,  # Adjust the time if needed for display purposes\n",
    "                \"Value\": Y[:, neuron_idx],\n",
    "                \"Type\": \"Target (+ offset)\",\n",
    "                \"Neuron\": neuron_label,\n",
    "            }\n",
    "        )\n",
    "        data_fragments.append(target_df)\n",
    "\n",
    "        # Collect output data with time offset\n",
    "        output_df = pd.DataFrame(\n",
    "            {\n",
    "                \"Time\": time + x_offset,  # Adjust the time if needed for display purposes\n",
    "                \"Value\": Y_pred[:, neuron_idx],\n",
    "                \"Type\": \"Model Output\",\n",
    "                \"Neuron\": neuron_label,\n",
    "            }\n",
    "        )\n",
    "        data_fragments.append(output_df)\n",
    "\n",
    "    # Concatenate all fragments to create the full DataFrame\n",
    "    df = pd.concat(data_fragments, ignore_index=True)\n",
    "\n",
    "    # Plot the lines\n",
    "    for i, neuron_label in enumerate(neurons_to_plot):\n",
    "        sns.lineplot(\n",
    "            data=df[df[\"Neuron\"] == NEURONS_302[neuron_label]],\n",
    "            x=\"Time\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Type\",\n",
    "            ax=axes[i, col],\n",
    "            palette=\"tab10\",\n",
    "            alpha=0.7,\n",
    "            errorbar=None,\n",
    "        )\n",
    "        axes[i, col].set_ylabel(NEURONS_302[neuron_label])\n",
    "        axes[i, col].get_legend().remove()\n",
    "\n",
    "        # Set the x-axis label to empty\n",
    "        axes[i, col].set_xlabel(\"\")\n",
    "\n",
    "        # Hide x-axis labels and ticks for all but the last row\n",
    "        if i < num_neurons - 1:\n",
    "            axes[i, col].set_xticklabels([])\n",
    "            axes[i, col].set_xticks([])\n",
    "        else:\n",
    "            axes[i, col].set_xlabel(\"Time (s)\")\n",
    "\n",
    "        # Set title only for the first row of each column\n",
    "        if i == 0:\n",
    "            axes[i, col].set_title(f\"{phase.capitalize()} half\")\n",
    "        else:\n",
    "            axes[i, col].set_title(\"\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Display name of dataset and worm as suptitle\n",
    "fig.suptitle(f\"{worm_dataset} {wormID}\", fontsize=14)\n",
    "\n",
    "# Display the legend outside the rightmost plot\n",
    "handles, labels = axes[0, -1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "\n",
    "all_unique_vectors = torch.vstack([tup[0] for tup in list(train_dataset)]).unique(dim=0)\n",
    "learned_unique_vectors = model.neural_embedding.unique(dim=0)\n",
    "print(all_unique_vectors.shape, learned_unique_vectors.shape)\n",
    "print()\n",
    "\n",
    "all_unique_vectors = {tuple(row.round(decimals=3).cpu().numpy()) for row in all_unique_vectors}\n",
    "learned_unique_vectors = {\n",
    "    tuple(row.round(decimals=3).cpu().numpy()) for row in learned_unique_vectors\n",
    "}\n",
    "print(f\"There are {len(all_unique_vectors)} unique embeddings that generated the neural data.\")\n",
    "print(\n",
    "    f\"The model learned {len(learned_unique_vectors)} unique neural embeddings. But are they the same?\"\n",
    ")\n",
    "print()\n",
    "\n",
    "inter = all_unique_vectors.intersection(learned_unique_vectors)\n",
    "diff = all_unique_vectors - learned_unique_vectors\n",
    "print(f\"Model learned to reproduce {len(inter)}/{len(all_unique_vectors)} embeddings exactly.\")\n",
    "print(f\"The remaining {len(diff)}/{len(all_unique_vectors)} are superpositions of embeddings!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worm-graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
