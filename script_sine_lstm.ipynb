{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "'''\n",
    "@author: ivy\n",
    "@contact: ivyivyzhao77@gmail.com\n",
    "@software: PyCharm 2022.3\n",
    "@file: script_pipeline_validation.py\n",
    "@time: 2023/3/10 14:54\n",
    "'''\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from train._main import *\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.fft as fft\n",
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from models._main import *\n",
    "from preprocess._utils import *\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "from torch.optim import optimizer\n",
    "\n",
    "\n",
    "def create_synthetic_data(d, n, ifnoise=False):\n",
    "    res = np.zeros((d, n))\n",
    "    assert isinstance(n, int), \"wrong number for samples\"\n",
    "    der = []\n",
    "    for i in range(0, n):\n",
    "        freq = np.random.uniform(1.0 / d, 5 * 1.0 / d)\n",
    "        phi = np.random.random()\n",
    "        step = np.arange(d)\n",
    "        res[:, i] = np.sin(2 * np.pi * freq * step + phi * (np.pi / 180))\n",
    "        der.append(2 * np.pi * freq)\n",
    "\n",
    "    return res, der\n",
    "\n",
    "\n",
    "def create_dataset(raw_data, raw_der):\n",
    "    sine_dataset = dict()\n",
    "    for i, real_data in enumerate(raw_data):\n",
    "        worm = \"worm\" + str(i)\n",
    "        max_time = seq_len\n",
    "        num_neurons = num_signal\n",
    "        der = np.array(raw_der[i])\n",
    "        der = der.reshape(der.shape[0], 1).T\n",
    "        time_in_seconds = torch.tensor(np.array(np.arange(seq_len)).reshape(seq_len, 1))\n",
    "        dt = torch.tensor(der)\n",
    "\n",
    "        num_named = num_neurons\n",
    "        real_data = torch.tensor(\n",
    "            real_data, dtype=torch.float64\n",
    "        )\n",
    "\n",
    "        smooth_real_data, residual, smooth_residual = smooth_data_preprocess(real_data, \"fft\")\n",
    "\n",
    "        for i in range(residual.shape[1]):\n",
    "            residual[:, i] = residual[:, i] / dt[:, i]\n",
    "\n",
    "        sine_dataset.update(\n",
    "            {\n",
    "                worm: {\n",
    "                    \"dataset\": \"sine\",\n",
    "                    \"worm\": worm,\n",
    "                    \"calcium_data\": real_data,\n",
    "                    \"smooth_calcium_data\": smooth_real_data,\n",
    "                    \"residual_calcium\": residual,\n",
    "                    \"residual_smooth_calcium\": smooth_residual,\n",
    "                    \"neuron_to_idx\": range(0, num_neurons),\n",
    "                    \"idx_to_neuron\": range(num_neurons - 1, -1, -1),\n",
    "                    \"max_time\": int(max_time),\n",
    "                    \"time_in_seconds\": time_in_seconds,\n",
    "                    \"dt\": dt,\n",
    "                    \"named_neurons_mask\": torch.full((num_neurons,), True),\n",
    "                    \"named_neuron_to_idx\": range(0, num_neurons),\n",
    "                    \"idx_to_named_neuron\": range(num_neurons - 1, -1, -1),\n",
    "                    \"num_neurons\": int(num_neurons),\n",
    "                    \"num_named_neurons\": num_named,\n",
    "                    \"num_unknown_neurons\": int(num_neurons) - num_named,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    return sine_dataset\n",
    "\n",
    "\n",
    "class lstm(nn.Module):\n",
    "    def __init__(self, INPUT_SIZE, HIDDEN_SIZE, LAYERS, DROP_RATE):\n",
    "        super(lstm, self).__init__()\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=INPUT_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            num_layers=LAYERS,\n",
    "            dropout=DROP_RATE,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.hidden_out = nn.Linear(HIDDEN_SIZE, INPUT_SIZE)\n",
    "        self.h_s = None\n",
    "        self.h_c = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        r_out, (h_s, h_c) = self.rnn(x)\n",
    "        output = self.hidden_out(r_out)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataset', 'worm', 'calcium_data', 'smooth_calcium_data', 'residual_calcium', 'residual_smooth_calcium', 'neuron_to_idx', 'idx_to_neuron', 'max_time', 'time_in_seconds', 'dt', 'named_neurons_mask', 'named_neuron_to_idx', 'idx_to_named_neuron', 'num_neurons', 'num_named_neurons', 'num_unknown_neurons'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nh/w4w39l451_gbdf_6q7r0pg8w0000gn/T/ipykernel_8879/1094297658.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  real_data = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "# Creating signal\n",
    "seq_len = 3312\n",
    "num_signal = 302\n",
    "if_noise = False\n",
    "num_worms = 6\n",
    "raw_data = []\n",
    "raw_der = []\n",
    "for j in range(num_worms):\n",
    "    x, der = create_synthetic_data(seq_len, num_signal, if_noise)\n",
    "    x_torch = Variable(torch.from_numpy(x), requires_grad=False)\n",
    "    raw_data.append(x_torch)\n",
    "    raw_der.append(der)\n",
    "\n",
    "dataset = create_dataset(raw_data, raw_der)\n",
    "print(dataset[\"worm0\"].keys())\n",
    "\n",
    "# plt.plot(dataset[\"worm0\"][\"calcium_data\"][:, 3])\n",
    "# plt.plot(dataset[\"worm0\"][\"residual_calcium\"][:, 3])\n",
    "# plt.legend([\"cal\", \"res\"], loc=\"upper right\")\n",
    "# plt.show()\n",
    "\n",
    "# config = OmegaConf.load(\"conf/model.yaml\")\n",
    "# print(\"Model:\", OmegaConf.to_yaml(config), end=\"\\n\\n\")\n",
    "# model = get_model(config)\n",
    "#\n",
    "# config = OmegaConf.load(\"conf/train.yaml\")\n",
    "# model, log_dir = train_model(model, dataset, config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "### quick trial on LSTM, model defined above\n",
    "time_step = 4\n",
    "# hyperparams\n",
    "INPUT_SIZE = 302\n",
    "HIDDEN_SIZE = 302\n",
    "EPOCH = 50\n",
    "LR = 0.1  # learning rate\n",
    "DROP_RATE = 0.2  # drop out\n",
    "LAYERS = 2  # hidden layer\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = lstm(INPUT_SIZE, HIDDEN_SIZE, LAYERS, DROP_RATE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "worm_train = []\n",
    "worm_test = []\n",
    "for i in range(len(dataset)):\n",
    "    worm = \"worm\" + str(i)\n",
    "    calcium_data = dataset[worm][\"calcium_data\"].float()\n",
    "    residual_calcium = dataset[worm][\"residual_calcium\"].float()\n",
    "    train_size = int(0.8 * dataset[worm][\"max_time\"])\n",
    "    test_size = dataset[worm][\"max_time\"] - train_size\n",
    "\n",
    "    train_ca, test_ca = calcium_data.split(train_size, dim=0)\n",
    "    train_res, test_res = residual_calcium.split(train_size, dim=0)\n",
    "    train_batch_size = train_size // time_step\n",
    "    test_batch_size = test_size // time_step\n",
    "\n",
    "    train_dataset = Data.TensorDataset(train_ca, train_res)\n",
    "    train_cal = Data.TensorDataset(train_ca, train_ca)\n",
    "    test_dataset = Data.TensorDataset(test_ca, test_res)\n",
    "    test_cal = Data.TensorDataset(test_ca, test_ca)\n",
    "    train_loader = Data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    ca_train_loader = Data.DataLoader(\n",
    "        dataset=train_ca,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    test_loader = Data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    ca_test_loader = Data.DataLoader(\n",
    "        dataset=test_ca,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    worm_train.append(train_loader)\n",
    "    worm_test.append(test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "--------worm No. 0--------\n",
      "epoch = 0, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 1, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 2, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 3, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 4, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 5, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 6, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 7, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 8, train_loss = -0.0000, test_loss = 0.0001\n",
      "epoch = 9, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 10, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 11, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 12, train_loss = -0.0000, test_loss = 0.0001\n",
      "epoch = 13, train_loss = 0.0000, test_loss = 0.0001\n",
      "epoch = 14, train_loss = -0.0000, test_loss = 0.0001\n",
      "epoch = 15, train_loss = -0.0000, test_loss = 0.0001\n",
      "epoch = 16, train_loss = -0.0000, test_loss = 0.0001\n",
      "epoch = 17, train_loss = -0.0000, test_loss = 0.0001\n",
      "epoch = 18, train_loss = -0.0000, test_loss = 0.0001\n",
      "epoch = 19, train_loss = -0.0000, test_loss = 0.0002\n",
      "epoch = 20, train_loss = 0.0000, test_loss = 0.0002\n",
      "epoch = 21, train_loss = -0.0000, test_loss = 0.0002\n",
      "epoch = 22, train_loss = -0.0000, test_loss = 0.0002\n",
      "epoch = 23, train_loss = -0.0000, test_loss = 0.0002\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Training!\")\n",
    "# train\n",
    "for i in range(len(dataset) - 5):\n",
    "    print(\"--------worm \" + \"No. \" + str(i) + \"--------\")\n",
    "    train_loader = worm_train[i]\n",
    "    test_loader = worm_test[i]\n",
    "    train_base_loss_history = []\n",
    "    train_pred_loss_history = []\n",
    "    test_base_loss_history = []\n",
    "    test_pred_loss_history = []\n",
    "    for e in range(EPOCH):\n",
    "        train_base_loss = 0\n",
    "        train_pred_loss = 0\n",
    "        test_base_loss = 0\n",
    "        test_pred_loss = 0\n",
    "        for X_train, Y_train in train_loader:\n",
    "            X_train, Y_train = X_train.to(DEVICE), Y_train.to(DEVICE)\n",
    "            # Baseline: loss if the model predicted the residual to be 0\n",
    "            base = criterion(torch.zeros_like(Y_train), Y_train)\n",
    "            Y_tr = model(X_train)  # Forward pass.\n",
    "            loss = criterion(Y_tr, Y_train)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            train_base_loss += base.detach().item()\n",
    "            train_pred_loss += loss.detach().item()\n",
    "        train_base_loss_history.append(train_base_loss / train_batch_size)\n",
    "        train_pred_loss_history.append(train_pred_loss / train_batch_size)\n",
    "\n",
    "        for X_test, Y_test in test_loader:\n",
    "            X_test, Y_test = X_test.to(DEVICE), Y_test.to(DEVICE)\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            # Baseline: loss if the model predicted the residual to be 0\n",
    "            base = criterion(torch.zeros_like(Y_test), Y_test)\n",
    "            Y_pred = model(X_test)  # Forward pass.\n",
    "            loss = criterion(Y_pred, Y_test)\n",
    "            test_base_loss += base.detach().item()\n",
    "            test_pred_loss += loss.detach().item()\n",
    "        test_base_loss_history.append(test_base_loss / test_batch_size)\n",
    "        test_pred_loss_history.append(test_pred_loss / test_batch_size)\n",
    "\n",
    "        print(\"epoch = {}, train_loss = {:.4f}, test_loss = {:.4f}\".format(e,\n",
    "                                                                 train_pred_loss_history[-1] - train_base_loss_history[\n",
    "                                                                     -1],\n",
    "                                                                 test_pred_loss_history[-1] - test_base_loss_history[\n",
    "                                                                     -1]))\n",
    "\n",
    "    delta_train = [train_pred_loss_history[i] - train_base_loss_history[i] for i in range(len(train_base_loss_history))]\n",
    "    delta_test = [test_pred_loss_history[i] - test_base_loss_history[i] for i in range(len(test_base_loss_history))]\n",
    "    plt.plot(delta_train)\n",
    "    plt.plot(delta_test)\n",
    "    plt.legend([\"train\", \"test\"])\n",
    "    plt.ylabel(\"Loss-Baseline\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.title(\"Worm \" + str(i) + \", Epoch: \" + str(EPOCH))\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prediction visualization\n",
    "calcium_data = dataset[\"worm0\"][\"calcium_data\"].float()\n",
    "residual_calcium = dataset[\"worm0\"][\"residual_calcium\"].float()\n",
    "prediction = model(calcium_data)\n",
    "plt.plot(prediction[:, 0].detach().numpy())\n",
    "plt.plot(residual_calcium[:, 0])\n",
    "\n",
    "plt.legend([\"pred\", \"target\"])\n",
    "plt.xlabel(\"time\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
