{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-11T13:36:41.277020Z",
     "end_time": "2023-04-11T13:36:41.282442Z"
    }
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from preprocess._utils import *\n",
    "smooth_method = 'fft'\n",
    "transform = MinMaxScaler(feature_range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-11T13:36:41.282792Z",
     "end_time": "2023-04-11T13:36:45.241747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract finish\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# extract .tar.gz file\n",
    "source_path = os.getcwd()\n",
    "filename = \"exported_data\"\n",
    "tf = tarfile.open(os.path.join(source_path, filename + \".tar.gz\"))\n",
    "tf.extractall()\n",
    "print(\"extract finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:-0.10157551800984738\n",
      "shape:(4454,)\n",
      "min:-1.1795625039931958\n",
      "max:1.3235919496305641\n",
      "['ASELL', 'ASELL', 'ASELL', 'SAADL', 'ASGRL', 'ASGRL', 'SAADL', 'FLPRL', 'AWBRL', '48L', '48L', 'RIGL', 'RIGL', 'FLPRL', 'AWBRL']\n",
      "--- 114 (4454, 114)\n",
      "------ 0\n",
      "['114', 'IL2V', 'BAGL', 'ASEL', 'ASEL', 'SMDVL', 'AMsoL', 'I1L', '122', '123', 'RMED', '125', 'NSML', 'M3L', 'M1', 'RMDVL', 'ASEL', 'AVDL', 'AWBL', 'AVEL', 'FLPL', '135', '136', '137', '138', '139', '140', 'I3', 'URXL', '143', '144', 'AVAL', '146', '147', 'SMDDL', 'AIYL', 'AIZL', 'AMsoR', 'OLQDR', '153', '154', 'IL1VL', '156', 'SAAD', '158', 'AQR', 'RMDDL', '161', '162', '163', 'AVJR', 'AWAR', '166', '167', 'I1R', 'OLLR', 'AVDR', 'ASGR', 'M3R', 'ASGR', 'OLQVR', '175', 'SAAD', '177', '178', '179', 'DB1', '181', 'VA1', '183', '184', '185', 'I2R', '187', '188', 'FLPR', '190', 'AWBR', '41', '193', 'AIZR', '48', '48', 'DB2', 'VB1', 'RIG', 'RIG', '201', '202', '203', '204', '205', '206', 'FLPR', '208', 'AWBR', '21', 'AVER', '212', '213', '214', '215', 'RMDVR', 'RMER', '218', 'ASER', '220', '221', '222', '223', '224', '225', '226', '227'] {}\n",
      "len. Ca recording 4454, total num. neurons 114, num. ID'd neurons 0\n",
      "\n",
      "0 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Number of neurons in calcium dataset does not match number of recorded neurons.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[60], line 122\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[39mprint\u001B[39m(\u001B[39mlen\u001B[39m(data_dict[worm][\u001B[39m\"\u001B[39m\u001B[39mneuron_to_idx\u001B[39m\u001B[39m\"\u001B[39m]), \u001B[39mlen\u001B[39m(data_dict[worm][\u001B[39m\"\u001B[39m\u001B[39midx_to_neuron\u001B[39m\u001B[39m\"\u001B[39m]))\n\u001B[1;32m    121\u001B[0m \u001B[39m# standardize the shape of calcium data to 302 x time\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m data_dict[worm] \u001B[39m=\u001B[39m reshape_calcium_data(data_dict[worm])\n",
      "File \u001B[0;32m~/Desktop/MetaConscious/worm-graph/preprocess/_utils.py:449\u001B[0m, in \u001B[0;36mreshape_calcium_data\u001B[0;34m(single_worm_dataset)\u001B[0m\n\u001B[1;32m    447\u001B[0m neurons_302 \u001B[39m=\u001B[39m NEURONS_302\n\u001B[1;32m    448\u001B[0m \u001B[39m# check the calcium data\u001B[39;00m\n\u001B[0;32m--> 449\u001B[0m \u001B[39massert\u001B[39;00m \u001B[39mlen\u001B[39m(idx_to_neuron) \u001B[39m==\u001B[39m origin_calcium_data\u001B[39m.\u001B[39msize(\n\u001B[1;32m    450\u001B[0m     \u001B[39m1\u001B[39m\n\u001B[1;32m    451\u001B[0m ), \u001B[39m\"\u001B[39m\u001B[39mNumber of neurons in calcium dataset does not match number of recorded neurons.\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[1;32m    452\u001B[0m \u001B[39m# create new maps of neurons to indices\u001B[39;00m\n\u001B[1;32m    453\u001B[0m named_neuron_to_idx \u001B[39m=\u001B[39m \u001B[39mdict\u001B[39m()\n",
      "\u001B[0;31mAssertionError\u001B[0m: Number of neurons in calcium dataset does not match number of recorded neurons."
     ]
    }
   ],
   "source": [
    "import math\n",
    "def str_to_float(str_num):\n",
    "    '''\n",
    "    Change textual scientific notation into a floating-point number\n",
    "    '''\n",
    "    before_e = float(str_num.split('e')[0])\n",
    "    sign = str_num.split('e')[1][:1]\n",
    "    after_e = int(str_num.split('e')[1][1:])\n",
    "\n",
    "    if sign == '+':\n",
    "        float_num = before_e * math.pow(10, after_e)\n",
    "    elif sign == '-':\n",
    "        float_num = before_e * math.pow(10, -after_e)\n",
    "    else:\n",
    "        float_num = None\n",
    "        print('error: unknown sign')\n",
    "    return float_num\n",
    "\n",
    "# calculate the number of worms\n",
    "files = os.listdir(\"./\" + filename)\n",
    "num = int(len(files) / 6) # every worm has 6 txt files\n",
    "data_dict = {}\n",
    "\n",
    "for i in range(0, 1):\n",
    "    worm = \"worm\" + str(i)\n",
    "\n",
    "    real_data = []\n",
    "    with open(filename + \"/\" + str(i) + \"_gcamp.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            cal = list(map(float, line.split(\" \")))\n",
    "            real_data.append(cal)\n",
    "    real_data = np.array(real_data) # format: (time, neuron)\n",
    "\n",
    "    sc = transform  # normalize data\n",
    "    real_data = sc.fit_transform(real_data)\n",
    "\n",
    "    delta = (real_data[:, 83] - real_data[:, 82])\n",
    "    \n",
    "    print(f'mean:{delta.mean()}\\nshape:{delta.shape}\\nmin:{delta.min()}\\nmax:{delta.max()}')\n",
    "    \n",
    "    label_list = []\n",
    "    with open(\"./\" + filename + \"/\" + str(i) + \"_labels.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.strip(\"\\n\")\n",
    "            label_list.append(l)\n",
    "\n",
    "   \n",
    "    for j, val in enumerate(label_list):\n",
    "        if val == '' or val == 'smthng else':\n",
    "            label_list[j] = str(j+ len(label_list))\n",
    "\n",
    "    neuron_to_idx = {}\n",
    "    neurons_copy = []\n",
    "    for neuron in label_list:\n",
    "        if neuron not in set(neurons_copy) and label_list.count(neuron) > 1:\n",
    "            neurons_copy.append(neuron + \"L\")\n",
    "    \n",
    "    print(neurons_copy)\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"---\", len(label_list), real_data.shape)\n",
    "    print(\"------\", len(neuron_to_idx))\n",
    "    print(label_list, neuron_to_idx)\n",
    "\n",
    "\n",
    "    num_named = len(neuron_to_idx)\n",
    "\n",
    "    sc = transform  # normalize data\n",
    "    real_data = sc.fit_transform(real_data)\n",
    "    real_data = torch.tensor(\n",
    "            real_data, dtype=torch.float32\n",
    "        )  # add a feature dimension and convert to tensor\n",
    "    smooth_real_data, residual, smooth_residual = smooth_data_preprocess(\n",
    "        real_data, smooth_method\n",
    "    )\n",
    "    \n",
    "    timeVectorSeconds = []\n",
    "    with open(\"./\" + filename + \"/\" + str(i) + \"_t.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            l = line.strip(\"\\n\")\n",
    "            timeVectorSeconds.append(str_to_float(l))\n",
    "    \n",
    "    time_in_seconds = np.array(timeVectorSeconds)\n",
    "    time_in_seconds = torch.tensor(time_in_seconds).to(torch.float32)\n",
    "    dt = torch.zeros_like(time_in_seconds).to(torch.float32)\n",
    "    dt[1:] = time_in_seconds[1:] - time_in_seconds[:-1]\n",
    "\n",
    "    num_neurons = real_data.shape[1]\n",
    "    max_time = real_data.shape[0]\n",
    "\n",
    "    print(\n",
    "            \"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"\n",
    "            % (max_time, num_neurons, num_named),\n",
    "            end=\"\\n\\n\",\n",
    "    )\n",
    "\n",
    "    data_dict.update(\n",
    "            {\n",
    "                worm: {\n",
    "                    \"dataset\": \"Leifer2023\",\n",
    "                    \"worm\": worm,\n",
    "                    \"calcium_data\": real_data,\n",
    "                    \"smooth_calcium_data\": smooth_real_data,\n",
    "                    \"residual_calcium\": residual,\n",
    "                    \"smooth_residual_calcium\": smooth_residual,\n",
    "                    \"neuron_to_idx\": neuron_to_idx,\n",
    "                    \"idx_to_neuron\": dict((v, k) for k, v in neuron_to_idx.items()),\n",
    "                    \"max_time\": int(max_time),\n",
    "                    \"time_in_seconds\": time_in_seconds,\n",
    "                    \"dt\": dt,\n",
    "                    \"num_neurons\": int(num_neurons),\n",
    "                    \"num_named_neurons\": num_named,\n",
    "                    \"num_unknown_neurons\": int(num_neurons) - num_named,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(len(data_dict[worm][\"neuron_to_idx\"]), len(data_dict[worm][\"idx_to_neuron\"]))\n",
    "    \n",
    "    # standardize the shape of calcium data to 302 x time\n",
    "    data_dict[worm] = reshape_calcium_data(data_dict[worm])\n",
    "    \n",
    "    # # pickle the data\n",
    "    # file = os.path.join(processed_path, \"Leifer2023.pickle\")\n",
    "    # pickle_out = open(file, \"wb\")\n",
    "    # pickle.dump(data_dict, pickle_out)\n",
    "    # pickle_out.close()\n",
    "    # pickle_in = open(file, \"rb\")\n",
    "    # Leifer2023 = pickle.load(pickle_in)\n",
    "    # print(Leifer2023.keys(), end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "#     cal_noNull = []\n",
    "#     l_noNull = []\n",
    "#     # subtract data without label\n",
    "#     for j in range(0, len(label_list)):\n",
    "#         if label_list[j] != \"\":\n",
    "#             cal_noNull.append(real_data[j])\n",
    "#             l_noNull.append(label_list[j])\n",
    "\n",
    "#     cal_noNull_array = np.array(cal_noNull)\n",
    "#     gcamp_np.append(dict(zip(l_noNull, cal_noNull_array)))\n",
    "# print(\"------data reformatted------\")\n",
    "# print(str(num) + \" txt have been reformatted.\")\n",
    "\n",
    "# file = open(\"gcamp.pickle\", \"wb\")\n",
    "# pickle.dump(gcamp_np, file)\n",
    "# file.close()\n",
    "# print(\"------save to Pickle------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“k0”",
   "language": "python",
   "name": "k0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
