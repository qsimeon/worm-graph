{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reproducing some results of Quilee's notebook in _CElegansNeuralPrediction.ipynb_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing train/_main.py\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the dataset returned after running `get_dataset()` doesn't have the \"worm\" keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports for train module\n",
    "import torch\n",
    "import os\n",
    "import hydra\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from scipy.linalg import solve\n",
    "from typing import Tuple, Union\n",
    "from datetime import datetime\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf\n",
    "from utils import DEVICE, LOGS_DIR, NEURONS_302\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from models._utils import NetworkLSTM\n",
    "from data._utils import NeuralActivityDataset, pick_worm\n",
    "from data._main import get_dataset\n",
    "from models._main import get_model\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: train:\n",
      "  optimizer: SGD\n",
      "  learn_rate: 0.01\n",
      "  epochs: 1\n",
      "  save_freq: 100\n",
      "  seq_len: 100\n",
      "  k_splits: 2\n",
      "  num_samples: 16\n",
      "  num_batches: 1\n",
      "  tau_in: 1\n",
      "  shuffle: false\n",
      "  reverse: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load(\"../../conf/train.yaml\")\n",
    "print(\"config:\", OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized a new model.\n",
      "\n",
      "Model: LinearNN(\n",
      "  (identity): Identity()\n",
      "  (linear): Linear(in_features=512, out_features=302, bias=True)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=302, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Linear(in_features=512, out_features=302, bias=True)\n",
      "  )\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_model(OmegaConf.load(\"../../conf/model.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen dataset(s): ['Kaplan2020', 'Kato2015', 'Nichols2017', 'Skora2018']\n",
      "Num. worms: 87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(OmegaConf.load(\"../../conf/dataset.yaml\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['worm0', 'worm1', 'worm2', 'worm3', 'worm4', 'worm5', 'worm6', 'worm7', 'worm8', 'worm9', 'worm10', 'worm11', 'worm12', 'worm13', 'worm14', 'worm15', 'worm16', 'worm17', 'worm18', 'worm19', 'worm20', 'worm21', 'worm22', 'worm23', 'worm24', 'worm25', 'worm26', 'worm27', 'worm28', 'worm29', 'worm30', 'worm31', 'worm32', 'worm33', 'worm34', 'worm35', 'worm36', 'worm37', 'worm38', 'worm39', 'worm40', 'worm41', 'worm42', 'worm43', 'worm44', 'worm45', 'worm46', 'worm47', 'worm48', 'worm49', 'worm50', 'worm51', 'worm52', 'worm53', 'worm54', 'worm55', 'worm56', 'worm57', 'worm58', 'worm59', 'worm60', 'worm61', 'worm62', 'worm63', 'worm64', 'worm65', 'worm66', 'worm67', 'worm68', 'worm69', 'worm70', 'worm71', 'worm72', 'worm73', 'worm74', 'worm75', 'worm76', 'worm77', 'worm78', 'worm79', 'worm80', 'worm81', 'worm82', 'worm83', 'worm84', 'worm85', 'worm86'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for ds in dataset:\n",
    "    features.append(list(dataset[ds].keys()))\n",
    "\n",
    "features = np.unique(np.concatenate(features))\n",
    "\n",
    "for ds in dataset:\n",
    "    for f in features:\n",
    "        if f not in dataset[ds].keys():\n",
    "            print(dataset[ds]['dataset'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets have the same features (verifying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['calcium_data', 'dataset', 'dt', 'max_timesteps',\n",
       "       'named_neuron_to_slot', 'named_neurons_mask', 'neuron_to_slot',\n",
       "       'neurons_mask', 'num_named_neurons', 'num_neurons',\n",
       "       'num_unknown_neurons', 'residual_calcium', 'slot_to_named_neuron',\n",
       "       'slot_to_neuron', 'slot_to_unknown_neuron', 'smooth_calcium_data',\n",
       "       'smooth_method', 'smooth_residual_calcium', 'time_in_seconds',\n",
       "       'unknown_neuron_to_slot', 'unknown_neurons_mask', 'worm'],\n",
       "      dtype='<U23')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Datasets\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of calcium imaging data from our worm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Paper Link             |Database Link     |Files w/ Data          |Num. ID'd| \n",
    "|-----------------------|------------------|-----------------------|---------|\n",
    "|tinyurl.com/Uzel2022   |osf.io/3vkxn/     |`Uzel_WT.mat`          |54/154   |\n",
    "|tinyurl.com/Kaplan20   |osf.io/9nfhz/     |`Neuron2019_Data_*.mat`|48/103   |\n",
    "|tinyurl.com/Nguyen17   |tinyurl.com/LeiferIEEE|`heatData*.mat`    |0/156    | \n",
    "|tinyurl.com/Skora2018  |osf.io/za3gt/     |`WT_*.mat`             |40/139   |\n",
    "|tinyurl.com/Nichols2017|osf.io/kbf38/     |`*let.mat`             |35/116   |\n",
    "|tinyurl.com/Kato2015   |osf.io/2395t/     |`WT_*Stim.mat`         |38/109   |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the datasets we can use the functions in `data/_utilis.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data._utils import load_dataset, pick_worm, find_reliable_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['worm0', 'worm1', 'worm2'])\n",
      "dict_keys(['dataset', 'smooth_method', 'worm', 'calcium_data', 'smooth_calcium_data', 'residual_calcium', 'smooth_residual_calcium', 'max_timesteps', 'time_in_seconds', 'dt', 'num_neurons', 'num_named_neurons', 'num_unknown_neurons', 'named_neurons_mask', 'unknown_neurons_mask', 'neurons_mask', 'slot_to_named_neuron', 'named_neuron_to_slot', 'slot_to_unknown_neuron', 'unknown_neuron_to_slot', 'slot_to_neuron', 'neuron_to_slot'])\n",
      "torch.Size([3044, 302])\n"
     ]
    }
   ],
   "source": [
    "# load a dataset\n",
    "Nguyen2017 = load_dataset(\"Nguyen2017\")\n",
    "print(Nguyen2017.keys())\n",
    "print(Nguyen2017['worm0'].keys())\n",
    "print(Nguyen2017['worm0']['calcium_data'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a hierarchy to access the data of each worm in the dataset:\n",
    " - First the choosen worm\n",
    " - Second the dataset itself with its features and other informations (note that the key `dataset` corresponds to the name of the dataset and not to the raw data or something like that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'worm2' in set(Nguyen2017.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m wormid \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39mlist\u001b[39m(Nguyen2017\u001b[39m.\u001b[39mkeys()))\n\u001b[0;32m----> 2\u001b[0m single_worm_dataset \u001b[39m=\u001b[39m pick_worm(Nguyen2017, wormid)\n",
      "File \u001b[0;32m~/projects/worm-graph/data/_utils.py:276\u001b[0m, in \u001b[0;36mpick_worm\u001b[0;34m(dataset, wormid)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(worm, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m worm \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m    273\u001b[0m         avail_worms\n\u001b[1;32m    274\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mChoose a worm from: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(avail_worms)\n\u001b[1;32m    275\u001b[0m     worm \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mworm\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(wormid)\n\u001b[0;32m--> 276\u001b[0m single_worm_dataset \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dataset[\u001b[39m\"\u001b[39m\u001b[39mgenerator\u001b[39m\u001b[39m\"\u001b[39m])[worm]\n\u001b[1;32m    277\u001b[0m \u001b[39mreturn\u001b[39;00m single_worm_dataset\n",
      "\u001b[0;31mKeyError\u001b[0m: 'generator'"
     ]
    }
   ],
   "source": [
    "wormid = np.random.choice(list(Nguyen2017.keys()))\n",
    "single_worm_dataset = pick_worm(Nguyen2017, wormid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worm-graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
