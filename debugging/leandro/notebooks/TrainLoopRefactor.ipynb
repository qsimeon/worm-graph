{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from data._main import get_dataset\n",
    "from omegaconf import OmegaConf\n",
    "from utils import NEURONS_302"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  train:\n",
      "    name:\n",
      "    - Kato2015\n",
      "    - Nichols2017\n",
      "    num_named_neurons: 5\n",
      "    num_worms: 10\n",
      "    save: false\n",
      "  predict:\n",
      "    name: Kato2015\n",
      "    num_named_neurons: 1\n",
      "    num_worms: 1\n",
      "    save: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_config = OmegaConf.load(\"../../../configs/submodule/dataset.yaml\")\n",
    "dataset_config.dataset.train.name = ['Kato2015', 'Nichols2017']\n",
    "dataset_config.dataset.train.num_worms = 10\n",
    "# Print config\n",
    "print(OmegaConf.to_yaml(dataset_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = get_dataset(dataset_config=dataset_config.dataset.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = {\n",
    "    'dataset': [],\n",
    "    'original_index': [],\n",
    "    'combined_dataset_index': [],\n",
    "    'neurons': [],\n",
    "}\n",
    "\n",
    "combined_dataset_neurons = []\n",
    "\n",
    "for worm, data in combined_dataset.items():\n",
    "    dataset_info['dataset'].append(data['dataset'])\n",
    "    dataset_info['original_index'].append(data['original_worm'])\n",
    "    dataset_info['combined_dataset_index'].append(data['worm'])\n",
    "    worm_neurons = [neuron for slot, neuron in data['slot_to_named_neuron'].items()]\n",
    "    dataset_info['neurons'].append(worm_neurons)\n",
    "    combined_dataset_neurons = combined_dataset_neurons + worm_neurons\n",
    "\n",
    "dataset_info = pd.DataFrame(dataset_info)\n",
    "\n",
    "combined_dataset_neurons = np.array(combined_dataset_neurons)\n",
    "# Count occurernces of each neuron\n",
    "neuron_counts = np.unique(combined_dataset_neurons, return_counts=True)\n",
    "# Sort by neuron count\n",
    "neuron_counts = np.array(sorted(zip(*neuron_counts), key=lambda x: x[1], reverse=True))\n",
    "# Create a dataframe\n",
    "neuron_counts = pd.DataFrame(neuron_counts, columns=[\"neuron\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train._utils import split_train_test\n",
    "from data._utils import NeuralActivityDataset, create_combined_dataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from omegaconf import OmegaConf, DictConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split (combined) dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_samples(data_splits, total_nb_samples):\n",
    "    # Calculate the base number of samples for each split\n",
    "    base_samples_per_split = total_nb_samples // len(data_splits)\n",
    "    # Calculate the remainder\n",
    "    remainder = total_nb_samples % len(data_splits)\n",
    "\n",
    "    samples_to_take = []\n",
    "    \n",
    "    # Distribute the samples\n",
    "    for i in range(len(data_splits)):\n",
    "        if i < remainder:\n",
    "            samples_to_take.append(base_samples_per_split + 1)\n",
    "        else:\n",
    "            samples_to_take.append(base_samples_per_split)\n",
    "\n",
    "    return samples_to_take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_combined_dataset(combined_dataset, k_splits, num_train_samples, num_val_samples, seq_len, tau, reverse, use_residual, smooth_data):\n",
    "\n",
    "    # Choose whether to use calcium or residual data\n",
    "    if use_residual:\n",
    "        key_data = \"residual_calcium\"\n",
    "    else:\n",
    "        key_data = \"calcium_data\"\n",
    "\n",
    "    # Choose whether to use original or smoothed data\n",
    "    if smooth_data:\n",
    "        key_data = \"smooth_\" + key_data\n",
    "    else:\n",
    "        key_data = key_data\n",
    "\n",
    "    # Store the training and validation datasets\n",
    "    train_dataset = []\n",
    "    val_dataset = []\n",
    "\n",
    "    # Loop through the worms in the dataset\n",
    "    for wormID, single_worm_dataset in combined_dataset.items():\n",
    "\n",
    "        # Extract relevant features from the dataset\n",
    "        data = single_worm_dataset[key_data]\n",
    "        neurons_mask = single_worm_dataset[\"named_neurons_mask\"]\n",
    "        time_vec = single_worm_dataset[\"time_in_seconds\"]\n",
    "\n",
    "        # Verifications\n",
    "        assert isinstance(seq_len, int) and 0 < seq_len < len(data), \"seq_len must be an integer > 0 and < len(data)\"\n",
    "        assert isinstance(tau, int) and tau < (len(data) - seq_len), \"The desired tau is too long. Try a smaller value\"\n",
    "        assert seq_len < (len(data) // k_splits), \"The desired seq_len is too long. Try a smaller seq_len or decreasing k_splits\"\n",
    "\n",
    "        # Split the data and the time vector into k splits\n",
    "        data_splits = np.array_split(data, k_splits)\n",
    "        time_vec_splits = np.array_split(time_vec, k_splits)\n",
    "\n",
    "        # Separate the splits into training and validation sets\n",
    "        train_data_splits = data_splits[::2]\n",
    "        train_time_vec_splits = time_vec_splits[::2]\n",
    "        val_data_splits = data_splits[1::2]\n",
    "        val_time_vec_splits = time_vec_splits[1::2]\n",
    "\n",
    "        # Number of total time steps in each split\n",
    "        total_train_time_steps = np.sum([len(split) for split in train_data_splits])\n",
    "        total_val_time_steps = np.sum([len(split) for split in val_data_splits])\n",
    "\n",
    "        # Number of samples in each split\n",
    "        train_samples_per_split = distribute_samples(train_data_splits, num_train_samples)\n",
    "        val_samples_per_split = distribute_samples(val_data_splits, num_val_samples)\n",
    "\n",
    "        # Create a dataset for each split\n",
    "        for train_split, train_time_split, num_samples_split in zip(train_data_splits, train_time_vec_splits, train_samples_per_split):\n",
    "            train_dataset.append(\n",
    "                NeuralActivityDataset(\n",
    "                    data = train_split.detach(),\n",
    "                    time_vec = train_time_split.detach(),\n",
    "                    neurons_mask = neurons_mask,\n",
    "                    seq_len = seq_len,\n",
    "                    num_samples = num_samples_split,\n",
    "                    tau = tau,\n",
    "                    use_residual = use_residual,\n",
    "                    reverse = reverse,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        \n",
    "        for val_split, val_time_split, num_samples_split in zip(val_data_splits, val_time_vec_splits, val_samples_per_split):\n",
    "            val_dataset.append(\n",
    "                NeuralActivityDataset(\n",
    "                    data = val_split.detach(),\n",
    "                    time_vec = val_time_split.detach(),\n",
    "                    neurons_mask = neurons_mask,\n",
    "                    seq_len = seq_len,\n",
    "                    num_samples = num_samples_split,\n",
    "                    tau = tau,\n",
    "                    use_residual = use_residual,\n",
    "                    reverse = reverse,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Concatenate the datasets\n",
    "    train_dataset = torch.utils.data.ConcatDataset(train_dataset) # Nb of train examples = nb train samples * nb of worms\n",
    "    val_dataset = torch.utils.data.ConcatDataset(val_dataset) # Nb of val examples = nb train samples * nb of worms\n",
    "\n",
    "    # Save the datasets\n",
    "    torch.save(train_dataset, \"train_dataset.pth\")\n",
    "    torch.save(val_dataset, \"val_dataset.pth\")\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experimental_datasets:\n",
      "- Kato2015\n",
      "- Nichols2017\n",
      "num_named_neurons: 2\n",
      "num_worms: 10\n",
      "k_splits: 2\n",
      "num_train_samples: 16\n",
      "num_val_samples: 4\n",
      "seq_len: 120\n",
      "tau: 1\n",
      "reverse: false\n",
      "use_residual: false\n",
      "smooth_data: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_config = OmegaConf.load(\"../../../configs/submodule/dataset_new.yaml\")\n",
    "print(OmegaConf.to_yaml(dataset_config.dataset.for_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(dataset_config: DictConfig, name='train'):\n",
    "\n",
    "    experimental_datasets = dataset_config.experimental_datasets\n",
    "    num_named_neurons = dataset_config.num_named_neurons\n",
    "    num_worms = dataset_config.num_worms\n",
    "    k_splits = dataset_config.k_splits\n",
    "    num_train_samples = dataset_config.num_train_samples\n",
    "    num_val_samples = dataset_config.num_val_samples\n",
    "    seq_len = dataset_config.seq_len\n",
    "    tau = dataset_config.tau\n",
    "    reverse = dataset_config.reverse\n",
    "    use_residual = dataset_config.use_residual\n",
    "    smooth_data = dataset_config.smooth_data\n",
    "\n",
    "    # Verifications\n",
    "    assert isinstance(k_splits, int) and k_splits > 1, \"k_splits must be an integer > 1\"\n",
    "\n",
    "    assert isinstance(num_named_neurons, int) or num_named_neurons == \"all\", (\n",
    "        \"num_named_neurons must be a positive integer or 'all'.\"\n",
    "    )\n",
    "\n",
    "    assert isinstance(num_worms, int) or num_worms == \"all\", (\n",
    "        \"num_worms must be a positive integer or 'all'.\"\n",
    "    )\n",
    "\n",
    "    combined_dataset = create_combined_dataset(experimental_datasets, num_named_neurons, num_worms, name=name)\n",
    "    train_dataset, val_dataset = split_combined_dataset(combined_dataset, k_splits, num_train_samples,\n",
    "                                                         num_val_samples, seq_len, tau, reverse,\n",
    "                                                         use_residual, smooth_data)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = get_datasets(dataset_config.dataset.for_training, name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1,m1,m = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x1[1:,:] == y1[:-1,:]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = split_combined_dataset(\n",
    "    combined_dataset = combined_dataset,\n",
    "    k_splits = 2,\n",
    "    num_train_samples = 16,\n",
    "    num_val_samples = 8,\n",
    "    seq_len = 100,\n",
    "    tau = 1,\n",
    "    reverse = False,\n",
    "    use_residual = False,\n",
    "    smooth_data = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 16,\n",
    "    shuffle = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 100, 302]) torch.Size([16, 100, 302]) torch.Size([16, 302])\n",
      "torch.Size([16, 100, 302]) torch.Size([16, 100, 302]) torch.Size([16, 302])\n",
      "torch.Size([16, 100, 302]) torch.Size([16, 100, 302]) torch.Size([16, 302])\n",
      "torch.Size([16, 100, 302]) torch.Size([16, 100, 302]) torch.Size([16, 302])\n",
      "torch.Size([16, 100, 302]) torch.Size([16, 100, 302]) torch.Size([16, 302])\n",
      "torch.Size([16, 100, 302]) torch.Size([16, 100, 302]) torch.Size([16, 302])\n",
      "torch.Size([16, 100, 302]) torch.Size([16, 100, 302]) torch.Size([16, 302])\n",
      "torch.Size([16, 100, 302]) torch.Size([16, 100, 302]) torch.Size([16, 302])\n",
      "torch.Size([16, 100, 302]) torch.Size([16, 100, 302]) torch.Size([16, 302])\n",
      "torch.Size([16, 100, 302]) torch.Size([16, 100, 302]) torch.Size([16, 302])\n"
     ]
    }
   ],
   "source": [
    "for x, y, mask, _ in trainloader:\n",
    "    print(x.shape, y.shape, mask.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worm-graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
