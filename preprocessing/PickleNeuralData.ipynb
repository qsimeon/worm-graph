{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import download_url, extract_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import mat73\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/9dnzrlh12hf5p89/opensource_data.zip?dl=1\n",
      "Extracting opensource_data.zip\n"
     ]
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "url = 'https://www.dropbox.com/s/9dnzrlh12hf5p89/opensource_data.zip?dl=1'\n",
    "filename = os.path.join('opensource_data.zip')\n",
    "data_path = os.path.join(os.getcwd(), 'opensource_data')\n",
    "\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "if not os.path.exists(data_path):\n",
    "    download_url(url=url, folder=os.getcwd(), filename=filename)\n",
    "    extract_zip(filename, folder=data_path) # extract zip file\n",
    "    os.unlink(filename) # remove zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__version__', '__globals__', 'hasPointsTime', 'ethoTrack', 'R2', 'G2', 'Ratio2', 'acorr', 'cgIdx', 'cgIdxRev', 'DmatAll']\n",
      "\n",
      "len. Ca recording 1516, num. neurons 77\n",
      "\n",
      "['__header__', '__version__', '__globals__', 'hasPointsTime', 'ethoTrack', 'R2', 'G2', 'Ratio2', 'acorr', 'cgIdx', 'cgIdxRev', 'rRaw', 'gRaw', 'rPhotoCorr', 'gPhotoCorr']\n",
      "\n",
      "len. Ca recording 2849, num. neurons 156\n",
      "\n",
      "dict_keys(['worm1', 'worm2'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Nguyen et al., PLOS CompBio 2017, *Automatically tracking neurons in a moving and deforming brain* \n",
    "\n",
    "# WORM 1\n",
    "# load .mat file for  worm 1\n",
    "arr1 = loadmat(os.path.join(data_path, 'Nguyen2017', 'heatData_worm1.mat')) # load .mat file\n",
    "print(list(arr1.keys()))\n",
    "print()\n",
    "# get data for worm 1\n",
    "G2 = arr1['G2'] # the ratio signal is defined as gPhotoCorr/rPhotoCorr, the Ratio is then normalized as delta R/ R0. is the same way as R2 and G2.\n",
    "cgIdx = arr1['cgIdx'].squeeze() # ordered indices derived from heirarchically clustering the correlation matrix. \n",
    "real_data1 = G2[cgIdx-1, :].T # to show organized traces, use Ratio2(cgIdx,:)\n",
    "real_data1 = np.nan_to_num(real_data1) # replace NaNs \n",
    "max_time1, num_neurons1 = real_data1.shape \n",
    "worm1_IDs = {i: str(i) for i in range(num_neurons1)}\n",
    "print('len. Ca recording %s, num. neurons %s'%(max_time1, num_neurons1))\n",
    "print()\n",
    "# normalize the data \n",
    "sc = preprocessing.MinMaxScaler()\n",
    "real_data1 = sc.fit_transform(real_data1[:, :num_neurons1]) \n",
    "\n",
    "# WORM 2\n",
    "# load .mat file for  worm 1\n",
    "arr2 = loadmat(os.path.join(data_path, 'Nguyen2017', 'heatData_worm2.mat')) # load .mat file\n",
    "print(list(arr2.keys()))\n",
    "print()\n",
    "# get data for worm 2\n",
    "G2 = arr2['G2'] # the ratio signal is defined as gPhotoCorr/rPhotoCorr, the Ratio is then normalized as delta R/ R0. is the same way as R2 and G2.\n",
    "cgIdx = arr2['cgIdx'].squeeze() # ordered indices derived from heirarchically clustering the correlation matrix. \n",
    "real_data2 = G2[cgIdx-1, :].T # to show organized traces, use Ratio2(cgIdx,:)\n",
    "real_data2 = np.nan_to_num(real_data2) # replace NaNs \n",
    "max_time2, num_neurons2 = real_data2.shape \n",
    "worm2_IDs = {i+1: str(i+1) for i in range(num_neurons2)}\n",
    "print('len. Ca recording %s, num. neurons %s'%(max_time2, num_neurons2))\n",
    "print()\n",
    "# normalize the data \n",
    "sc = preprocessing.MinMaxScaler()\n",
    "real_data2 = sc.fit_transform(real_data2[:, :num_neurons2]) \n",
    "\n",
    "# pickle the data\n",
    "data_dict = {'worm1': {'data': real_data1, 'neuron_ids': worm1_IDs, 'max_time': max_time1, 'num_neurons': num_neurons1}, \n",
    "             'worm2': {'data': real_data2, 'neuron_ids': worm2_IDs, 'max_time': max_time2, 'num_neurons': num_neurons2},\n",
    "            }\n",
    "file = os.path.join(root, \"Nguyen2017.pickle\")\n",
    "pickle_out = open(file, \"wb\")\n",
    "pickle.dump(data_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_in = open(file, \"rb\")\n",
    "Nguyen2017 = pickle.load(pickle_in)\n",
    "print(Nguyen2017.keys())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['five_state_annotation', 'neuron_ID', 'parameters', 'peaks', 'raw_traces', 'time_vector', 'traces_bleach_corrected', 'traces_bleach_corrected_detrended', 'traces_derivatives', 'volumes_per_second']\n",
      "\n",
      "num. worms: 10\n",
      "\n",
      "len. Ca recording 5455, total num. neurons 103, num. ID'd neurons 103\n",
      "len. Ca recording 5455, total num. neurons 129, num. ID'd neurons 129\n",
      "len. Ca recording 5455, total num. neurons 122, num. ID'd neurons 122\n",
      "len. Ca recording 5455, total num. neurons 119, num. ID'd neurons 119\n",
      "len. Ca recording 5455, total num. neurons 124, num. ID'd neurons 124\n",
      "len. Ca recording 3578, total num. neurons 111, num. ID'd neurons 111\n",
      "len. Ca recording 3269, total num. neurons 128, num. ID'd neurons 128\n",
      "len. Ca recording 2941, total num. neurons 127, num. ID'd neurons 127\n",
      "len. Ca recording 4010, total num. neurons 129, num. ID'd neurons 129\n",
      "len. Ca recording 3692, total num. neurons 124, num. ID'd neurons 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['five_state_annotation', 'neuron_ID', 'parameters', 'peaks', 'raw_traces', 'time_vector', 'traces_bleach_corrected', 'traces_bleach_corrected_detrended', 'traces_derivatives', 'volumes_per_second']\n",
      "\n",
      "num. worms: 4\n",
      "\n",
      "len. Ca recording 2812, total num. neurons 102, num. ID'd neurons 102\n",
      "len. Ca recording 2917, total num. neurons 99, num. ID'd neurons 99\n",
      "len. Ca recording 3163, total num. neurons 107, num. ID'd neurons 107\n",
      "len. Ca recording 2795, total num. neurons 97, num. ID'd neurons 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['five_state_annotation', 'neuron_ID', 'parameters', 'peaks', 'raw_traces', 'time_vector', 'traces_bleach_corrected', 'traces_bleach_corrected_detrended', 'traces_derivatives', 'volumes_per_second']\n",
      "\n",
      "num. worms: 5\n",
      "\n",
      "len. Ca recording 3805, total num. neurons 105, num. ID'd neurons 105\n",
      "len. Ca recording 3803, total num. neurons 114, num. ID'd neurons 114\n",
      "len. Ca recording 3657, total num. neurons 123, num. ID'd neurons 123\n",
      "len. Ca recording 2808, total num. neurons 97, num. ID'd neurons 97\n",
      "len. Ca recording 3157, total num. neurons 117, num. ID'd neurons 117\n",
      "dict_keys(['worm1', 'worm2', 'worm3', 'worm4', 'worm5', 'worm6', 'worm7', 'worm8', 'worm9', 'worm10', 'worm11', 'worm12', 'worm13', 'worm14', 'worm15', 'worm16', 'worm17', 'worm18', 'worm19'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Kaplan et al., Neuron 2020, *Nested Neuronal Dynamics Orchestrate a Behavioral Hierarchy across Timescales*\n",
    "\n",
    "data_dict = dict()\n",
    "\n",
    "# 'RIShisCl_Neuron2019'\n",
    "# load the first .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Kaplan2020', 'Neuron2019_Data_RIShisCl.mat'))['RIShisCl_Neuron2019']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['neuron_ID'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces_bleach_corrected'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "\n",
    "for i, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(i+1)\n",
    "    neuron_IDs = {nid+1: str(j) for nid, j in enumerate(all_IDs[i])} \n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, len(neuron_IDs)))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons},\n",
    "                     })\n",
    "    \n",
    "# 'MNhisCl_RIShisCl_Neuron2019'\n",
    "# load the second .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Kaplan2020', 'Neuron2019_Data_MNhisCl_RIShisCl.mat'))['MNhisCl_RIShisCl_Neuron2019']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['neuron_ID'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces_bleach_corrected'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "for ii, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(ii+1 + i+1)\n",
    "    neuron_IDs = {nid+1: str(j) for nid, j in enumerate(all_IDs[ii])} \n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, len(neuron_IDs)))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons},\n",
    "                     })\n",
    "    \n",
    "\n",
    "# 'MNhisCl_RIShisCl_Neuron2019'\n",
    "# load the third .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Kaplan2020', 'Neuron2019_Data_SMDhisCl_RIShisCl.mat'))['SMDhisCl_RIShisCl_Neuron2019']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['neuron_ID'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces_bleach_corrected'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "for iii, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(iii+1 + ii+1 + i+1)\n",
    "    neuron_IDs = {nid+1: str(j) for nid, j in enumerate(all_IDs[iii])} \n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, len(neuron_IDs)))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons},\n",
    "                     })\n",
    "\n",
    "    \n",
    "# pickle the data\n",
    "file = os.path.join(root, \"Kaplan2020.pickle\")\n",
    "pickle_out = open(file, \"wb\")\n",
    "pickle.dump(data_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_in = open(file, \"rb\")\n",
    "Kaplan2020 = pickle.load(pickle_in)\n",
    "print(Kaplan2020.keys())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IDs', 'derivatives', 'fps', 'options', 'states', 'statesKey', 'traces', 'tv']\n",
      "\n",
      "num. worms: 6\n",
      "\n",
      "len. Ca recording 3312, total num. neurons 154, num. ID'd neurons 154\n",
      "len. Ca recording 3749, total num. neurons 154, num. ID'd neurons 154\n",
      "len. Ca recording 4126, total num. neurons 133, num. ID'd neurons 133\n",
      "len. Ca recording 5450, total num. neurons 124, num. ID'd neurons 124\n",
      "len. Ca recording 3313, total num. neurons 136, num. ID'd neurons 136\n",
      "len. Ca recording 3311, total num. neurons 133, num. ID'd neurons 133\n",
      "dict_keys(['worm1', 'worm2', 'worm3', 'worm4', 'worm5', 'worm6'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Uzel et al 2022., Cell CurrBio 2022, *A set of hub neurons and non-local connectivity features support global brain dynamics in C. elegans*\n",
    "\n",
    "data_dict = dict()\n",
    "\n",
    "# load .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Uzel2022', 'Uzel_WT.mat'))['Uzel_WT'] # load .mat file\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "\n",
    "# get data for all worms\n",
    "all_IDs = arr['IDs'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "for i, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(i+1)\n",
    "    i_IDs = [np.array(j).item() for j in all_IDs[i]]\n",
    "    neuron_IDs = {nid+1: (str(int(j)) if type(j)!=str else j) for nid,j  in enumerate(i_IDs)} \n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, len(neuron_IDs)))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons},\n",
    "                     })\n",
    "    \n",
    "# pickle the data\n",
    "file = os.path.join(root, \"Uzel2022.pickle\")\n",
    "pickle_out = open(file, \"wb\")\n",
    "pickle.dump(data_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_in = open(file, \"rb\")\n",
    "Uzel2022 = pickle.load(pickle_in)\n",
    "print(Uzel2022.keys())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Delete the downloaded raw datasets.\n",
    "#@markdown The files are too large to push to GitHub.\n",
    "shutil.rmtree(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "66fc48d1066461498fec04d710ecd27d9a00e22a5fbd94430ea07906109bd52e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
