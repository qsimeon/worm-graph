{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import download_url, extract_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import mat73\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should only be needed to run once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/5dv8ezn8ehjwyid/opensource_data.zip?dl=1\n",
      "Extracting opensource_data.zip\n"
     ]
    }
   ],
   "source": [
    "#@title Download the curated open-source worm datasets from host server\n",
    "#@markdown Downloading can take up to 8 minutes depending on your network speed!\n",
    "\n",
    "root = os.getcwd()\n",
    "url = 'https://www.dropbox.com/s/5dv8ezn8ehjwyid/opensource_data.zip?dl=1'\n",
    "filename = os.path.join('opensource_data.zip')\n",
    "data_path = os.path.join(os.getcwd(), 'opensource_data')\n",
    "\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "if not os.path.exists(data_path):\n",
    "    download_url(url=url, folder=os.getcwd(), filename=filename)\n",
    "    extract_zip(filename, folder=data_path) # extract zip file\n",
    "    os.unlink(filename) # remove zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IDs', 'States', 'dataset', 'fps', 'stimulus', 'timeVectorSeconds', 'traces', 'tracesDif', 'traces_raw']\n",
      "\n",
      "num. worms: 7\n",
      "\n",
      "len. Ca recording 2198, total num. neurons 107, num. ID'd neurons 43\n",
      "len. Ca recording 2017, total num. neurons 122, num. ID'd neurons 44\n",
      "len. Ca recording 2197, total num. neurons 124, num. ID'd neurons 37\n",
      "len. Ca recording 2018, total num. neurons 134, num. ID'd neurons 51\n",
      "len. Ca recording 2201, total num. neurons 123, num. ID'd neurons 48\n",
      "len. Ca recording 2017, total num. neurons 151, num. ID'd neurons 44\n",
      "len. Ca recording 2019, total num. neurons 146, num. ID'd neurons 51\n",
      "['NeuronNames', 'Opts', 'States', 'dataset', 'deltaFOverF', 'deltaFOverF_bc', 'derivs', 'fps', 'stateParams', 'tv']\n",
      "\n",
      "num. worms: 5\n",
      "\n",
      "len. Ca recording 3137, total num. neurons 109, num. ID'd neurons 38\n",
      "len. Ca recording 3134, total num. neurons 135, num. ID'd neurons 44\n",
      "len. Ca recording 3059, total num. neurons 131, num. ID'd neurons 32\n",
      "len. Ca recording 3311, total num. neurons 125, num. ID'd neurons 47\n",
      "len. Ca recording 3021, total num. neurons 129, num. ID'd neurons 31\n",
      "\n",
      "dict_keys(['worm1', 'worm2', 'worm3', 'worm4', 'worm5', 'worm6', 'worm7', 'worm8', 'worm9', 'worm10', 'worm11', 'worm12'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Kato et al., Cell Reports 2015, *Global Brain Dynamics Embed the Motor Command Sequence of Caenorhabditis elegans* \n",
    "\n",
    "data_dict = dict()\n",
    "\n",
    "# 'WT_Stim'\n",
    "# load the first .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Kato2015', 'WT_Stim.mat'))['WT_Stim']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['IDs'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "\n",
    "for i, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(i+1)\n",
    "    i_IDs = [(j[0] if isinstance(j,list) else j) for j in all_IDs[i]]\n",
    "    neuron_IDs = {nid+1: (str(nid+1) if (j is None or isinstance(j, np.ndarray)) else str(j)) for nid,j  in enumerate(i_IDs)} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, 'num_named': num_named},\n",
    "                     })\n",
    "\n",
    "# 'WT_NoStim'\n",
    "# load the second .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Kato2015', 'WT_NoStim.mat'))['WT_NoStim']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['NeuronNames'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['deltaFOverF_bc'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "\n",
    "for ii, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(ii+1 + i+1)\n",
    "    ii_IDs = [(j[0] if isinstance(j,list) else j) for j in all_IDs[ii]]\n",
    "    neuron_IDs = {nid+1: (str(nid+1) if (j is None or isinstance(j, np.ndarray)) else str(j)) for nid,j  in enumerate(ii_IDs)} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, 'num_named': num_named},\n",
    "                     })\n",
    "    \n",
    "\n",
    "# pickle the data\n",
    "file = os.path.join(root, \"Kato2015.pickle\")\n",
    "pickle_out = open(file, \"wb\")\n",
    "pickle.dump(data_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_in = open(file, \"rb\")\n",
    "Kato2015 = pickle.load(pickle_in)\n",
    "print()\n",
    "print(Kato2015.keys())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FiveStates', 'FourStates', 'IDs', 'dataset', 'fps', 'stimulus', 'timeVectorSeconds', 'traces', 'tracesDif']\n",
      "\n",
      "num. worms: 12\n",
      "\n",
      "len. Ca recording 4224, total num. neurons 117, num. ID'd neurons 22\n",
      "len. Ca recording 3445, total num. neurons 130, num. ID'd neurons 37\n",
      "len. Ca recording 3792, total num. neurons 121, num. ID'd neurons 35\n",
      "len. Ca recording 3793, total num. neurons 119, num. ID'd neurons 36\n",
      "len. Ca recording 4212, total num. neurons 104, num. ID'd neurons 33\n",
      "len. Ca recording 3642, total num. neurons 121, num. ID'd neurons 26\n",
      "len. Ca recording 4214, total num. neurons 124, num. ID'd neurons 36\n",
      "len. Ca recording 3190, total num. neurons 116, num. ID'd neurons 35\n",
      "len. Ca recording 3646, total num. neurons 113, num. ID'd neurons 40\n",
      "len. Ca recording 4212, total num. neurons 108, num. ID'd neurons 32\n",
      "len. Ca recording 3154, total num. neurons 127, num. ID'd neurons 29\n",
      "len. Ca recording 3034, total num. neurons 104, num. ID'd neurons 36\n",
      "['FiveStates', 'FourStates', 'IDs', 'dataset', 'fps', 'stimulus', 'timeVectorSeconds', 'traces', 'tracesDif']\n",
      "\n",
      "num. worms: 11\n",
      "\n",
      "len. Ca recording 3721, total num. neurons 116, num. ID'd neurons 35\n",
      "len. Ca recording 3863, total num. neurons 111, num. ID'd neurons 38\n",
      "len. Ca recording 3595, total num. neurons 106, num. ID'd neurons 30\n",
      "len. Ca recording 3596, total num. neurons 115, num. ID'd neurons 39\n",
      "len. Ca recording 3453, total num. neurons 100, num. ID'd neurons 35\n",
      "len. Ca recording 3446, total num. neurons 102, num. ID'd neurons 27\n",
      "len. Ca recording 3957, total num. neurons 102, num. ID'd neurons 34\n",
      "len. Ca recording 3797, total num. neurons 87, num. ID'd neurons 33\n",
      "len. Ca recording 3033, total num. neurons 82, num. ID'd neurons 33\n",
      "len. Ca recording 3797, total num. neurons 104, num. ID'd neurons 39\n",
      "len. Ca recording 3633, total num. neurons 82, num. ID'd neurons 32\n",
      "['FiveStates', 'FourStates', 'IDs', 'dataset', 'fps', 'stimulus', 'timeVectorSeconds', 'traces', 'tracesDif']\n",
      "\n",
      "num. worms: 11\n",
      "\n",
      "len. Ca recording 4226, total num. neurons 101, num. ID'd neurons 36\n",
      "len. Ca recording 4222, total num. neurons 126, num. ID'd neurons 42\n",
      "len. Ca recording 4943, total num. neurons 99, num. ID'd neurons 29\n",
      "len. Ca recording 4221, total num. neurons 114, num. ID'd neurons 44\n",
      "len. Ca recording 4213, total num. neurons 114, num. ID'd neurons 43\n",
      "len. Ca recording 4755, total num. neurons 112, num. ID'd neurons 44\n",
      "len. Ca recording 4942, total num. neurons 104, num. ID'd neurons 37\n",
      "len. Ca recording 4213, total num. neurons 123, num. ID'd neurons 42\n",
      "len. Ca recording 4763, total num. neurons 114, num. ID'd neurons 40\n",
      "len. Ca recording 3652, total num. neurons 118, num. ID'd neurons 38\n",
      "len. Ca recording 3649, total num. neurons 103, num. ID'd neurons 36\n",
      "['FiveStates', 'FourStates', 'IDs', 'dataset', 'fps', 'stimulus', 'timeVectorSeconds', 'traces', 'tracesDif']\n",
      "\n",
      "num. worms: 10\n",
      "\n",
      "len. Ca recording 4055, total num. neurons 114, num. ID'd neurons 38\n",
      "len. Ca recording 3956, total num. neurons 99, num. ID'd neurons 31\n",
      "len. Ca recording 4397, total num. neurons 97, num. ID'd neurons 29\n",
      "len. Ca recording 4217, total num. neurons 116, num. ID'd neurons 39\n",
      "len. Ca recording 3790, total num. neurons 115, num. ID'd neurons 41\n",
      "len. Ca recording 3790, total num. neurons 113, num. ID'd neurons 38\n",
      "len. Ca recording 4211, total num. neurons 105, num. ID'd neurons 34\n",
      "len. Ca recording 4205, total num. neurons 87, num. ID'd neurons 29\n",
      "len. Ca recording 3443, total num. neurons 80, num. ID'd neurons 29\n",
      "len. Ca recording 3584, total num. neurons 109, num. ID'd neurons 35\n",
      "\n",
      "dict_keys(['worm1', 'worm2', 'worm3', 'worm4', 'worm5', 'worm6', 'worm7', 'worm8', 'worm9', 'worm10', 'worm11', 'worm12', 'worm13', 'worm14', 'worm15', 'worm16', 'worm17', 'worm18', 'worm19', 'worm20', 'worm21', 'worm22', 'worm23', 'worm24', 'worm25', 'worm26', 'worm27', 'worm28', 'worm29', 'worm30', 'worm31', 'worm32', 'worm33', 'worm34', 'worm35', 'worm36', 'worm37', 'worm38', 'worm39', 'worm40', 'worm41', 'worm42', 'worm43', 'worm44'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Nichols et al., Science 2017, *A global brain state underlies C. elegans sleep behavior* \n",
    "\n",
    "data_dict = dict()\n",
    "\n",
    "# 'n2_let'\n",
    "# load the first .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Nichols2017', 'n2_let.mat'))['n2_let']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['IDs'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "\n",
    "for i, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(i+1)\n",
    "    i_IDs = [(j[0] if isinstance(j,list) else j) for j in all_IDs[i]]\n",
    "    neuron_IDs = {nid+1: (str(nid+1) if (j is None or isinstance(j, np.ndarray)) else str(j)) for nid,j  in enumerate(i_IDs)} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, 'num_named': num_named},\n",
    "                     })\n",
    "\n",
    "\n",
    "# 'n2_prelet'\n",
    "# load the second .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Nichols2017', 'n2_prelet.mat'))['n2_prelet']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['IDs'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "\n",
    "for ii, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(ii+1 + i+1)\n",
    "    ii_IDs = [(j[0] if isinstance(j,list) else j) for j in all_IDs[ii]]\n",
    "    neuron_IDs = {nid+1: (str(nid+1) if (j is None or isinstance(j, np.ndarray)) else str(j)) for nid,j  in enumerate(ii_IDs)} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, 'num_named': num_named},\n",
    "                     })\n",
    "    \n",
    "\n",
    "# 'npr1_let'\n",
    "# load the third .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Nichols2017', 'npr1_let.mat'))['npr1_let']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['IDs'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "\n",
    "for iii, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(iii+1 + ii+1 + i+1)\n",
    "    iii_IDs = [(j[0] if isinstance(j,list) else j) for j in all_IDs[iii]]\n",
    "    neuron_IDs = {nid+1: (str(nid+1) if (j is None or isinstance(j, np.ndarray)) else str(j)) for nid,j  in enumerate(iii_IDs)} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, 'num_named': num_named},\n",
    "                     })\n",
    "\n",
    "# 'npr1_prelet'\n",
    "# load the fourth .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Nichols2017', 'npr1_prelet.mat'))['npr1_prelet']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['IDs'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "\n",
    "for iv, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(iv+1 + iii+1 + ii+1 + i+1)\n",
    "    iv_IDs = [(j[0] if isinstance(j,list) else j) for j in all_IDs[iv]]\n",
    "    neuron_IDs = {nid+1: (str(nid+1) if (j is None or isinstance(j, np.ndarray)) else str(j)) for nid,j  in enumerate(iv_IDs)} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, 'num_named': num_named},\n",
    "                     })\n",
    "\n",
    "# pickle the data\n",
    "file = os.path.join(root, \"Nichols2017.pickle\")\n",
    "pickle_out = open(file, \"wb\")\n",
    "pickle.dump(data_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_in = open(file, \"rb\")\n",
    "Nichols2017 = pickle.load(pickle_in)\n",
    "print()\n",
    "print(Nichols2017.keys())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__version__', '__globals__', 'hasPointsTime', 'ethoTrack', 'R2', 'G2', 'Ratio2', 'acorr', 'cgIdx', 'cgIdxRev', 'DmatAll']\n",
      "\n",
      "len. Ca recording 1516, total num. neurons 77, num. ID'd neurons 0\n",
      "\n",
      "['__header__', '__version__', '__globals__', 'hasPointsTime', 'ethoTrack', 'R2', 'G2', 'Ratio2', 'acorr', 'cgIdx', 'cgIdxRev', 'rRaw', 'gRaw', 'rPhotoCorr', 'gPhotoCorr']\n",
      "\n",
      "len. Ca recording 2849, total num. neurons 156, num. ID'd neurons 0\n",
      "\n",
      "dict_keys(['worm1', 'worm2'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Nguyen et al., PLOS CompBio 2017, *Automatically tracking neurons in a moving and deforming brain* \n",
    "\n",
    "# WORM 1\n",
    "# load .mat file for  worm 1\n",
    "arr1 = loadmat(os.path.join(data_path, 'Nguyen2017', 'heatData_worm1.mat')) # load .mat file\n",
    "print(list(arr1.keys()))\n",
    "print()\n",
    "# get data for worm 1\n",
    "G2 = arr1['G2'] # the ratio signal is defined as gPhotoCorr/rPhotoCorr, the Ratio is then normalized as delta R/ R0. is the same way as R2 and G2.\n",
    "cgIdx = arr1['cgIdx'].squeeze() # ordered indices derived from heirarchically clustering the correlation matrix. \n",
    "real_data1 = G2[cgIdx-1, :].T # to show organized traces, use Ratio2(cgIdx,:)\n",
    "real_data1 = np.nan_to_num(real_data1) # replace NaNs \n",
    "max_time1, num_neurons1 = real_data1.shape \n",
    "num_named1 = 0\n",
    "worm1_IDs = {i+1: str(i+1) for i in range(num_neurons1)}\n",
    "print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time1, num_neurons1, num_named1))\n",
    "print()\n",
    "# normalize the data \n",
    "sc = preprocessing.MinMaxScaler()\n",
    "real_data1 = sc.fit_transform(real_data1[:, :num_neurons1]) \n",
    "# add a feature dimension and convert to tensor\n",
    "real_data1 = np.expand_dims(real_data1, axis=-1)\n",
    "real_data1 = torch.tensor(real_data1, dtype=torch.float64)\n",
    "\n",
    "\n",
    "# WORM 2\n",
    "# load .mat file for  worm 1\n",
    "arr2 = loadmat(os.path.join(data_path, 'Nguyen2017', 'heatData_worm2.mat')) # load .mat file\n",
    "print(list(arr2.keys()))\n",
    "print()\n",
    "# get data for worm 2\n",
    "G2 = arr2['G2'] # the ratio signal is defined as gPhotoCorr/rPhotoCorr, the Ratio is then normalized as delta R/ R0. is the same way as R2 and G2.\n",
    "cgIdx = arr2['cgIdx'].squeeze() # ordered indices derived from heirarchically clustering the correlation matrix. \n",
    "real_data2 = G2[cgIdx-1, :].T # to show organized traces, use Ratio2(cgIdx,:)\n",
    "real_data2 = np.nan_to_num(real_data2) # replace NaNs \n",
    "max_time2, num_neurons2 = real_data2.shape \n",
    "num_named2 = 0\n",
    "worm2_IDs = {i+1: str(i+1) for i in range(num_neurons2)}\n",
    "print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time2, num_neurons2, num_named2))\n",
    "print()\n",
    "# normalize the data \n",
    "sc = preprocessing.MinMaxScaler()\n",
    "real_data2 = sc.fit_transform(real_data2[:, :num_neurons2]) \n",
    "# add a feature dimension and convert to tensor\n",
    "real_data2 = np.expand_dims(real_data2, axis=-1)\n",
    "real_data2 = torch.tensor(real_data2, dtype=torch.float64)\n",
    "\n",
    "\n",
    "# pickle the data\n",
    "data_dict = {'worm1': {'data': real_data1, 'neuron_ids': worm1_IDs, 'max_time': max_time1, \n",
    "                       'num_neurons': num_neurons1, 'num_named': num_named1}, \n",
    "             'worm2': {'data': real_data2, 'neuron_ids': worm2_IDs, 'max_time': max_time2, \n",
    "                       'num_neurons': num_neurons2, 'num_named': num_named2},\n",
    "            }\n",
    "file = os.path.join(root, \"Nguyen2017.pickle\")\n",
    "pickle_out = open(file, \"wb\")\n",
    "pickle.dump(data_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_in = open(file, \"rb\")\n",
    "Nguyen2017 = pickle.load(pickle_in)\n",
    "print(Nguyen2017.keys())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FourStateKey', 'FourStates', 'IDs', 'dataset', 'fps', 'stimulus', 'timeVectorSeconds', 'traces']\n",
      "\n",
      "num. worms: 6\n",
      "\n",
      "len. Ca recording 2597, total num. neurons 139, num. ID'd neurons 40\n",
      "len. Ca recording 2397, total num. neurons 143, num. ID'd neurons 51\n",
      "len. Ca recording 2585, total num. neurons 131, num. ID'd neurons 55\n",
      "len. Ca recording 2209, total num. neurons 114, num. ID'd neurons 48\n",
      "len. Ca recording 2204, total num. neurons 123, num. ID'd neurons 45\n",
      "len. Ca recording 2810, total num. neurons 127, num. ID'd neurons 42\n",
      "['FourStateKey', 'FourStates', 'IDs', 'dataset', 'fps', 'stimulus', 'timeVectorSeconds', 'traces']\n",
      "\n",
      "num. worms: 6\n",
      "\n",
      "len. Ca recording 2366, total num. neurons 128, num. ID'd neurons 48\n",
      "len. Ca recording 2362, total num. neurons 147, num. ID'd neurons 52\n",
      "len. Ca recording 2353, total num. neurons 127, num. ID'd neurons 45\n",
      "len. Ca recording 1893, total num. neurons 125, num. ID'd neurons 39\n",
      "len. Ca recording 1894, total num. neurons 126, num. ID'd neurons 47\n",
      "len. Ca recording 2495, total num. neurons 123, num. ID'd neurons 49\n",
      "\n",
      "dict_keys(['worm1', 'worm2', 'worm3', 'worm4', 'worm5', 'worm6', 'worm7', 'worm8', 'worm9', 'worm10', 'worm11', 'worm12'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Skora et al., Cell Reports 2018, *Energy Scarcity Promotes a Brain-wide Sleep State Modulated by Insulin Signaling in C. elegans* \n",
    "\n",
    "data_dict = dict()\n",
    "\n",
    "# 'WT_fasted'\n",
    "# load the first .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Skora2018', 'WT_fasted.mat'))['WT_fasted']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['IDs'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "\n",
    "for i, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(i+1)\n",
    "    i_IDs = [(j[0] if isinstance(j,list) else j) for j in all_IDs[i]]\n",
    "    neuron_IDs = {nid+1: (str(nid+1) if (j is None or isinstance(j, np.ndarray)) else str(j)) for nid,j  in enumerate(i_IDs)} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, 'num_named': num_named},\n",
    "                     })\n",
    "\n",
    "# 'WT_starved'\n",
    "# load the second .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Skora2018', 'WT_starved.mat'))['WT_starved']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['IDs'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "\n",
    "for ii, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(ii+1 + i+1)\n",
    "    ii_IDs = [(j[0] if isinstance(j,list) else j) for j in all_IDs[ii]]\n",
    "    neuron_IDs = {nid+1: (str(nid+1) if (j is None or isinstance(j, np.ndarray)) else str(j)) for nid,j  in enumerate(ii_IDs)} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, 'num_named': num_named},\n",
    "                     })\n",
    "    \n",
    "\n",
    "# pickle the data\n",
    "file = os.path.join(root, \"Skora2018.pickle\")\n",
    "pickle_out = open(file, \"wb\")\n",
    "pickle.dump(data_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_in = open(file, \"rb\")\n",
    "Skora2018 = pickle.load(pickle_in)\n",
    "print()\n",
    "print(Skora2018.keys())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['five_state_annotation', 'neuron_ID', 'parameters', 'peaks', 'raw_traces', 'time_vector', 'traces_bleach_corrected', 'traces_bleach_corrected_detrended', 'traces_derivatives', 'volumes_per_second']\n",
      "\n",
      "num. worms: 10\n",
      "\n",
      "len. Ca recording 5455, total num. neurons 103, num. ID'd neurons 45\n",
      "len. Ca recording 5455, total num. neurons 129, num. ID'd neurons 53\n",
      "len. Ca recording 5455, total num. neurons 122, num. ID'd neurons 48\n",
      "len. Ca recording 5455, total num. neurons 119, num. ID'd neurons 53\n",
      "len. Ca recording 5455, total num. neurons 124, num. ID'd neurons 53\n",
      "len. Ca recording 3578, total num. neurons 111, num. ID'd neurons 40\n",
      "len. Ca recording 3269, total num. neurons 128, num. ID'd neurons 37\n",
      "len. Ca recording 2941, total num. neurons 127, num. ID'd neurons 48\n",
      "len. Ca recording 4010, total num. neurons 129, num. ID'd neurons 45\n",
      "len. Ca recording 3692, total num. neurons 124, num. ID'd neurons 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['five_state_annotation', 'neuron_ID', 'parameters', 'peaks', 'raw_traces', 'time_vector', 'traces_bleach_corrected', 'traces_bleach_corrected_detrended', 'traces_derivatives', 'volumes_per_second']\n",
      "\n",
      "num. worms: 4\n",
      "\n",
      "len. Ca recording 2812, total num. neurons 102, num. ID'd neurons 28\n",
      "len. Ca recording 2917, total num. neurons 99, num. ID'd neurons 29\n",
      "len. Ca recording 3163, total num. neurons 107, num. ID'd neurons 28\n",
      "len. Ca recording 2795, total num. neurons 97, num. ID'd neurons 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n",
      "ERROR:root:ERROR: MATLAB type not supported: function_handle_workspace, (uint32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['five_state_annotation', 'neuron_ID', 'parameters', 'peaks', 'raw_traces', 'time_vector', 'traces_bleach_corrected', 'traces_bleach_corrected_detrended', 'traces_derivatives', 'volumes_per_second']\n",
      "\n",
      "num. worms: 5\n",
      "\n",
      "len. Ca recording 3805, total num. neurons 105, num. ID'd neurons 25\n",
      "len. Ca recording 3803, total num. neurons 114, num. ID'd neurons 30\n",
      "len. Ca recording 3657, total num. neurons 123, num. ID'd neurons 27\n",
      "len. Ca recording 2808, total num. neurons 97, num. ID'd neurons 28\n",
      "len. Ca recording 3157, total num. neurons 117, num. ID'd neurons 23\n",
      "\n",
      "dict_keys(['worm1', 'worm2', 'worm3', 'worm4', 'worm5', 'worm6', 'worm7', 'worm8', 'worm9', 'worm10', 'worm11', 'worm12', 'worm13', 'worm14', 'worm15', 'worm16', 'worm17', 'worm18', 'worm19'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Kaplan et al., Neuron 2020, *Nested Neuronal Dynamics Orchestrate a Behavioral Hierarchy across Timescales*\n",
    "\n",
    "data_dict = dict()\n",
    "\n",
    "# 'RIShisCl_Neuron2019'\n",
    "# load the first .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Kaplan2020', 'Neuron2019_Data_RIShisCl.mat'))['RIShisCl_Neuron2019']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['neuron_ID'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces_bleach_corrected'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "\n",
    "for i, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(i+1)\n",
    "    neuron_IDs = {nid+1: str(j) for nid, j in enumerate(all_IDs[i])} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, \n",
    "                             'num_named': num_named},\n",
    "                     })\n",
    "    \n",
    "\n",
    "# 'MNhisCl_RIShisCl_Neuron2019'\n",
    "# load the second .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Kaplan2020', 'Neuron2019_Data_MNhisCl_RIShisCl.mat'))['MNhisCl_RIShisCl_Neuron2019']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['neuron_ID'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces_bleach_corrected'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "for ii, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(ii+1 + i+1)\n",
    "    neuron_IDs = {nid+1: str(j) for nid, j in enumerate(all_IDs[ii])} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, \n",
    "                             'num_named': num_named},\n",
    "                     })\n",
    "    \n",
    "\n",
    "# 'MNhisCl_RIShisCl_Neuron2019'\n",
    "# load the third .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Kaplan2020', 'Neuron2019_Data_SMDhisCl_RIShisCl.mat'))['SMDhisCl_RIShisCl_Neuron2019']\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "# get data for all worms\n",
    "all_IDs = arr['neuron_ID'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces_bleach_corrected'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "for iii, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(iii+1 + ii+1 + i+1)\n",
    "    neuron_IDs = {nid+1: str(j) for nid, j in enumerate(all_IDs[iii])} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, \n",
    "                             'num_named': num_named},\n",
    "                     })\n",
    "\n",
    "    \n",
    "# pickle the data\n",
    "file = os.path.join(root, \"Kaplan2020.pickle\")\n",
    "pickle_out = open(file, \"wb\")\n",
    "pickle.dump(data_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_in = open(file, \"rb\")\n",
    "Kaplan2020 = pickle.load(pickle_in)\n",
    "print()\n",
    "print(Kaplan2020.keys())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IDs', 'derivatives', 'fps', 'options', 'states', 'statesKey', 'traces', 'tv']\n",
      "\n",
      "num. worms: 6\n",
      "\n",
      "len. Ca recording 3312, total num. neurons 154, num. ID'd neurons 58\n",
      "len. Ca recording 3749, total num. neurons 154, num. ID'd neurons 54\n",
      "len. Ca recording 4126, total num. neurons 133, num. ID'd neurons 48\n",
      "len. Ca recording 5450, total num. neurons 124, num. ID'd neurons 46\n",
      "len. Ca recording 3313, total num. neurons 136, num. ID'd neurons 51\n",
      "len. Ca recording 3311, total num. neurons 133, num. ID'd neurons 47\n",
      "dict_keys(['worm1', 'worm2', 'worm3', 'worm4', 'worm5', 'worm6'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Uzel et al 2022., Cell CurrBio 2022, *A set of hub neurons and non-local connectivity features support global brain dynamics in C. elegans*\n",
    "\n",
    "data_dict = dict()\n",
    "\n",
    "# load .mat file\n",
    "arr = mat73.loadmat(os.path.join(data_path, 'Uzel2022', 'Uzel_WT.mat'))['Uzel_WT'] # load .mat file\n",
    "print(list(arr.keys()))\n",
    "print()\n",
    "\n",
    "# get data for all worms\n",
    "all_IDs = arr['IDs'] # identified neuron IDs (only subset have neuron names)\n",
    "all_traces = arr['traces'] # neural activity traces corrected for bleaching\n",
    "print('num. worms:', len(all_IDs))\n",
    "print()\n",
    "for i, real_data in enumerate(all_traces):\n",
    "    worm =  \"worm\"+str(i+1)\n",
    "    i_IDs = [np.array(j).item() for j in all_IDs[i]]\n",
    "    neuron_IDs = {nid+1: (str(int(j)) if type(j)!=str else j) for nid,j  in enumerate(i_IDs)} \n",
    "    neuron_IDs = {nid: (name.replace('0','') if not name.endswith('0') and not name.isnumeric() else name) for nid, name in neuron_IDs.items()}\n",
    "    max_time, num_neurons = real_data.shape  \n",
    "    num_named = len([v for v in neuron_IDs.values() if not v.isnumeric()]) # number of neurons that were ID'd\n",
    "    print(\"len. Ca recording %s, total num. neurons %s, num. ID'd neurons %s\"%(\n",
    "        max_time, num_neurons, num_named))\n",
    "    sc = preprocessing.MinMaxScaler() # normalize data\n",
    "    real_data = sc.fit_transform(real_data[:, :num_neurons]) \n",
    "    real_data = np.expand_dims(real_data, axis=-1)\n",
    "    real_data = torch.tensor(real_data, dtype=torch.float64) # add a feature dimension and convert to tensor\n",
    "    data_dict.update({worm: {'data': real_data, 'neuron_ids': neuron_IDs, \n",
    "                             'max_time': max_time, 'num_neurons': num_neurons, 'num_named': num_named},\n",
    "                     })\n",
    "    \n",
    "# pickle the data\n",
    "file = os.path.join(root, \"Uzel2022.pickle\")\n",
    "pickle_out = open(file, \"wb\")\n",
    "pickle.dump(data_dict, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_in = open(file, \"rb\")\n",
    "Uzel2022 = pickle.load(pickle_in)\n",
    "print(Uzel2022.keys())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Delete the downloaded raw datasets.\n",
    "#@markdown The files are too large to push to GitHub.\n",
    "\n",
    "shutil.rmtree(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "66fc48d1066461498fec04d710ecd27d9a00e22a5fbd94430ea07906109bd52e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
